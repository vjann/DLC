{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RK255E7YoEIt"
   },
   "source": [
    "# DeepLabCut Toolbox\n",
    "https://github.com/AlexEMG/DeepLabCut\n",
    "\n",
    "Nath\\*, Mathis\\* et al. *Using DeepLabCut for markerless pose estimation during behavior across species*, (under revision).\n",
    "\n",
    "This notebook demonstrates the necessary steps to use DeepLabCut for your own project.\n",
    "This shows the most simple code to do so, but many of the functions have additional features, so please check out the overview & the protocol paper!\n",
    "\n",
    "This notebook illustrates how to:\n",
    "- create a project\n",
    "- extract training frames\n",
    "- label the frames\n",
    "- plot the labeled images\n",
    "- create a training set\n",
    "- train a network\n",
    "- evaluate a network\n",
    "- analyze a novel video\n",
    "- create an automatically labeled video \n",
    "- plot the trajectories\n",
    "\n",
    "*Note*: Refine a network based after the network was trained on just a few labeled images is illustrated in \"Demo-labeledexample-MouseReaching.ipynb\". This demo also contains an already labeled data set and is perhaps the best starting point for brand new users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Uoz9mdPoEIy"
   },
   "source": [
    "## Create a new project\n",
    "\n",
    "It is always good idea to keep the projects separate. This function creates a new project with subdirectories and a basic configuration file in the user defined directory otherwise the project is created in the current working directory.\n",
    "\n",
    "You can always add new videos to the project at any stage of the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jqLZhp7EoEI0"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import deeplabcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9DjG55FoEI7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created \"C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-06\\videos\"\n",
      "Created \"C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-06\\labeled-data\"\n",
      "Created \"C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-06\\training-datasets\"\n",
      "Created \"C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-06\\dlc-models\"\n",
      "Copying the videos\n",
      "C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-06\\videos\\1080p.MOV\n",
      "Generated \"C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-06\\config.yaml\"\n",
      "\n",
      "A new project with name frontslowmo-vj-2019-06-06 is created at C:\\Users\\vjj14\\Desktop\\DeepLabCut and a configurable file (config.yaml) is stored there. Change the parameters in this file to adapt to your project's needs.\n",
      " Once you have changed the configuration file, use the function 'extract_frames' to select frames for labeling.\n",
      ". [OPTIONAL] Use the function 'add_new_videos' to add new videos to your project (at any stage).\n"
     ]
    }
   ],
   "source": [
    "task='frontslowmo' # Enter the name of your experiment Task\n",
    "experimenter='vj' # Enter the name of the experimenter\n",
    "video=[r\"C:\\Users\\vjj14\\Downloads\\1080p.MOV\"] # Enter the paths of your videos you want to grab frames from.\n",
    "time_log= open(\"time_log{}.txt\".format(task),\"w+\")\n",
    "path_config_file=deeplabcut.create_new_project(task,experimenter,video, working_directory= r'C:\\Users\\vjj14\\Desktop\\DeepLabCut',copy_videos=True) #change the working directory to where you want the folders created.\n",
    "\n",
    "# The function returns the path, where your project is. \n",
    "# You could also enter this manually (e.g. if the project is already created and you want to pick up, where you stopped...)\n",
    "#path_config_file = '/home/Mackenzie/Reaching/config.yaml' # Enter the path of the config file that was just created from the above step (check the folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_config_file='C:\\\\Users\\\\vjj14\\\\Desktop\\\\DeepLabCut\\\\frontslowmo-vj-2019-06-05\\\\config.yaml'\n",
    "project_name='frontslowmo'\n",
    "def write_log(text, new_line=True):\n",
    "    time_log= open(\"{0}time_log{1}.txt\".format(path_config_file[:-11], project_name),\"a+\")\n",
    "    if new_line:\n",
    "        time_log.write(\"\\n\")\n",
    "    time_log.write(text)\n",
    "    time_log.close()\n",
    "    print(\"log written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-05\\\n",
      "log written\n"
     ]
    }
   ],
   "source": [
    "print(path_config_file[:-11])\n",
    "write_log(\"test1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0yXW0bx1oEJA"
   },
   "source": [
    "## Extract frames from videos \n",
    "A key point for a successful feature detector is to select diverse frames, which are typical for the behavior you study that should be labeled.\n",
    "\n",
    "This function selects N frames either uniformly sampled from a particular video (or folder) (algo=='uniform'). Note: this might not yield diverse frames, if the behavior is sparsely distributed (consider using kmeans), and/or select frames manually etc.\n",
    "\n",
    "Also make sure to get select data from different (behavioral) sessions and different animals if those vary substantially (to train an invariant feature detector).\n",
    "\n",
    "Individual images should not be too big (i.e. < 850 x 850 pixel). Although this can be taken care of later as well, it is advisable to crop the frames, to remove unnecessary parts of the frame as much as possible.\n",
    "\n",
    "Always check the output of cropping. If you are happy with the results proceed to labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t1ulumCuoEJC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file read successfully.\n",
      "Do you want to extract (perhaps additional) frames for video: C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-05\\videos\\1080p.MOV ?\n",
      "yes/nono\n",
      "\n",
      "Frames were selected.\n",
      "You can now label the frames using the function 'label_frames' (if you extracted enough frames for all videos).\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "write_log(str(start) + \"***extract_frames start\")\n",
    "\n",
    "%matplotlib inline\n",
    "deeplabcut.extract_frames(path_config_file,'automatic','uniform',crop=False) #there are other ways to grab frames, such as by clustering 'kmeans'; please see the paper. \n",
    "\n",
    "end = datetime.datetime.now()\n",
    "write_log(str(end) + \"***extract_frames end\")\n",
    "write_log(\"   time elapsed:\" + str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gjn6ZDonoEJH"
   },
   "source": [
    "## Label the extracted frames\n",
    "Only videos in the config file can be used to extract the frames. Extracted labels for each video are stored in the project directory under the subdirectory **'labeled-data'**. Each subdirectory is named after the name of the video. The toolbox has a labeling toolbox which could be used for labeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iyROSOiEoEJI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Cannot activate multiple GUI eventloops\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Cannot load backend 'TkAgg' which requires the 'tk' interactive framework, as 'wx' is currently running",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6005a6684cbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gui wx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdeeplabcut\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\generate_training_dataset\\trainingsetmanipulation.py\u001b[0m in \u001b[0;36mlabel_frames\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;31m# labeling_toolbox.show(config,Screens,scale_w,scale_h, winHack, img_scale)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m     \u001b[0mlabeling_toolbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstartpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\generate_training_dataset\\labeling_toolbox.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m     \u001b[0mapp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mApp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m     \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMainFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mShow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m     \u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMainLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\generate_training_dataset\\labeling_toolbox.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, parent, config)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mvSplitter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSplitterWindow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopSplitter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_panel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImagePanel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvSplitter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice_panel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScrollPanel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvSplitter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mvSplitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSplitVertically\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_panel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice_panel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msashPosition\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\generate_training_dataset\\labeling_toolbox.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, parent, config, gui_size, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFigureCanvas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morig_xlim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morig_ylim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\matplotlib\\backends\\backend_wx.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, parent, id, figure)\u001b[0m\n\u001b[0;32m    589\u001b[0m         \"\"\"\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    592\u001b[0m         \u001b[1;31m# Set preferred window size hint - helps the sizer (if one is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[1;31m# connected)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, figure)\u001b[0m\n\u001b[0;32m   1580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1582\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fix_ipython_backend2gui\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1583\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_idle_drawing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1584\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_saving\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36m_fix_ipython_backend2gui\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m   1627\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m                 \u001b[0mmpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParamsOrig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"backend\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"backend\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1629\u001b[1;33m                 \u001b[0mip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1630\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m                 \u001b[0mmpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParamsOrig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"backend\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morig_origbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36menable_matplotlib\u001b[1;34m(self, gui)\u001b[0m\n\u001b[0;32m   2913\u001b[0m                 \u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_gui_and_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpylab_gui_select\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2915\u001b[1;33m         \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivate_matplotlib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2916\u001b[0m         \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfigure_inline_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\IPython\\core\\pylabtools.py\u001b[0m in \u001b[0;36mactivate_matplotlib\u001b[1;34m(backend)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswitch_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;31m# This must be imported last in the matplotlib series, after\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mswitch_backend\u001b[1;34m(newbackend)\u001b[0m\n\u001b[0;32m    228\u001b[0m                 \u001b[1;34m\"Cannot load backend {!r} which requires the {!r} interactive \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                 \"framework, as {!r} is currently running\".format(\n\u001b[1;32m--> 230\u001b[1;33m                     newbackend, required_framework, current_framework))\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'backend'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrcParamsDefault\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'backend'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Cannot load backend 'TkAgg' which requires the 'tk' interactive framework, as 'wx' is currently running"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "write_log(str(start) + \"***label_frames start\")\n",
    "\n",
    "%gui wx\n",
    "deeplabcut.label_frames(path_config_file)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "write_log(str(end) + \"***label_frames end\")\n",
    "write_log(\"   time elapsed:\" + str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vim95ZvkPSeN"
   },
   "source": [
    "**Check the labels**\n",
    "\n",
    "Checking if the labels were created and stored correctly is beneficial for training, since labeling is one of the most critical parts for creating the training dataset. The DeepLabCut toolbox provides a function `check\\_labels'  to do so. It is used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NwvgPJouPP2O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating images with labels by vj.\n",
      "C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-05\\labeled-data\\1080p_labeled  already exists!\n",
      "They are stored in the following folder: C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-05\\labeled-data\\1080p_labeled.\n",
      "Attention: C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-05\\labeled-data\\1080p does not appear to have labeled data!\n",
      "If all the labels are ok, then use the function 'create_training_dataset' to create the training dataset!\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.check_labels(path_config_file) #this creates a subdirectory with the frames + your labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "of87fOjgPqzH"
   },
   "source": [
    "If the labels need adjusted, you can use the refinement GUI to move them around! Check that out below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNi9s1dboEJN"
   },
   "source": [
    "## Create a training dataset\n",
    "This function generates the training data information for DeepCut (which requires a mat file) based on the pandas dataframes that hold label information. The user can set the fraction of the training set size (from all labeled image in the hd5 file) in the config.yaml file. While creating the dataset, the user can create multiple shuffles. \n",
    "\n",
    "After running this script the training dataset is created and saved in the project directory under the subdirectory **'training-datasets'**\n",
    "\n",
    "This function also creates new subdirectories under **dlc-models** and appends the project config.yaml file with the correct path to the training and testing pose configuration file. These files hold the parameters for training the network. Such an example file is provided with the toolbox and named as **pose_cfg.yaml**.\n",
    "\n",
    "Now it is the time to start training the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMeUwgxPoEJP",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-05\\training-datasets\\iteration-0\\UnaugmentedDataSet_frontslowmoJun5  already exists!\n",
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "write_log(str(start) + \"***create_training_dataset start\")\n",
    "\n",
    "deeplabcut.create_training_dataset(path_config_file)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "write_log(str(end) + \"***create_training_dataset end\")\n",
    "write_log(\"   time elapsed:\" + str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "?deeplabcut.create_training_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4FczXGDoEJU"
   },
   "source": [
    "## Start training - If you want to use a CPU, continue. \n",
    "### If you want to use your GPU, you need to exit here and either work from the Docker container, your own TensorFlow installation in an Anaconda env\n",
    "\n",
    "This function trains the network for a specific shuffle of the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "def prompt_with_timeout(prompt_message):\n",
    "  print('Waiting... please press Ctrl-C when you wish to proceed.')\n",
    "  try:\n",
    "    for i in range(0, 2*60): # 2 minutes\n",
    "      sleep(1)\n",
    "    return \"No Input Received\"\n",
    "  except KeyboardInterrupt:\n",
    "    return input(prompt_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pOvDq_2oEJW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log written\n",
      "Waiting... please press Ctrl-C when you wish to proceed.\n",
      "Training description(Network, Iterations, etc) continuation of the second network (the best one), maybe include relabeled frames from clock time.\n",
      "log written\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3]],\n",
      " 'all_joints_names': ['LeftHand', 'RightHand', 'Nose', 'Pellet'],\n",
      " 'batch_size': 4,\n",
      " 'bottomheight': 400,\n",
      " 'crop': True,\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets\\\\iteration-1\\\\UnaugmentedDataSet_frontslowmoJun5\\\\frontslowmo_vj95shuffle1.mat',\n",
      " 'dataset_type': 'default',\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': 'C:\\\\Users\\\\vjj14\\\\Desktop\\\\DeepLabCut\\\\frontslowmo-vj-2019-06-05\\\\dlc-models\\\\iteration-1\\\\frontslowmoJun5-trainset95shuffle1\\\\train\\\\snapshot-30000',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'leftwidth': 400,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets\\\\iteration-1\\\\UnaugmentedDataSet_frontslowmoJun5\\\\Documentation_data-frontslowmo_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'minsize': 100,\n",
      " 'mirror': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 4,\n",
      " 'optimizer': 'sgd',\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': 'C:\\\\Users\\\\vjj14\\\\Desktop\\\\DeepLabCut\\\\frontslowmo-vj-2019-06-05',\n",
      " 'regularize': False,\n",
      " 'rightwidth': 400,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'C:\\\\Users\\\\vjj14\\\\Desktop\\\\DeepLabCut\\\\frontslowmo-vj-2019-06-05\\\\dlc-models\\\\iteration-1\\\\frontslowmoJun5-trainset95shuffle1\\\\train\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'topheight': 400,\n",
      " 'use_gt_segm': False,\n",
      " 'video': False,\n",
      " 'video_batch': False,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-05\\dlc-models\\iteration-1\\frontslowmoJun5-trainset95shuffle1\\train\\snapshot-30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-05\\dlc-models\\iteration-1\\frontslowmoJun5-trainset95shuffle1\\train\\snapshot-30000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_iters overwritten as 300001\n",
      "Display_iters overwritten as 100\n",
      "Save_iters overwritten as 1000\n",
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'weigh_only_present_joints': False, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': 'C:\\\\Users\\\\vjj14\\\\Desktop\\\\DeepLabCut\\\\frontslowmo-vj-2019-06-05\\\\dlc-models\\\\iteration-1\\\\frontslowmoJun5-trainset95shuffle1\\\\train\\\\snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'mirror': False, 'crop_pad': 0, 'scoremap_dir': 'test', 'dataset_type': 'default', 'use_gt_segm': False, 'batch_size': 1, 'video': False, 'video_batch': False, 'crop': True, 'cropratio': 0.4, 'minsize': 100, 'leftwidth': 400, 'rightwidth': 400, 'topheight': 400, 'bottomheight': 400, 'all_joints': [[0], [1], [2], [3]], 'all_joints_names': ['LeftHand', 'RightHand', 'Nose', 'Pellet'], 'dataset': 'training-datasets\\\\iteration-1\\\\UnaugmentedDataSet_frontslowmoJun5\\\\frontslowmo_vj95shuffle1.mat', 'init_weights': 'C:\\\\Users\\\\vjj14\\\\Desktop\\\\DeepLabCut\\\\frontslowmo-vj-2019-06-05\\\\dlc-models\\\\iteration-1\\\\frontslowmoJun5-trainset95shuffle1\\\\train\\\\snapshot-30000', 'net_type': 'resnet_50', 'num_joints': 4, 'display_iters': 1000, 'max_input_size': 1500, 'metadataset': 'training-datasets\\\\iteration-1\\\\UnaugmentedDataSet_frontslowmoJun5\\\\Documentation_data-frontslowmo_95shuffle1.pickle', 'min_input_size': 64, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'pos_dist_thresh': 17, 'project_path': 'C:\\\\Users\\\\vjj14\\\\Desktop\\\\DeepLabCut\\\\frontslowmo-vj-2019-06-05', 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25}\n",
      "Starting training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 100 loss: 0.0391 lr: 0.005\n",
      "iteration: 200 loss: 0.0109 lr: 0.005\n",
      "iteration: 300 loss: 0.0071 lr: 0.005\n",
      "iteration: 400 loss: 0.0064 lr: 0.005\n",
      "iteration: 500 loss: 0.0053 lr: 0.005\n",
      "iteration: 600 loss: 0.0052 lr: 0.005\n",
      "iteration: 700 loss: 0.0053 lr: 0.005\n",
      "iteration: 800 loss: 0.0051 lr: 0.005\n",
      "iteration: 900 loss: 0.0046 lr: 0.005\n",
      "iteration: 1000 loss: 0.0043 lr: 0.005\n",
      "iteration: 1100 loss: 0.0040 lr: 0.005\n",
      "iteration: 1200 loss: 0.0041 lr: 0.005\n",
      "iteration: 1300 loss: 0.0041 lr: 0.005\n",
      "iteration: 1400 loss: 0.0040 lr: 0.005\n",
      "iteration: 1500 loss: 0.0037 lr: 0.005\n",
      "iteration: 1600 loss: 0.0040 lr: 0.005\n",
      "iteration: 1700 loss: 0.0039 lr: 0.005\n",
      "iteration: 1800 loss: 0.0033 lr: 0.005\n",
      "iteration: 1900 loss: 0.0035 lr: 0.005\n",
      "iteration: 2000 loss: 0.0033 lr: 0.005\n",
      "iteration: 2100 loss: 0.0038 lr: 0.005\n",
      "iteration: 2200 loss: 0.0039 lr: 0.005\n",
      "iteration: 2300 loss: 0.0037 lr: 0.005\n",
      "iteration: 2400 loss: 0.0032 lr: 0.005\n",
      "iteration: 2500 loss: 0.0029 lr: 0.005\n",
      "iteration: 2600 loss: 0.0037 lr: 0.005\n",
      "iteration: 2700 loss: 0.0035 lr: 0.005\n",
      "iteration: 2800 loss: 0.0029 lr: 0.005\n",
      "iteration: 2900 loss: 0.0033 lr: 0.005\n",
      "iteration: 3000 loss: 0.0028 lr: 0.005\n",
      "iteration: 3100 loss: 0.0027 lr: 0.005\n",
      "iteration: 3200 loss: 0.0034 lr: 0.005\n",
      "iteration: 3300 loss: 0.0031 lr: 0.005\n",
      "iteration: 3400 loss: 0.0031 lr: 0.005\n",
      "iteration: 3500 loss: 0.0028 lr: 0.005\n",
      "iteration: 3600 loss: 0.0032 lr: 0.005\n",
      "iteration: 3700 loss: 0.0029 lr: 0.005\n",
      "iteration: 3800 loss: 0.0024 lr: 0.005\n",
      "iteration: 3900 loss: 0.0025 lr: 0.005\n",
      "iteration: 4000 loss: 0.0029 lr: 0.005\n",
      "iteration: 4100 loss: 0.0029 lr: 0.005\n",
      "iteration: 4200 loss: 0.0026 lr: 0.005\n",
      "iteration: 4300 loss: 0.0029 lr: 0.005\n",
      "iteration: 4400 loss: 0.0027 lr: 0.005\n",
      "iteration: 4500 loss: 0.0027 lr: 0.005\n",
      "iteration: 4600 loss: 0.0027 lr: 0.005\n",
      "iteration: 4700 loss: 0.0028 lr: 0.005\n",
      "iteration: 4800 loss: 0.0025 lr: 0.005\n",
      "iteration: 4900 loss: 0.0026 lr: 0.005\n",
      "iteration: 5000 loss: 0.0028 lr: 0.005\n",
      "iteration: 5100 loss: 0.0024 lr: 0.005\n",
      "iteration: 5200 loss: 0.0024 lr: 0.005\n",
      "iteration: 5300 loss: 0.0025 lr: 0.005\n",
      "iteration: 5400 loss: 0.0023 lr: 0.005\n",
      "iteration: 5500 loss: 0.0027 lr: 0.005\n",
      "iteration: 5600 loss: 0.0026 lr: 0.005\n",
      "iteration: 5700 loss: 0.0024 lr: 0.005\n",
      "iteration: 5800 loss: 0.0025 lr: 0.005\n",
      "iteration: 5900 loss: 0.0026 lr: 0.005\n",
      "iteration: 6000 loss: 0.0025 lr: 0.005\n",
      "iteration: 6100 loss: 0.0022 lr: 0.005\n",
      "iteration: 6200 loss: 0.0025 lr: 0.005\n",
      "iteration: 6300 loss: 0.0024 lr: 0.005\n",
      "iteration: 6400 loss: 0.0023 lr: 0.005\n",
      "iteration: 6500 loss: 0.0025 lr: 0.005\n",
      "iteration: 6600 loss: 0.0022 lr: 0.005\n",
      "iteration: 6700 loss: 0.0025 lr: 0.005\n",
      "iteration: 6800 loss: 0.0025 lr: 0.005\n",
      "iteration: 6900 loss: 0.0022 lr: 0.005\n",
      "iteration: 7000 loss: 0.0026 lr: 0.005\n",
      "iteration: 7100 loss: 0.0025 lr: 0.005\n",
      "iteration: 7200 loss: 0.0024 lr: 0.005\n",
      "iteration: 7300 loss: 0.0026 lr: 0.005\n",
      "iteration: 7400 loss: 0.0024 lr: 0.005\n",
      "iteration: 7500 loss: 0.0025 lr: 0.005\n",
      "iteration: 7600 loss: 0.0024 lr: 0.005\n",
      "iteration: 7700 loss: 0.0021 lr: 0.005\n",
      "iteration: 7800 loss: 0.0020 lr: 0.005\n",
      "iteration: 7900 loss: 0.0021 lr: 0.005\n",
      "iteration: 8000 loss: 0.0026 lr: 0.005\n",
      "iteration: 8100 loss: 0.0025 lr: 0.005\n",
      "iteration: 8200 loss: 0.0025 lr: 0.005\n",
      "iteration: 8300 loss: 0.0022 lr: 0.005\n",
      "iteration: 8400 loss: 0.0020 lr: 0.005\n",
      "iteration: 8500 loss: 0.0025 lr: 0.005\n",
      "iteration: 8600 loss: 0.0024 lr: 0.005\n",
      "iteration: 8700 loss: 0.0023 lr: 0.005\n",
      "iteration: 8800 loss: 0.0021 lr: 0.005\n",
      "iteration: 8900 loss: 0.0022 lr: 0.005\n",
      "iteration: 9000 loss: 0.0024 lr: 0.005\n",
      "iteration: 9100 loss: 0.0027 lr: 0.005\n",
      "iteration: 9200 loss: 0.0021 lr: 0.005\n",
      "iteration: 9300 loss: 0.0024 lr: 0.005\n",
      "iteration: 9400 loss: 0.0022 lr: 0.005\n",
      "iteration: 9500 loss: 0.0022 lr: 0.005\n",
      "iteration: 9600 loss: 0.0020 lr: 0.005\n",
      "iteration: 9700 loss: 0.0021 lr: 0.005\n",
      "iteration: 9800 loss: 0.0019 lr: 0.005\n",
      "iteration: 9900 loss: 0.0024 lr: 0.005\n",
      "iteration: 10000 loss: 0.0021 lr: 0.005\n",
      "iteration: 10100 loss: 0.0029 lr: 0.02\n",
      "iteration: 10200 loss: 0.0034 lr: 0.02\n",
      "iteration: 10300 loss: 0.0038 lr: 0.02\n",
      "iteration: 10400 loss: 0.0034 lr: 0.02\n",
      "iteration: 10500 loss: 0.0036 lr: 0.02\n",
      "iteration: 10600 loss: 0.0039 lr: 0.02\n",
      "iteration: 10700 loss: 0.0035 lr: 0.02\n",
      "iteration: 10800 loss: 0.0039 lr: 0.02\n",
      "iteration: 10900 loss: 0.0035 lr: 0.02\n",
      "iteration: 11000 loss: 0.0030 lr: 0.02\n",
      "iteration: 11100 loss: 0.0040 lr: 0.02\n",
      "iteration: 11200 loss: 0.0037 lr: 0.02\n",
      "iteration: 11300 loss: 0.0038 lr: 0.02\n",
      "iteration: 11400 loss: 0.0040 lr: 0.02\n",
      "iteration: 11500 loss: 0.0036 lr: 0.02\n",
      "iteration: 11600 loss: 0.0034 lr: 0.02\n",
      "iteration: 11700 loss: 0.0034 lr: 0.02\n",
      "iteration: 11800 loss: 0.0037 lr: 0.02\n",
      "iteration: 11900 loss: 0.0039 lr: 0.02\n",
      "iteration: 12000 loss: 0.0036 lr: 0.02\n",
      "iteration: 12100 loss: 0.0038 lr: 0.02\n",
      "iteration: 12200 loss: 0.0035 lr: 0.02\n",
      "iteration: 12300 loss: 0.0034 lr: 0.02\n",
      "iteration: 12400 loss: 0.0037 lr: 0.02\n",
      "iteration: 12500 loss: 0.0037 lr: 0.02\n",
      "iteration: 12600 loss: 0.0034 lr: 0.02\n",
      "iteration: 12700 loss: 0.0030 lr: 0.02\n",
      "iteration: 12800 loss: 0.0030 lr: 0.02\n",
      "iteration: 12900 loss: 0.0034 lr: 0.02\n",
      "iteration: 13000 loss: 0.0038 lr: 0.02\n",
      "iteration: 13100 loss: 0.0035 lr: 0.02\n",
      "iteration: 13200 loss: 0.0028 lr: 0.02\n",
      "iteration: 13300 loss: 0.0032 lr: 0.02\n",
      "iteration: 13400 loss: 0.0028 lr: 0.02\n",
      "iteration: 13500 loss: 0.0033 lr: 0.02\n",
      "iteration: 13600 loss: 0.0032 lr: 0.02\n",
      "iteration: 13700 loss: 0.0032 lr: 0.02\n",
      "iteration: 13800 loss: 0.0028 lr: 0.02\n",
      "iteration: 13900 loss: 0.0036 lr: 0.02\n",
      "iteration: 14000 loss: 0.0030 lr: 0.02\n",
      "iteration: 14100 loss: 0.0036 lr: 0.02\n",
      "iteration: 14200 loss: 0.0031 lr: 0.02\n",
      "iteration: 14300 loss: 0.0030 lr: 0.02\n",
      "iteration: 14400 loss: 0.0033 lr: 0.02\n",
      "iteration: 14500 loss: 0.0039 lr: 0.02\n",
      "iteration: 14600 loss: 0.0034 lr: 0.02\n",
      "iteration: 14700 loss: 0.0036 lr: 0.02\n",
      "iteration: 14800 loss: 0.0032 lr: 0.02\n",
      "iteration: 14900 loss: 0.0029 lr: 0.02\n",
      "iteration: 15000 loss: 0.0029 lr: 0.02\n",
      "iteration: 15100 loss: 0.0028 lr: 0.02\n",
      "iteration: 15200 loss: 0.0035 lr: 0.02\n",
      "iteration: 15300 loss: 0.0030 lr: 0.02\n",
      "iteration: 15400 loss: 0.0031 lr: 0.02\n",
      "iteration: 15500 loss: 0.0035 lr: 0.02\n",
      "iteration: 15600 loss: 0.0030 lr: 0.02\n",
      "iteration: 15700 loss: 0.0035 lr: 0.02\n",
      "iteration: 15800 loss: 0.0034 lr: 0.02\n",
      "iteration: 15900 loss: 0.0031 lr: 0.02\n",
      "iteration: 16000 loss: 0.0031 lr: 0.02\n",
      "iteration: 16100 loss: 0.0030 lr: 0.02\n",
      "iteration: 16200 loss: 0.0030 lr: 0.02\n",
      "iteration: 16300 loss: 0.0029 lr: 0.02\n",
      "iteration: 16400 loss: 0.0033 lr: 0.02\n",
      "iteration: 16500 loss: 0.0033 lr: 0.02\n",
      "iteration: 16600 loss: 0.0028 lr: 0.02\n",
      "iteration: 16700 loss: 0.0029 lr: 0.02\n",
      "iteration: 16800 loss: 0.0024 lr: 0.02\n",
      "iteration: 16900 loss: 0.0029 lr: 0.02\n",
      "iteration: 17000 loss: 0.0027 lr: 0.02\n",
      "iteration: 17100 loss: 0.0031 lr: 0.02\n",
      "iteration: 17200 loss: 0.0029 lr: 0.02\n",
      "iteration: 17300 loss: 0.0036 lr: 0.02\n",
      "iteration: 17400 loss: 0.0028 lr: 0.02\n",
      "iteration: 17500 loss: 0.0029 lr: 0.02\n",
      "iteration: 17600 loss: 0.0033 lr: 0.02\n",
      "iteration: 17700 loss: 0.0027 lr: 0.02\n",
      "iteration: 17800 loss: 0.0031 lr: 0.02\n",
      "iteration: 17900 loss: 0.0031 lr: 0.02\n",
      "iteration: 18000 loss: 0.0028 lr: 0.02\n",
      "iteration: 18100 loss: 0.0029 lr: 0.02\n",
      "iteration: 18200 loss: 0.0028 lr: 0.02\n",
      "iteration: 18300 loss: 0.0034 lr: 0.02\n",
      "iteration: 18400 loss: 0.0028 lr: 0.02\n",
      "iteration: 18500 loss: 0.0031 lr: 0.02\n",
      "iteration: 18600 loss: 0.0026 lr: 0.02\n",
      "iteration: 18700 loss: 0.0031 lr: 0.02\n",
      "iteration: 18800 loss: 0.0026 lr: 0.02\n",
      "iteration: 18900 loss: 0.0027 lr: 0.02\n",
      "iteration: 19000 loss: 0.0027 lr: 0.02\n",
      "iteration: 19100 loss: 0.0025 lr: 0.02\n",
      "iteration: 19200 loss: 0.0031 lr: 0.02\n",
      "iteration: 19300 loss: 0.0029 lr: 0.02\n",
      "iteration: 19400 loss: 0.0024 lr: 0.02\n",
      "iteration: 19500 loss: 0.0026 lr: 0.02\n",
      "iteration: 19600 loss: 0.0028 lr: 0.02\n",
      "iteration: 19700 loss: 0.0029 lr: 0.02\n",
      "iteration: 19800 loss: 0.0027 lr: 0.02\n",
      "iteration: 19900 loss: 0.0026 lr: 0.02\n",
      "iteration: 20000 loss: 0.0030 lr: 0.02\n",
      "iteration: 20100 loss: 0.0027 lr: 0.02\n",
      "iteration: 20200 loss: 0.0027 lr: 0.02\n",
      "iteration: 20300 loss: 0.0026 lr: 0.02\n",
      "iteration: 20400 loss: 0.0028 lr: 0.02\n",
      "iteration: 20500 loss: 0.0031 lr: 0.02\n",
      "iteration: 20600 loss: 0.0033 lr: 0.02\n",
      "iteration: 20700 loss: 0.0029 lr: 0.02\n",
      "iteration: 20800 loss: 0.0025 lr: 0.02\n",
      "iteration: 20900 loss: 0.0028 lr: 0.02\n",
      "iteration: 21000 loss: 0.0028 lr: 0.02\n",
      "iteration: 21100 loss: 0.0024 lr: 0.02\n",
      "iteration: 21200 loss: 0.0027 lr: 0.02\n",
      "iteration: 21300 loss: 0.0025 lr: 0.02\n",
      "iteration: 21400 loss: 0.0027 lr: 0.02\n",
      "iteration: 21500 loss: 0.0027 lr: 0.02\n",
      "iteration: 21600 loss: 0.0025 lr: 0.02\n",
      "iteration: 21700 loss: 0.0025 lr: 0.02\n",
      "iteration: 21800 loss: 0.0026 lr: 0.02\n",
      "iteration: 21900 loss: 0.0027 lr: 0.02\n",
      "iteration: 22000 loss: 0.0025 lr: 0.02\n",
      "iteration: 22100 loss: 0.0029 lr: 0.02\n",
      "iteration: 22200 loss: 0.0027 lr: 0.02\n",
      "iteration: 22300 loss: 0.0024 lr: 0.02\n",
      "iteration: 22400 loss: 0.0025 lr: 0.02\n",
      "iteration: 22500 loss: 0.0026 lr: 0.02\n",
      "iteration: 22600 loss: 0.0031 lr: 0.02\n",
      "iteration: 22700 loss: 0.0026 lr: 0.02\n",
      "iteration: 22800 loss: 0.0027 lr: 0.02\n",
      "iteration: 22900 loss: 0.0025 lr: 0.02\n",
      "iteration: 23000 loss: 0.0026 lr: 0.02\n",
      "iteration: 23100 loss: 0.0028 lr: 0.02\n",
      "iteration: 23200 loss: 0.0024 lr: 0.02\n",
      "iteration: 23300 loss: 0.0022 lr: 0.02\n",
      "iteration: 23400 loss: 0.0025 lr: 0.02\n",
      "iteration: 23500 loss: 0.0026 lr: 0.02\n",
      "iteration: 23600 loss: 0.0028 lr: 0.02\n",
      "iteration: 23700 loss: 0.0025 lr: 0.02\n",
      "iteration: 23800 loss: 0.0026 lr: 0.02\n",
      "iteration: 23900 loss: 0.0023 lr: 0.02\n",
      "iteration: 24000 loss: 0.0024 lr: 0.02\n",
      "iteration: 24100 loss: 0.0023 lr: 0.02\n",
      "iteration: 24200 loss: 0.0026 lr: 0.02\n",
      "iteration: 24300 loss: 0.0023 lr: 0.02\n",
      "iteration: 24400 loss: 0.0023 lr: 0.02\n",
      "iteration: 24500 loss: 0.0025 lr: 0.02\n",
      "iteration: 24600 loss: 0.0028 lr: 0.02\n",
      "iteration: 24700 loss: 0.0030 lr: 0.02\n",
      "iteration: 24800 loss: 0.0026 lr: 0.02\n",
      "iteration: 24900 loss: 0.0024 lr: 0.02\n",
      "iteration: 25000 loss: 0.0028 lr: 0.02\n",
      "iteration: 25100 loss: 0.0025 lr: 0.02\n",
      "iteration: 25200 loss: 0.0029 lr: 0.02\n",
      "iteration: 25300 loss: 0.0027 lr: 0.02\n",
      "iteration: 25400 loss: 0.0025 lr: 0.02\n",
      "iteration: 25500 loss: 0.0025 lr: 0.02\n",
      "iteration: 25600 loss: 0.0026 lr: 0.02\n",
      "iteration: 25700 loss: 0.0024 lr: 0.02\n",
      "iteration: 25800 loss: 0.0026 lr: 0.02\n",
      "iteration: 25900 loss: 0.0027 lr: 0.02\n",
      "iteration: 26000 loss: 0.0025 lr: 0.02\n",
      "iteration: 26100 loss: 0.0026 lr: 0.02\n",
      "iteration: 26200 loss: 0.0024 lr: 0.02\n",
      "iteration: 26300 loss: 0.0028 lr: 0.02\n",
      "iteration: 26400 loss: 0.0025 lr: 0.02\n",
      "iteration: 26500 loss: 0.0024 lr: 0.02\n",
      "iteration: 26600 loss: 0.0024 lr: 0.02\n",
      "iteration: 26700 loss: 0.0023 lr: 0.02\n",
      "iteration: 26800 loss: 0.0023 lr: 0.02\n",
      "iteration: 26900 loss: 0.0023 lr: 0.02\n",
      "iteration: 27000 loss: 0.0026 lr: 0.02\n",
      "iteration: 27100 loss: 0.0025 lr: 0.02\n",
      "iteration: 27200 loss: 0.0024 lr: 0.02\n",
      "iteration: 27300 loss: 0.0023 lr: 0.02\n",
      "iteration: 27400 loss: 0.0023 lr: 0.02\n",
      "iteration: 27500 loss: 0.0025 lr: 0.02\n",
      "iteration: 27600 loss: 0.0022 lr: 0.02\n",
      "iteration: 27700 loss: 0.0021 lr: 0.02\n",
      "iteration: 27800 loss: 0.0028 lr: 0.02\n",
      "iteration: 27900 loss: 0.0022 lr: 0.02\n",
      "iteration: 28000 loss: 0.0024 lr: 0.02\n",
      "iteration: 28100 loss: 0.0028 lr: 0.02\n",
      "iteration: 28200 loss: 0.0028 lr: 0.02\n",
      "iteration: 28300 loss: 0.0023 lr: 0.02\n",
      "iteration: 28400 loss: 0.0025 lr: 0.02\n",
      "iteration: 28500 loss: 0.0024 lr: 0.02\n",
      "iteration: 28600 loss: 0.0021 lr: 0.02\n",
      "iteration: 28700 loss: 0.0022 lr: 0.02\n",
      "iteration: 28800 loss: 0.0025 lr: 0.02\n",
      "iteration: 28900 loss: 0.0026 lr: 0.02\n",
      "iteration: 29000 loss: 0.0023 lr: 0.02\n",
      "iteration: 29100 loss: 0.0019 lr: 0.02\n",
      "iteration: 29200 loss: 0.0024 lr: 0.02\n",
      "iteration: 29300 loss: 0.0024 lr: 0.02\n",
      "iteration: 29400 loss: 0.0021 lr: 0.02\n",
      "iteration: 29500 loss: 0.0023 lr: 0.02\n",
      "iteration: 29600 loss: 0.0024 lr: 0.02\n",
      "iteration: 29700 loss: 0.0023 lr: 0.02\n",
      "iteration: 29800 loss: 0.0023 lr: 0.02\n",
      "iteration: 29900 loss: 0.0023 lr: 0.02\n",
      "iteration: 30000 loss: 0.0027 lr: 0.02\n",
      "iteration: 30100 loss: 0.0027 lr: 0.02\n",
      "iteration: 30200 loss: 0.0028 lr: 0.02\n",
      "iteration: 30300 loss: 0.0024 lr: 0.02\n",
      "iteration: 30400 loss: 0.0024 lr: 0.02\n",
      "iteration: 30500 loss: 0.0021 lr: 0.02\n",
      "iteration: 30600 loss: 0.0027 lr: 0.02\n",
      "iteration: 30700 loss: 0.0022 lr: 0.02\n",
      "iteration: 30800 loss: 0.0028 lr: 0.02\n",
      "iteration: 30900 loss: 0.0022 lr: 0.02\n",
      "iteration: 31000 loss: 0.0023 lr: 0.02\n",
      "iteration: 31100 loss: 0.0020 lr: 0.02\n",
      "iteration: 31200 loss: 0.0022 lr: 0.02\n",
      "iteration: 31300 loss: 0.0020 lr: 0.02\n",
      "iteration: 31400 loss: 0.0022 lr: 0.02\n",
      "iteration: 31500 loss: 0.0023 lr: 0.02\n",
      "iteration: 31600 loss: 0.0021 lr: 0.02\n",
      "iteration: 31700 loss: 0.0021 lr: 0.02\n",
      "iteration: 31800 loss: 0.0022 lr: 0.02\n",
      "iteration: 31900 loss: 0.0020 lr: 0.02\n",
      "iteration: 32000 loss: 0.0022 lr: 0.02\n",
      "iteration: 32100 loss: 0.0022 lr: 0.02\n",
      "iteration: 32200 loss: 0.0025 lr: 0.02\n",
      "iteration: 32300 loss: 0.0023 lr: 0.02\n",
      "iteration: 32400 loss: 0.0023 lr: 0.02\n",
      "iteration: 32500 loss: 0.0021 lr: 0.02\n",
      "iteration: 32600 loss: 0.0020 lr: 0.02\n",
      "iteration: 32700 loss: 0.0024 lr: 0.02\n",
      "iteration: 32800 loss: 0.0019 lr: 0.02\n",
      "iteration: 32900 loss: 0.0020 lr: 0.02\n",
      "iteration: 33000 loss: 0.0019 lr: 0.02\n",
      "iteration: 33100 loss: 0.0020 lr: 0.02\n",
      "iteration: 33200 loss: 0.0021 lr: 0.02\n",
      "iteration: 33300 loss: 0.0020 lr: 0.02\n",
      "iteration: 33400 loss: 0.0023 lr: 0.02\n",
      "iteration: 33500 loss: 0.0022 lr: 0.02\n",
      "iteration: 33600 loss: 0.0021 lr: 0.02\n",
      "iteration: 33700 loss: 0.0021 lr: 0.02\n",
      "iteration: 33800 loss: 0.0021 lr: 0.02\n",
      "iteration: 33900 loss: 0.0022 lr: 0.02\n",
      "iteration: 34000 loss: 0.0022 lr: 0.02\n",
      "iteration: 34100 loss: 0.0018 lr: 0.02\n",
      "iteration: 34200 loss: 0.0021 lr: 0.02\n",
      "iteration: 34300 loss: 0.0026 lr: 0.02\n",
      "iteration: 34400 loss: 0.0020 lr: 0.02\n",
      "iteration: 34500 loss: 0.0021 lr: 0.02\n",
      "iteration: 34600 loss: 0.0025 lr: 0.02\n",
      "iteration: 34700 loss: 0.0021 lr: 0.02\n",
      "iteration: 34800 loss: 0.0023 lr: 0.02\n",
      "iteration: 34900 loss: 0.0022 lr: 0.02\n",
      "iteration: 35000 loss: 0.0019 lr: 0.02\n",
      "iteration: 35100 loss: 0.0024 lr: 0.02\n",
      "iteration: 35200 loss: 0.0020 lr: 0.02\n",
      "iteration: 35300 loss: 0.0022 lr: 0.02\n",
      "iteration: 35400 loss: 0.0022 lr: 0.02\n",
      "iteration: 35500 loss: 0.0022 lr: 0.02\n",
      "iteration: 35600 loss: 0.0022 lr: 0.02\n",
      "iteration: 35700 loss: 0.0018 lr: 0.02\n",
      "iteration: 35800 loss: 0.0023 lr: 0.02\n",
      "iteration: 35900 loss: 0.0020 lr: 0.02\n",
      "iteration: 36000 loss: 0.0022 lr: 0.02\n",
      "iteration: 36100 loss: 0.0020 lr: 0.02\n",
      "iteration: 36200 loss: 0.0019 lr: 0.02\n",
      "iteration: 36300 loss: 0.0021 lr: 0.02\n",
      "iteration: 36400 loss: 0.0021 lr: 0.02\n",
      "iteration: 36500 loss: 0.0020 lr: 0.02\n",
      "iteration: 36600 loss: 0.0021 lr: 0.02\n",
      "iteration: 36700 loss: 0.0020 lr: 0.02\n",
      "iteration: 36800 loss: 0.0026 lr: 0.02\n",
      "iteration: 36900 loss: 0.0020 lr: 0.02\n",
      "iteration: 37000 loss: 0.0022 lr: 0.02\n",
      "iteration: 37100 loss: 0.0020 lr: 0.02\n",
      "iteration: 37200 loss: 0.0021 lr: 0.02\n",
      "iteration: 37300 loss: 0.0022 lr: 0.02\n",
      "iteration: 37400 loss: 0.0023 lr: 0.02\n",
      "iteration: 37500 loss: 0.0019 lr: 0.02\n",
      "iteration: 37600 loss: 0.0021 lr: 0.02\n",
      "iteration: 37700 loss: 0.0020 lr: 0.02\n",
      "iteration: 37800 loss: 0.0019 lr: 0.02\n",
      "iteration: 37900 loss: 0.0022 lr: 0.02\n",
      "iteration: 38000 loss: 0.0020 lr: 0.02\n",
      "iteration: 38100 loss: 0.0023 lr: 0.02\n",
      "iteration: 38200 loss: 0.0022 lr: 0.02\n",
      "iteration: 38300 loss: 0.0021 lr: 0.02\n",
      "iteration: 38400 loss: 0.0018 lr: 0.02\n",
      "iteration: 38500 loss: 0.0020 lr: 0.02\n",
      "iteration: 38600 loss: 0.0021 lr: 0.02\n",
      "iteration: 38700 loss: 0.0021 lr: 0.02\n",
      "iteration: 38800 loss: 0.0021 lr: 0.02\n",
      "iteration: 38900 loss: 0.0021 lr: 0.02\n",
      "iteration: 39000 loss: 0.0019 lr: 0.02\n",
      "iteration: 39100 loss: 0.0022 lr: 0.02\n",
      "iteration: 39200 loss: 0.0020 lr: 0.02\n",
      "iteration: 39300 loss: 0.0022 lr: 0.02\n",
      "iteration: 39400 loss: 0.0018 lr: 0.02\n",
      "iteration: 39500 loss: 0.0022 lr: 0.02\n",
      "iteration: 39600 loss: 0.0022 lr: 0.02\n",
      "iteration: 39700 loss: 0.0019 lr: 0.02\n",
      "iteration: 39800 loss: 0.0019 lr: 0.02\n",
      "iteration: 39900 loss: 0.0021 lr: 0.02\n",
      "iteration: 40000 loss: 0.0020 lr: 0.02\n",
      "iteration: 40100 loss: 0.0021 lr: 0.02\n",
      "iteration: 40200 loss: 0.0018 lr: 0.02\n",
      "iteration: 40300 loss: 0.0020 lr: 0.02\n",
      "iteration: 40400 loss: 0.0020 lr: 0.02\n",
      "iteration: 40500 loss: 0.0022 lr: 0.02\n",
      "iteration: 40600 loss: 0.0018 lr: 0.02\n",
      "iteration: 40700 loss: 0.0020 lr: 0.02\n",
      "iteration: 40800 loss: 0.0020 lr: 0.02\n",
      "iteration: 40900 loss: 0.0020 lr: 0.02\n",
      "iteration: 41000 loss: 0.0019 lr: 0.02\n",
      "iteration: 41100 loss: 0.0023 lr: 0.02\n",
      "iteration: 41200 loss: 0.0022 lr: 0.02\n",
      "iteration: 41300 loss: 0.0020 lr: 0.02\n",
      "iteration: 41400 loss: 0.0022 lr: 0.02\n",
      "iteration: 41500 loss: 0.0022 lr: 0.02\n",
      "iteration: 41600 loss: 0.0020 lr: 0.02\n",
      "iteration: 41700 loss: 0.0022 lr: 0.02\n",
      "iteration: 41800 loss: 0.0019 lr: 0.02\n",
      "iteration: 41900 loss: 0.0019 lr: 0.02\n",
      "iteration: 42000 loss: 0.0022 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 42100 loss: 0.0021 lr: 0.02\n",
      "iteration: 42200 loss: 0.0020 lr: 0.02\n",
      "iteration: 42300 loss: 0.0020 lr: 0.02\n",
      "iteration: 42400 loss: 0.0020 lr: 0.02\n",
      "iteration: 42500 loss: 0.0022 lr: 0.02\n",
      "iteration: 42600 loss: 0.0019 lr: 0.02\n",
      "iteration: 42700 loss: 0.0020 lr: 0.02\n",
      "iteration: 42800 loss: 0.0020 lr: 0.02\n",
      "iteration: 42900 loss: 0.0021 lr: 0.02\n",
      "iteration: 43000 loss: 0.0021 lr: 0.02\n",
      "iteration: 43100 loss: 0.0024 lr: 0.02\n",
      "iteration: 43200 loss: 0.0020 lr: 0.02\n",
      "iteration: 43300 loss: 0.0022 lr: 0.02\n",
      "iteration: 43400 loss: 0.0019 lr: 0.02\n",
      "iteration: 43500 loss: 0.0021 lr: 0.02\n",
      "iteration: 43600 loss: 0.0019 lr: 0.02\n",
      "iteration: 43700 loss: 0.0021 lr: 0.02\n",
      "iteration: 43800 loss: 0.0019 lr: 0.02\n",
      "iteration: 43900 loss: 0.0019 lr: 0.02\n",
      "iteration: 44000 loss: 0.0022 lr: 0.02\n",
      "iteration: 44100 loss: 0.0019 lr: 0.02\n",
      "iteration: 44200 loss: 0.0017 lr: 0.02\n",
      "iteration: 44300 loss: 0.0019 lr: 0.02\n",
      "iteration: 44400 loss: 0.0022 lr: 0.02\n",
      "iteration: 44500 loss: 0.0020 lr: 0.02\n",
      "iteration: 44600 loss: 0.0017 lr: 0.02\n",
      "iteration: 44700 loss: 0.0022 lr: 0.02\n",
      "iteration: 44800 loss: 0.0019 lr: 0.02\n",
      "iteration: 44900 loss: 0.0020 lr: 0.02\n",
      "iteration: 45000 loss: 0.0020 lr: 0.02\n",
      "iteration: 45100 loss: 0.0017 lr: 0.02\n",
      "iteration: 45200 loss: 0.0017 lr: 0.02\n",
      "iteration: 45300 loss: 0.0020 lr: 0.02\n",
      "iteration: 45400 loss: 0.0019 lr: 0.02\n",
      "iteration: 45500 loss: 0.0019 lr: 0.02\n",
      "iteration: 45600 loss: 0.0017 lr: 0.02\n",
      "iteration: 45700 loss: 0.0019 lr: 0.02\n",
      "iteration: 45800 loss: 0.0023 lr: 0.02\n",
      "iteration: 45900 loss: 0.0019 lr: 0.02\n",
      "iteration: 46000 loss: 0.0018 lr: 0.02\n",
      "iteration: 46100 loss: 0.0018 lr: 0.02\n",
      "iteration: 46200 loss: 0.0020 lr: 0.02\n",
      "iteration: 46300 loss: 0.0021 lr: 0.02\n",
      "iteration: 46400 loss: 0.0020 lr: 0.02\n",
      "iteration: 46500 loss: 0.0021 lr: 0.02\n",
      "iteration: 46600 loss: 0.0020 lr: 0.02\n",
      "iteration: 46700 loss: 0.0021 lr: 0.02\n",
      "iteration: 46800 loss: 0.0022 lr: 0.02\n",
      "iteration: 46900 loss: 0.0020 lr: 0.02\n",
      "iteration: 47000 loss: 0.0021 lr: 0.02\n",
      "iteration: 47100 loss: 0.0018 lr: 0.02\n",
      "iteration: 47200 loss: 0.0022 lr: 0.02\n",
      "iteration: 47300 loss: 0.0022 lr: 0.02\n",
      "iteration: 47400 loss: 0.0019 lr: 0.02\n",
      "iteration: 47500 loss: 0.0020 lr: 0.02\n",
      "iteration: 47600 loss: 0.0021 lr: 0.02\n",
      "iteration: 47700 loss: 0.0023 lr: 0.02\n",
      "iteration: 47800 loss: 0.0018 lr: 0.02\n",
      "iteration: 47900 loss: 0.0018 lr: 0.02\n",
      "iteration: 48000 loss: 0.0019 lr: 0.02\n",
      "iteration: 48100 loss: 0.0019 lr: 0.02\n",
      "iteration: 48200 loss: 0.0020 lr: 0.02\n",
      "iteration: 48300 loss: 0.0017 lr: 0.02\n",
      "iteration: 48400 loss: 0.0019 lr: 0.02\n",
      "iteration: 48500 loss: 0.0021 lr: 0.02\n",
      "iteration: 48600 loss: 0.0019 lr: 0.02\n",
      "iteration: 48700 loss: 0.0019 lr: 0.02\n",
      "iteration: 48800 loss: 0.0019 lr: 0.02\n",
      "iteration: 48900 loss: 0.0020 lr: 0.02\n",
      "iteration: 49000 loss: 0.0019 lr: 0.02\n",
      "iteration: 49100 loss: 0.0020 lr: 0.02\n",
      "iteration: 49200 loss: 0.0018 lr: 0.02\n",
      "iteration: 49300 loss: 0.0019 lr: 0.02\n",
      "iteration: 49400 loss: 0.0020 lr: 0.02\n",
      "iteration: 49500 loss: 0.0018 lr: 0.02\n",
      "iteration: 49600 loss: 0.0020 lr: 0.02\n",
      "iteration: 49700 loss: 0.0016 lr: 0.02\n",
      "iteration: 49800 loss: 0.0017 lr: 0.02\n",
      "iteration: 49900 loss: 0.0017 lr: 0.02\n",
      "iteration: 50000 loss: 0.0018 lr: 0.02\n",
      "iteration: 50100 loss: 0.0015 lr: 0.02\n",
      "iteration: 50200 loss: 0.0017 lr: 0.02\n",
      "iteration: 50300 loss: 0.0019 lr: 0.02\n",
      "iteration: 50400 loss: 0.0021 lr: 0.02\n",
      "iteration: 50500 loss: 0.0021 lr: 0.02\n",
      "iteration: 50600 loss: 0.0022 lr: 0.02\n",
      "iteration: 50700 loss: 0.0018 lr: 0.02\n",
      "iteration: 50800 loss: 0.0016 lr: 0.02\n",
      "iteration: 50900 loss: 0.0018 lr: 0.02\n",
      "iteration: 51000 loss: 0.0019 lr: 0.02\n",
      "iteration: 51100 loss: 0.0018 lr: 0.02\n",
      "iteration: 51200 loss: 0.0017 lr: 0.02\n",
      "iteration: 51300 loss: 0.0019 lr: 0.02\n",
      "iteration: 51400 loss: 0.0018 lr: 0.02\n",
      "iteration: 51500 loss: 0.0018 lr: 0.02\n",
      "iteration: 51600 loss: 0.0018 lr: 0.02\n",
      "iteration: 51700 loss: 0.0019 lr: 0.02\n",
      "iteration: 51800 loss: 0.0016 lr: 0.02\n",
      "iteration: 51900 loss: 0.0020 lr: 0.02\n",
      "iteration: 52000 loss: 0.0018 lr: 0.02\n",
      "iteration: 52100 loss: 0.0019 lr: 0.02\n",
      "iteration: 52200 loss: 0.0020 lr: 0.02\n",
      "iteration: 52300 loss: 0.0018 lr: 0.02\n",
      "iteration: 52400 loss: 0.0019 lr: 0.02\n",
      "iteration: 52500 loss: 0.0016 lr: 0.02\n",
      "iteration: 52600 loss: 0.0018 lr: 0.02\n",
      "iteration: 52700 loss: 0.0019 lr: 0.02\n",
      "iteration: 52800 loss: 0.0019 lr: 0.02\n",
      "iteration: 52900 loss: 0.0017 lr: 0.02\n",
      "iteration: 53000 loss: 0.0020 lr: 0.02\n",
      "iteration: 53100 loss: 0.0019 lr: 0.02\n",
      "iteration: 53200 loss: 0.0019 lr: 0.02\n",
      "iteration: 53300 loss: 0.0017 lr: 0.02\n",
      "iteration: 53400 loss: 0.0018 lr: 0.02\n",
      "iteration: 53500 loss: 0.0020 lr: 0.02\n",
      "iteration: 53600 loss: 0.0019 lr: 0.02\n",
      "iteration: 53700 loss: 0.0019 lr: 0.02\n",
      "iteration: 53800 loss: 0.0019 lr: 0.02\n",
      "iteration: 53900 loss: 0.0019 lr: 0.02\n",
      "iteration: 54000 loss: 0.0018 lr: 0.02\n",
      "iteration: 54100 loss: 0.0018 lr: 0.02\n",
      "iteration: 54200 loss: 0.0019 lr: 0.02\n",
      "iteration: 54300 loss: 0.0017 lr: 0.02\n",
      "iteration: 54400 loss: 0.0019 lr: 0.02\n",
      "iteration: 54500 loss: 0.0018 lr: 0.02\n",
      "iteration: 54600 loss: 0.0018 lr: 0.02\n",
      "iteration: 54700 loss: 0.0018 lr: 0.02\n",
      "iteration: 54800 loss: 0.0017 lr: 0.02\n",
      "iteration: 54900 loss: 0.0016 lr: 0.02\n",
      "iteration: 55000 loss: 0.0018 lr: 0.02\n",
      "iteration: 55100 loss: 0.0017 lr: 0.02\n",
      "iteration: 55200 loss: 0.0018 lr: 0.02\n",
      "iteration: 55300 loss: 0.0017 lr: 0.02\n",
      "iteration: 55400 loss: 0.0017 lr: 0.02\n",
      "iteration: 55500 loss: 0.0020 lr: 0.02\n",
      "iteration: 55600 loss: 0.0019 lr: 0.02\n",
      "iteration: 55700 loss: 0.0021 lr: 0.02\n",
      "iteration: 55800 loss: 0.0015 lr: 0.02\n",
      "iteration: 55900 loss: 0.0016 lr: 0.02\n",
      "iteration: 56000 loss: 0.0017 lr: 0.02\n",
      "iteration: 56100 loss: 0.0020 lr: 0.02\n",
      "iteration: 56200 loss: 0.0017 lr: 0.02\n",
      "iteration: 56300 loss: 0.0018 lr: 0.02\n",
      "iteration: 56400 loss: 0.0017 lr: 0.02\n",
      "iteration: 56500 loss: 0.0016 lr: 0.02\n",
      "iteration: 56600 loss: 0.0017 lr: 0.02\n",
      "iteration: 56700 loss: 0.0017 lr: 0.02\n",
      "iteration: 56800 loss: 0.0018 lr: 0.02\n",
      "iteration: 56900 loss: 0.0020 lr: 0.02\n",
      "iteration: 57000 loss: 0.0016 lr: 0.02\n",
      "iteration: 57100 loss: 0.0019 lr: 0.02\n",
      "iteration: 57200 loss: 0.0017 lr: 0.02\n",
      "iteration: 57300 loss: 0.0018 lr: 0.02\n",
      "iteration: 57400 loss: 0.0018 lr: 0.02\n",
      "iteration: 57500 loss: 0.0020 lr: 0.02\n",
      "iteration: 57600 loss: 0.0017 lr: 0.02\n",
      "iteration: 57700 loss: 0.0016 lr: 0.02\n",
      "iteration: 57800 loss: 0.0019 lr: 0.02\n",
      "iteration: 57900 loss: 0.0020 lr: 0.02\n",
      "iteration: 58000 loss: 0.0020 lr: 0.02\n",
      "iteration: 58100 loss: 0.0018 lr: 0.02\n",
      "iteration: 58200 loss: 0.0019 lr: 0.02\n",
      "iteration: 58300 loss: 0.0020 lr: 0.02\n",
      "iteration: 58400 loss: 0.0019 lr: 0.02\n",
      "iteration: 58500 loss: 0.0017 lr: 0.02\n",
      "iteration: 58600 loss: 0.0018 lr: 0.02\n",
      "iteration: 58700 loss: 0.0018 lr: 0.02\n",
      "iteration: 58800 loss: 0.0018 lr: 0.02\n",
      "iteration: 58900 loss: 0.0016 lr: 0.02\n",
      "iteration: 59000 loss: 0.0016 lr: 0.02\n",
      "iteration: 59100 loss: 0.0021 lr: 0.02\n",
      "iteration: 59200 loss: 0.0017 lr: 0.02\n",
      "iteration: 59300 loss: 0.0018 lr: 0.02\n",
      "iteration: 59400 loss: 0.0018 lr: 0.02\n",
      "iteration: 59500 loss: 0.0019 lr: 0.02\n",
      "iteration: 59600 loss: 0.0018 lr: 0.02\n",
      "iteration: 59700 loss: 0.0017 lr: 0.02\n",
      "iteration: 59800 loss: 0.0019 lr: 0.02\n",
      "iteration: 59900 loss: 0.0020 lr: 0.02\n",
      "iteration: 60000 loss: 0.0017 lr: 0.02\n",
      "iteration: 60100 loss: 0.0018 lr: 0.02\n",
      "iteration: 60200 loss: 0.0020 lr: 0.02\n",
      "iteration: 60300 loss: 0.0019 lr: 0.02\n",
      "iteration: 60400 loss: 0.0017 lr: 0.02\n",
      "iteration: 60500 loss: 0.0019 lr: 0.02\n",
      "iteration: 60600 loss: 0.0019 lr: 0.02\n",
      "iteration: 60700 loss: 0.0017 lr: 0.02\n",
      "iteration: 60800 loss: 0.0019 lr: 0.02\n",
      "iteration: 60900 loss: 0.0018 lr: 0.02\n",
      "iteration: 61000 loss: 0.0016 lr: 0.02\n",
      "iteration: 61100 loss: 0.0018 lr: 0.02\n",
      "iteration: 61200 loss: 0.0018 lr: 0.02\n",
      "iteration: 61300 loss: 0.0018 lr: 0.02\n",
      "iteration: 61400 loss: 0.0017 lr: 0.02\n",
      "iteration: 61500 loss: 0.0016 lr: 0.02\n",
      "iteration: 61600 loss: 0.0016 lr: 0.02\n",
      "iteration: 61700 loss: 0.0016 lr: 0.02\n",
      "iteration: 61800 loss: 0.0016 lr: 0.02\n",
      "iteration: 61900 loss: 0.0016 lr: 0.02\n",
      "iteration: 62000 loss: 0.0017 lr: 0.02\n",
      "iteration: 62100 loss: 0.0019 lr: 0.02\n",
      "iteration: 62200 loss: 0.0018 lr: 0.02\n",
      "iteration: 62300 loss: 0.0017 lr: 0.02\n",
      "iteration: 62400 loss: 0.0017 lr: 0.02\n",
      "iteration: 62500 loss: 0.0019 lr: 0.02\n",
      "iteration: 62600 loss: 0.0017 lr: 0.02\n",
      "iteration: 62700 loss: 0.0019 lr: 0.02\n",
      "iteration: 62800 loss: 0.0020 lr: 0.02\n",
      "iteration: 62900 loss: 0.0017 lr: 0.02\n",
      "iteration: 63000 loss: 0.0016 lr: 0.02\n",
      "iteration: 63100 loss: 0.0015 lr: 0.02\n",
      "iteration: 63200 loss: 0.0013 lr: 0.02\n",
      "iteration: 63300 loss: 0.0018 lr: 0.02\n",
      "iteration: 63400 loss: 0.0018 lr: 0.02\n",
      "iteration: 63500 loss: 0.0018 lr: 0.02\n",
      "iteration: 63600 loss: 0.0018 lr: 0.02\n",
      "iteration: 63700 loss: 0.0019 lr: 0.02\n",
      "iteration: 63800 loss: 0.0017 lr: 0.02\n",
      "iteration: 63900 loss: 0.0017 lr: 0.02\n",
      "iteration: 64000 loss: 0.0016 lr: 0.02\n",
      "iteration: 64100 loss: 0.0014 lr: 0.02\n",
      "iteration: 64200 loss: 0.0015 lr: 0.02\n",
      "iteration: 64300 loss: 0.0017 lr: 0.02\n",
      "iteration: 64400 loss: 0.0017 lr: 0.02\n",
      "iteration: 64500 loss: 0.0017 lr: 0.02\n",
      "iteration: 64600 loss: 0.0018 lr: 0.02\n",
      "iteration: 64700 loss: 0.0016 lr: 0.02\n",
      "iteration: 64800 loss: 0.0018 lr: 0.02\n",
      "iteration: 64900 loss: 0.0016 lr: 0.02\n",
      "iteration: 65000 loss: 0.0016 lr: 0.02\n",
      "iteration: 65100 loss: 0.0020 lr: 0.02\n",
      "iteration: 65200 loss: 0.0018 lr: 0.02\n",
      "iteration: 65300 loss: 0.0017 lr: 0.02\n",
      "iteration: 65400 loss: 0.0017 lr: 0.02\n",
      "iteration: 65500 loss: 0.0018 lr: 0.02\n",
      "iteration: 65600 loss: 0.0018 lr: 0.02\n",
      "iteration: 65700 loss: 0.0017 lr: 0.02\n",
      "iteration: 65800 loss: 0.0017 lr: 0.02\n",
      "iteration: 65900 loss: 0.0016 lr: 0.02\n",
      "iteration: 66000 loss: 0.0018 lr: 0.02\n",
      "iteration: 66100 loss: 0.0017 lr: 0.02\n",
      "iteration: 66200 loss: 0.0019 lr: 0.02\n",
      "iteration: 66300 loss: 0.0017 lr: 0.02\n",
      "iteration: 66400 loss: 0.0019 lr: 0.02\n",
      "iteration: 66500 loss: 0.0018 lr: 0.02\n",
      "iteration: 66600 loss: 0.0017 lr: 0.02\n",
      "iteration: 66700 loss: 0.0017 lr: 0.02\n",
      "iteration: 66800 loss: 0.0017 lr: 0.02\n",
      "iteration: 66900 loss: 0.0017 lr: 0.02\n",
      "iteration: 67000 loss: 0.0018 lr: 0.02\n",
      "iteration: 67100 loss: 0.0017 lr: 0.02\n",
      "iteration: 67200 loss: 0.0019 lr: 0.02\n",
      "iteration: 67300 loss: 0.0017 lr: 0.02\n",
      "iteration: 67400 loss: 0.0016 lr: 0.02\n",
      "iteration: 67500 loss: 0.0015 lr: 0.02\n",
      "iteration: 67600 loss: 0.0017 lr: 0.02\n",
      "iteration: 67700 loss: 0.0016 lr: 0.02\n",
      "iteration: 67800 loss: 0.0016 lr: 0.02\n",
      "iteration: 67900 loss: 0.0017 lr: 0.02\n",
      "iteration: 68000 loss: 0.0017 lr: 0.02\n",
      "iteration: 68100 loss: 0.0018 lr: 0.02\n",
      "iteration: 68200 loss: 0.0015 lr: 0.02\n",
      "iteration: 68300 loss: 0.0017 lr: 0.02\n",
      "iteration: 68400 loss: 0.0016 lr: 0.02\n",
      "iteration: 68500 loss: 0.0016 lr: 0.02\n",
      "iteration: 68600 loss: 0.0018 lr: 0.02\n",
      "iteration: 68700 loss: 0.0017 lr: 0.02\n",
      "iteration: 68800 loss: 0.0018 lr: 0.02\n",
      "iteration: 68900 loss: 0.0018 lr: 0.02\n",
      "iteration: 69000 loss: 0.0016 lr: 0.02\n",
      "iteration: 69100 loss: 0.0016 lr: 0.02\n",
      "iteration: 69200 loss: 0.0018 lr: 0.02\n",
      "iteration: 69300 loss: 0.0017 lr: 0.02\n",
      "iteration: 69400 loss: 0.0016 lr: 0.02\n",
      "iteration: 69500 loss: 0.0014 lr: 0.02\n",
      "iteration: 69600 loss: 0.0018 lr: 0.02\n",
      "iteration: 69700 loss: 0.0016 lr: 0.02\n",
      "iteration: 69800 loss: 0.0017 lr: 0.02\n",
      "iteration: 69900 loss: 0.0015 lr: 0.02\n",
      "iteration: 70000 loss: 0.0017 lr: 0.02\n",
      "iteration: 70100 loss: 0.0016 lr: 0.02\n",
      "iteration: 70200 loss: 0.0015 lr: 0.02\n",
      "iteration: 70300 loss: 0.0018 lr: 0.02\n",
      "iteration: 70400 loss: 0.0017 lr: 0.02\n",
      "iteration: 70500 loss: 0.0015 lr: 0.02\n",
      "iteration: 70600 loss: 0.0016 lr: 0.02\n",
      "iteration: 70700 loss: 0.0019 lr: 0.02\n",
      "iteration: 70800 loss: 0.0016 lr: 0.02\n",
      "iteration: 70900 loss: 0.0016 lr: 0.02\n",
      "iteration: 71000 loss: 0.0016 lr: 0.02\n",
      "iteration: 71100 loss: 0.0019 lr: 0.02\n",
      "iteration: 71200 loss: 0.0019 lr: 0.02\n",
      "iteration: 71300 loss: 0.0016 lr: 0.02\n",
      "iteration: 71400 loss: 0.0016 lr: 0.02\n",
      "iteration: 71500 loss: 0.0018 lr: 0.02\n",
      "iteration: 71600 loss: 0.0016 lr: 0.02\n",
      "iteration: 71700 loss: 0.0016 lr: 0.02\n",
      "iteration: 71800 loss: 0.0019 lr: 0.02\n",
      "iteration: 71900 loss: 0.0015 lr: 0.02\n",
      "iteration: 72000 loss: 0.0016 lr: 0.02\n",
      "iteration: 72100 loss: 0.0018 lr: 0.02\n",
      "iteration: 72200 loss: 0.0016 lr: 0.02\n",
      "iteration: 72300 loss: 0.0018 lr: 0.02\n",
      "iteration: 72400 loss: 0.0016 lr: 0.02\n",
      "iteration: 72500 loss: 0.0015 lr: 0.02\n",
      "iteration: 72600 loss: 0.0018 lr: 0.02\n",
      "iteration: 72700 loss: 0.0013 lr: 0.02\n",
      "iteration: 72800 loss: 0.0017 lr: 0.02\n",
      "iteration: 72900 loss: 0.0016 lr: 0.02\n",
      "iteration: 73000 loss: 0.0016 lr: 0.02\n",
      "iteration: 73100 loss: 0.0015 lr: 0.02\n",
      "iteration: 73200 loss: 0.0014 lr: 0.02\n",
      "iteration: 73300 loss: 0.0016 lr: 0.02\n",
      "iteration: 73400 loss: 0.0017 lr: 0.02\n",
      "iteration: 73500 loss: 0.0016 lr: 0.02\n",
      "iteration: 73600 loss: 0.0015 lr: 0.02\n",
      "iteration: 73700 loss: 0.0017 lr: 0.02\n",
      "iteration: 73800 loss: 0.0015 lr: 0.02\n",
      "iteration: 73900 loss: 0.0016 lr: 0.02\n",
      "iteration: 74000 loss: 0.0015 lr: 0.02\n",
      "iteration: 74100 loss: 0.0014 lr: 0.02\n",
      "iteration: 74200 loss: 0.0015 lr: 0.02\n",
      "iteration: 74300 loss: 0.0018 lr: 0.02\n",
      "iteration: 74400 loss: 0.0017 lr: 0.02\n",
      "iteration: 74500 loss: 0.0015 lr: 0.02\n",
      "iteration: 74600 loss: 0.0017 lr: 0.02\n",
      "iteration: 74700 loss: 0.0018 lr: 0.02\n",
      "iteration: 74800 loss: 0.0016 lr: 0.02\n",
      "iteration: 74900 loss: 0.0018 lr: 0.02\n",
      "iteration: 75000 loss: 0.0014 lr: 0.02\n",
      "iteration: 75100 loss: 0.0014 lr: 0.02\n",
      "iteration: 75200 loss: 0.0017 lr: 0.02\n",
      "iteration: 75300 loss: 0.0017 lr: 0.02\n",
      "iteration: 75400 loss: 0.0014 lr: 0.02\n",
      "iteration: 75500 loss: 0.0016 lr: 0.02\n",
      "iteration: 75600 loss: 0.0015 lr: 0.02\n",
      "iteration: 75700 loss: 0.0015 lr: 0.02\n",
      "iteration: 75800 loss: 0.0017 lr: 0.02\n",
      "iteration: 75900 loss: 0.0015 lr: 0.02\n",
      "iteration: 76000 loss: 0.0016 lr: 0.02\n",
      "iteration: 76100 loss: 0.0016 lr: 0.02\n",
      "iteration: 76200 loss: 0.0016 lr: 0.02\n",
      "iteration: 76300 loss: 0.0018 lr: 0.02\n",
      "iteration: 76400 loss: 0.0018 lr: 0.02\n",
      "iteration: 76500 loss: 0.0019 lr: 0.02\n",
      "iteration: 76600 loss: 0.0019 lr: 0.02\n",
      "iteration: 76700 loss: 0.0019 lr: 0.02\n",
      "iteration: 76800 loss: 0.0015 lr: 0.02\n",
      "iteration: 76900 loss: 0.0016 lr: 0.02\n",
      "iteration: 77000 loss: 0.0016 lr: 0.02\n",
      "iteration: 77100 loss: 0.0017 lr: 0.02\n",
      "iteration: 77200 loss: 0.0017 lr: 0.02\n",
      "iteration: 77300 loss: 0.0016 lr: 0.02\n",
      "iteration: 77400 loss: 0.0018 lr: 0.02\n",
      "iteration: 77500 loss: 0.0015 lr: 0.02\n",
      "iteration: 77600 loss: 0.0018 lr: 0.02\n",
      "iteration: 77700 loss: 0.0017 lr: 0.02\n",
      "iteration: 77800 loss: 0.0014 lr: 0.02\n",
      "iteration: 77900 loss: 0.0017 lr: 0.02\n",
      "iteration: 78000 loss: 0.0015 lr: 0.02\n",
      "iteration: 78100 loss: 0.0016 lr: 0.02\n",
      "iteration: 78200 loss: 0.0018 lr: 0.02\n",
      "iteration: 78300 loss: 0.0015 lr: 0.02\n",
      "iteration: 78400 loss: 0.0018 lr: 0.02\n",
      "iteration: 78500 loss: 0.0016 lr: 0.02\n",
      "iteration: 78600 loss: 0.0017 lr: 0.02\n",
      "iteration: 78700 loss: 0.0015 lr: 0.02\n",
      "iteration: 78800 loss: 0.0015 lr: 0.02\n",
      "iteration: 78900 loss: 0.0016 lr: 0.02\n",
      "iteration: 79000 loss: 0.0014 lr: 0.02\n",
      "iteration: 79100 loss: 0.0016 lr: 0.02\n",
      "iteration: 79200 loss: 0.0017 lr: 0.02\n",
      "iteration: 79300 loss: 0.0020 lr: 0.02\n",
      "iteration: 79400 loss: 0.0015 lr: 0.02\n",
      "iteration: 79500 loss: 0.0018 lr: 0.02\n",
      "iteration: 79600 loss: 0.0016 lr: 0.02\n",
      "iteration: 79700 loss: 0.0018 lr: 0.02\n",
      "iteration: 79800 loss: 0.0015 lr: 0.02\n",
      "iteration: 79900 loss: 0.0014 lr: 0.02\n",
      "iteration: 80000 loss: 0.0014 lr: 0.02\n",
      "iteration: 80100 loss: 0.0017 lr: 0.02\n",
      "iteration: 80200 loss: 0.0018 lr: 0.02\n",
      "iteration: 80300 loss: 0.0017 lr: 0.02\n",
      "iteration: 80400 loss: 0.0015 lr: 0.02\n",
      "iteration: 80500 loss: 0.0016 lr: 0.02\n",
      "iteration: 80600 loss: 0.0015 lr: 0.02\n",
      "iteration: 80700 loss: 0.0016 lr: 0.02\n",
      "iteration: 80800 loss: 0.0015 lr: 0.02\n",
      "iteration: 80900 loss: 0.0018 lr: 0.02\n",
      "iteration: 81000 loss: 0.0017 lr: 0.02\n",
      "iteration: 81100 loss: 0.0016 lr: 0.02\n",
      "iteration: 81200 loss: 0.0015 lr: 0.02\n",
      "iteration: 81300 loss: 0.0015 lr: 0.02\n",
      "iteration: 81400 loss: 0.0017 lr: 0.02\n",
      "iteration: 81500 loss: 0.0014 lr: 0.02\n",
      "iteration: 81600 loss: 0.0015 lr: 0.02\n",
      "iteration: 81700 loss: 0.0016 lr: 0.02\n",
      "iteration: 81800 loss: 0.0016 lr: 0.02\n",
      "iteration: 81900 loss: 0.0014 lr: 0.02\n",
      "iteration: 82000 loss: 0.0013 lr: 0.02\n",
      "iteration: 82100 loss: 0.0016 lr: 0.02\n",
      "iteration: 82200 loss: 0.0014 lr: 0.02\n",
      "iteration: 82300 loss: 0.0016 lr: 0.02\n",
      "iteration: 82400 loss: 0.0013 lr: 0.02\n",
      "iteration: 82500 loss: 0.0017 lr: 0.02\n",
      "iteration: 82600 loss: 0.0015 lr: 0.02\n",
      "iteration: 82700 loss: 0.0018 lr: 0.02\n",
      "iteration: 82800 loss: 0.0015 lr: 0.02\n",
      "iteration: 82900 loss: 0.0016 lr: 0.02\n",
      "iteration: 83000 loss: 0.0015 lr: 0.02\n",
      "iteration: 83100 loss: 0.0015 lr: 0.02\n",
      "iteration: 83200 loss: 0.0017 lr: 0.02\n",
      "iteration: 83300 loss: 0.0015 lr: 0.02\n",
      "iteration: 83400 loss: 0.0013 lr: 0.02\n",
      "iteration: 83500 loss: 0.0014 lr: 0.02\n",
      "iteration: 83600 loss: 0.0016 lr: 0.02\n",
      "iteration: 83700 loss: 0.0015 lr: 0.02\n",
      "iteration: 83800 loss: 0.0016 lr: 0.02\n",
      "iteration: 83900 loss: 0.0015 lr: 0.02\n",
      "iteration: 84000 loss: 0.0017 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 84100 loss: 0.0019 lr: 0.02\n",
      "iteration: 84200 loss: 0.0016 lr: 0.02\n",
      "iteration: 84300 loss: 0.0015 lr: 0.02\n",
      "iteration: 84400 loss: 0.0014 lr: 0.02\n",
      "iteration: 84500 loss: 0.0015 lr: 0.02\n",
      "iteration: 84600 loss: 0.0016 lr: 0.02\n",
      "iteration: 84700 loss: 0.0013 lr: 0.02\n",
      "iteration: 84800 loss: 0.0015 lr: 0.02\n",
      "iteration: 84900 loss: 0.0015 lr: 0.02\n",
      "iteration: 85000 loss: 0.0014 lr: 0.02\n",
      "iteration: 85100 loss: 0.0014 lr: 0.02\n",
      "iteration: 85200 loss: 0.0017 lr: 0.02\n",
      "iteration: 85300 loss: 0.0017 lr: 0.02\n",
      "iteration: 85400 loss: 0.0015 lr: 0.02\n",
      "iteration: 85500 loss: 0.0016 lr: 0.02\n",
      "iteration: 85600 loss: 0.0017 lr: 0.02\n",
      "iteration: 85700 loss: 0.0017 lr: 0.02\n",
      "iteration: 85800 loss: 0.0014 lr: 0.02\n",
      "iteration: 85900 loss: 0.0016 lr: 0.02\n",
      "iteration: 86000 loss: 0.0014 lr: 0.02\n",
      "iteration: 86100 loss: 0.0017 lr: 0.02\n",
      "iteration: 86200 loss: 0.0016 lr: 0.02\n",
      "iteration: 86300 loss: 0.0015 lr: 0.02\n",
      "iteration: 86400 loss: 0.0015 lr: 0.02\n",
      "iteration: 86500 loss: 0.0018 lr: 0.02\n",
      "iteration: 86600 loss: 0.0016 lr: 0.02\n",
      "iteration: 86700 loss: 0.0015 lr: 0.02\n",
      "iteration: 86800 loss: 0.0016 lr: 0.02\n",
      "iteration: 86900 loss: 0.0017 lr: 0.02\n",
      "iteration: 87000 loss: 0.0016 lr: 0.02\n",
      "iteration: 87100 loss: 0.0015 lr: 0.02\n",
      "iteration: 87200 loss: 0.0015 lr: 0.02\n",
      "iteration: 87300 loss: 0.0016 lr: 0.02\n",
      "iteration: 87400 loss: 0.0016 lr: 0.02\n",
      "iteration: 87500 loss: 0.0017 lr: 0.02\n",
      "iteration: 87600 loss: 0.0014 lr: 0.02\n",
      "iteration: 87700 loss: 0.0017 lr: 0.02\n",
      "iteration: 87800 loss: 0.0012 lr: 0.02\n",
      "iteration: 87900 loss: 0.0016 lr: 0.02\n",
      "iteration: 88000 loss: 0.0015 lr: 0.02\n",
      "iteration: 88100 loss: 0.0015 lr: 0.02\n",
      "iteration: 88200 loss: 0.0017 lr: 0.02\n",
      "iteration: 88300 loss: 0.0015 lr: 0.02\n",
      "iteration: 88400 loss: 0.0015 lr: 0.02\n",
      "iteration: 88500 loss: 0.0015 lr: 0.02\n",
      "iteration: 88600 loss: 0.0021 lr: 0.02\n",
      "iteration: 88700 loss: 0.0015 lr: 0.02\n",
      "iteration: 88800 loss: 0.0015 lr: 0.02\n",
      "iteration: 88900 loss: 0.0015 lr: 0.02\n",
      "iteration: 89000 loss: 0.0015 lr: 0.02\n",
      "iteration: 89100 loss: 0.0016 lr: 0.02\n",
      "iteration: 89200 loss: 0.0015 lr: 0.02\n",
      "iteration: 89300 loss: 0.0016 lr: 0.02\n",
      "iteration: 89400 loss: 0.0015 lr: 0.02\n",
      "iteration: 89500 loss: 0.0015 lr: 0.02\n",
      "iteration: 89600 loss: 0.0017 lr: 0.02\n",
      "iteration: 89700 loss: 0.0013 lr: 0.02\n",
      "iteration: 89800 loss: 0.0017 lr: 0.02\n",
      "iteration: 89900 loss: 0.0017 lr: 0.02\n",
      "iteration: 90000 loss: 0.0016 lr: 0.02\n",
      "iteration: 90100 loss: 0.0015 lr: 0.02\n",
      "iteration: 90200 loss: 0.0015 lr: 0.02\n",
      "iteration: 90300 loss: 0.0013 lr: 0.02\n",
      "iteration: 90400 loss: 0.0014 lr: 0.02\n",
      "iteration: 90500 loss: 0.0013 lr: 0.02\n",
      "iteration: 90600 loss: 0.0014 lr: 0.02\n",
      "iteration: 90700 loss: 0.0015 lr: 0.02\n",
      "iteration: 90800 loss: 0.0014 lr: 0.02\n",
      "iteration: 90900 loss: 0.0015 lr: 0.02\n",
      "iteration: 91000 loss: 0.0014 lr: 0.02\n",
      "iteration: 91100 loss: 0.0016 lr: 0.02\n",
      "iteration: 91200 loss: 0.0014 lr: 0.02\n",
      "iteration: 91300 loss: 0.0015 lr: 0.02\n",
      "iteration: 91400 loss: 0.0017 lr: 0.02\n",
      "iteration: 91500 loss: 0.0015 lr: 0.02\n",
      "iteration: 91600 loss: 0.0015 lr: 0.02\n",
      "iteration: 91700 loss: 0.0016 lr: 0.02\n",
      "iteration: 91800 loss: 0.0017 lr: 0.02\n",
      "iteration: 91900 loss: 0.0014 lr: 0.02\n",
      "iteration: 92000 loss: 0.0017 lr: 0.02\n",
      "iteration: 92100 loss: 0.0017 lr: 0.02\n",
      "iteration: 92200 loss: 0.0015 lr: 0.02\n",
      "iteration: 92300 loss: 0.0015 lr: 0.02\n",
      "iteration: 92400 loss: 0.0016 lr: 0.02\n",
      "iteration: 92500 loss: 0.0015 lr: 0.02\n",
      "iteration: 92600 loss: 0.0013 lr: 0.02\n",
      "iteration: 92700 loss: 0.0017 lr: 0.02\n",
      "iteration: 92800 loss: 0.0014 lr: 0.02\n",
      "iteration: 92900 loss: 0.0016 lr: 0.02\n",
      "iteration: 93000 loss: 0.0016 lr: 0.02\n",
      "iteration: 93100 loss: 0.0015 lr: 0.02\n",
      "iteration: 93200 loss: 0.0015 lr: 0.02\n",
      "iteration: 93300 loss: 0.0015 lr: 0.02\n",
      "iteration: 93400 loss: 0.0016 lr: 0.02\n",
      "iteration: 93500 loss: 0.0015 lr: 0.02\n",
      "iteration: 93600 loss: 0.0015 lr: 0.02\n",
      "iteration: 93700 loss: 0.0016 lr: 0.02\n",
      "iteration: 93800 loss: 0.0013 lr: 0.02\n",
      "iteration: 93900 loss: 0.0013 lr: 0.02\n",
      "iteration: 94000 loss: 0.0015 lr: 0.02\n",
      "iteration: 94100 loss: 0.0014 lr: 0.02\n",
      "iteration: 94200 loss: 0.0014 lr: 0.02\n",
      "iteration: 94300 loss: 0.0015 lr: 0.02\n",
      "iteration: 94400 loss: 0.0016 lr: 0.02\n",
      "iteration: 94500 loss: 0.0016 lr: 0.02\n",
      "iteration: 94600 loss: 0.0018 lr: 0.02\n",
      "iteration: 94700 loss: 0.0016 lr: 0.02\n",
      "iteration: 94800 loss: 0.0014 lr: 0.02\n",
      "iteration: 94900 loss: 0.0016 lr: 0.02\n",
      "iteration: 95000 loss: 0.0016 lr: 0.02\n",
      "iteration: 95100 loss: 0.0017 lr: 0.02\n",
      "iteration: 95200 loss: 0.0014 lr: 0.02\n",
      "iteration: 95300 loss: 0.0016 lr: 0.02\n",
      "iteration: 95400 loss: 0.0015 lr: 0.02\n",
      "iteration: 95500 loss: 0.0016 lr: 0.02\n",
      "iteration: 95600 loss: 0.0015 lr: 0.02\n",
      "iteration: 95700 loss: 0.0015 lr: 0.02\n",
      "iteration: 95800 loss: 0.0015 lr: 0.02\n",
      "iteration: 95900 loss: 0.0017 lr: 0.02\n",
      "iteration: 96000 loss: 0.0015 lr: 0.02\n",
      "iteration: 96100 loss: 0.0014 lr: 0.02\n",
      "iteration: 96200 loss: 0.0015 lr: 0.02\n",
      "iteration: 96300 loss: 0.0018 lr: 0.02\n",
      "iteration: 96400 loss: 0.0016 lr: 0.02\n",
      "iteration: 96500 loss: 0.0013 lr: 0.02\n",
      "iteration: 96600 loss: 0.0015 lr: 0.02\n",
      "iteration: 96700 loss: 0.0013 lr: 0.02\n",
      "iteration: 96800 loss: 0.0015 lr: 0.02\n",
      "iteration: 96900 loss: 0.0015 lr: 0.02\n",
      "iteration: 97000 loss: 0.0014 lr: 0.02\n",
      "iteration: 97100 loss: 0.0015 lr: 0.02\n",
      "iteration: 97200 loss: 0.0014 lr: 0.02\n",
      "iteration: 97300 loss: 0.0014 lr: 0.02\n",
      "iteration: 97400 loss: 0.0014 lr: 0.02\n",
      "iteration: 97500 loss: 0.0017 lr: 0.02\n",
      "iteration: 97600 loss: 0.0014 lr: 0.02\n",
      "iteration: 97700 loss: 0.0015 lr: 0.02\n",
      "iteration: 97800 loss: 0.0014 lr: 0.02\n",
      "iteration: 97900 loss: 0.0016 lr: 0.02\n",
      "iteration: 98000 loss: 0.0012 lr: 0.02\n",
      "iteration: 98100 loss: 0.0015 lr: 0.02\n",
      "iteration: 98200 loss: 0.0015 lr: 0.02\n",
      "iteration: 98300 loss: 0.0016 lr: 0.02\n",
      "iteration: 98400 loss: 0.0016 lr: 0.02\n",
      "iteration: 98500 loss: 0.0013 lr: 0.02\n",
      "iteration: 98600 loss: 0.0016 lr: 0.02\n",
      "iteration: 98700 loss: 0.0013 lr: 0.02\n",
      "iteration: 98800 loss: 0.0014 lr: 0.02\n",
      "iteration: 98900 loss: 0.0014 lr: 0.02\n",
      "iteration: 99000 loss: 0.0014 lr: 0.02\n",
      "iteration: 99100 loss: 0.0019 lr: 0.02\n",
      "iteration: 99200 loss: 0.0015 lr: 0.02\n",
      "iteration: 99300 loss: 0.0014 lr: 0.02\n",
      "iteration: 99400 loss: 0.0015 lr: 0.02\n",
      "iteration: 99500 loss: 0.0014 lr: 0.02\n",
      "iteration: 99600 loss: 0.0015 lr: 0.02\n",
      "iteration: 99700 loss: 0.0015 lr: 0.02\n",
      "iteration: 99800 loss: 0.0014 lr: 0.02\n",
      "iteration: 99900 loss: 0.0016 lr: 0.02\n",
      "iteration: 100000 loss: 0.0014 lr: 0.02\n",
      "iteration: 100100 loss: 0.0013 lr: 0.02\n",
      "iteration: 100200 loss: 0.0017 lr: 0.02\n",
      "iteration: 100300 loss: 0.0018 lr: 0.02\n",
      "iteration: 100400 loss: 0.0015 lr: 0.02\n",
      "iteration: 100500 loss: 0.0017 lr: 0.02\n",
      "iteration: 100600 loss: 0.0016 lr: 0.02\n",
      "iteration: 100700 loss: 0.0016 lr: 0.02\n",
      "iteration: 100800 loss: 0.0015 lr: 0.02\n",
      "iteration: 100900 loss: 0.0016 lr: 0.02\n",
      "iteration: 101000 loss: 0.0014 lr: 0.02\n",
      "iteration: 101100 loss: 0.0015 lr: 0.02\n",
      "iteration: 101200 loss: 0.0015 lr: 0.02\n",
      "iteration: 101300 loss: 0.0016 lr: 0.02\n",
      "iteration: 101400 loss: 0.0015 lr: 0.02\n",
      "iteration: 101500 loss: 0.0015 lr: 0.02\n",
      "iteration: 101600 loss: 0.0014 lr: 0.02\n",
      "iteration: 101700 loss: 0.0016 lr: 0.02\n",
      "iteration: 101800 loss: 0.0019 lr: 0.02\n",
      "iteration: 101900 loss: 0.0018 lr: 0.02\n",
      "iteration: 102000 loss: 0.0016 lr: 0.02\n",
      "iteration: 102100 loss: 0.0016 lr: 0.02\n",
      "iteration: 102200 loss: 0.0017 lr: 0.02\n",
      "iteration: 102300 loss: 0.0014 lr: 0.02\n",
      "iteration: 102400 loss: 0.0015 lr: 0.02\n",
      "iteration: 102500 loss: 0.0017 lr: 0.02\n",
      "iteration: 102600 loss: 0.0016 lr: 0.02\n",
      "iteration: 102700 loss: 0.0016 lr: 0.02\n",
      "iteration: 102800 loss: 0.0015 lr: 0.02\n",
      "iteration: 102900 loss: 0.0017 lr: 0.02\n",
      "iteration: 103000 loss: 0.0015 lr: 0.02\n",
      "iteration: 103100 loss: 0.0015 lr: 0.02\n",
      "iteration: 103200 loss: 0.0014 lr: 0.02\n",
      "iteration: 103300 loss: 0.0016 lr: 0.02\n",
      "iteration: 103400 loss: 0.0014 lr: 0.02\n",
      "iteration: 103500 loss: 0.0017 lr: 0.02\n",
      "iteration: 103600 loss: 0.0014 lr: 0.02\n",
      "iteration: 103700 loss: 0.0014 lr: 0.02\n",
      "iteration: 103800 loss: 0.0017 lr: 0.02\n",
      "iteration: 103900 loss: 0.0016 lr: 0.02\n",
      "iteration: 104000 loss: 0.0015 lr: 0.02\n",
      "iteration: 104100 loss: 0.0015 lr: 0.02\n",
      "iteration: 104200 loss: 0.0015 lr: 0.02\n",
      "iteration: 104300 loss: 0.0016 lr: 0.02\n",
      "iteration: 104400 loss: 0.0014 lr: 0.02\n",
      "iteration: 104500 loss: 0.0015 lr: 0.02\n",
      "iteration: 104600 loss: 0.0015 lr: 0.02\n",
      "iteration: 104700 loss: 0.0016 lr: 0.02\n",
      "iteration: 104800 loss: 0.0014 lr: 0.02\n",
      "iteration: 104900 loss: 0.0016 lr: 0.02\n",
      "iteration: 105000 loss: 0.0014 lr: 0.02\n",
      "iteration: 105100 loss: 0.0014 lr: 0.02\n",
      "iteration: 105200 loss: 0.0015 lr: 0.02\n",
      "iteration: 105300 loss: 0.0015 lr: 0.02\n",
      "iteration: 105400 loss: 0.0015 lr: 0.02\n",
      "iteration: 105500 loss: 0.0014 lr: 0.02\n",
      "iteration: 105600 loss: 0.0016 lr: 0.02\n",
      "iteration: 105700 loss: 0.0016 lr: 0.02\n",
      "iteration: 105800 loss: 0.0014 lr: 0.02\n",
      "iteration: 105900 loss: 0.0015 lr: 0.02\n",
      "iteration: 106000 loss: 0.0013 lr: 0.02\n",
      "iteration: 106100 loss: 0.0015 lr: 0.02\n",
      "iteration: 106200 loss: 0.0014 lr: 0.02\n",
      "iteration: 106300 loss: 0.0015 lr: 0.02\n",
      "iteration: 106400 loss: 0.0015 lr: 0.02\n",
      "iteration: 106500 loss: 0.0014 lr: 0.02\n",
      "iteration: 106600 loss: 0.0016 lr: 0.02\n",
      "iteration: 106700 loss: 0.0014 lr: 0.02\n",
      "iteration: 106800 loss: 0.0013 lr: 0.02\n",
      "iteration: 106900 loss: 0.0015 lr: 0.02\n",
      "iteration: 107000 loss: 0.0013 lr: 0.02\n",
      "iteration: 107100 loss: 0.0014 lr: 0.02\n",
      "iteration: 107200 loss: 0.0014 lr: 0.02\n",
      "iteration: 107300 loss: 0.0016 lr: 0.02\n",
      "iteration: 107400 loss: 0.0016 lr: 0.02\n",
      "iteration: 107500 loss: 0.0015 lr: 0.02\n",
      "iteration: 107600 loss: 0.0014 lr: 0.02\n",
      "iteration: 107700 loss: 0.0015 lr: 0.02\n",
      "iteration: 107800 loss: 0.0014 lr: 0.02\n",
      "iteration: 107900 loss: 0.0015 lr: 0.02\n",
      "iteration: 108000 loss: 0.0014 lr: 0.02\n",
      "iteration: 108100 loss: 0.0014 lr: 0.02\n",
      "iteration: 108200 loss: 0.0013 lr: 0.02\n",
      "iteration: 108300 loss: 0.0015 lr: 0.02\n",
      "iteration: 108400 loss: 0.0016 lr: 0.02\n",
      "iteration: 108500 loss: 0.0016 lr: 0.02\n",
      "iteration: 108600 loss: 0.0015 lr: 0.02\n",
      "iteration: 108700 loss: 0.0014 lr: 0.02\n",
      "iteration: 108800 loss: 0.0016 lr: 0.02\n",
      "iteration: 108900 loss: 0.0016 lr: 0.02\n",
      "iteration: 109000 loss: 0.0014 lr: 0.02\n",
      "iteration: 109100 loss: 0.0015 lr: 0.02\n",
      "iteration: 109200 loss: 0.0015 lr: 0.02\n",
      "iteration: 109300 loss: 0.0013 lr: 0.02\n",
      "iteration: 109400 loss: 0.0014 lr: 0.02\n",
      "iteration: 109500 loss: 0.0014 lr: 0.02\n",
      "iteration: 109600 loss: 0.0014 lr: 0.02\n",
      "iteration: 109700 loss: 0.0015 lr: 0.02\n",
      "iteration: 109800 loss: 0.0015 lr: 0.02\n",
      "iteration: 109900 loss: 0.0015 lr: 0.02\n",
      "iteration: 110000 loss: 0.0014 lr: 0.02\n",
      "iteration: 110100 loss: 0.0014 lr: 0.02\n",
      "iteration: 110200 loss: 0.0015 lr: 0.02\n",
      "iteration: 110300 loss: 0.0014 lr: 0.02\n",
      "iteration: 110400 loss: 0.0014 lr: 0.02\n",
      "iteration: 110500 loss: 0.0013 lr: 0.02\n",
      "iteration: 110600 loss: 0.0014 lr: 0.02\n",
      "iteration: 110700 loss: 0.0015 lr: 0.02\n",
      "iteration: 110800 loss: 0.0016 lr: 0.02\n",
      "iteration: 110900 loss: 0.0012 lr: 0.02\n",
      "iteration: 111000 loss: 0.0016 lr: 0.02\n",
      "iteration: 111100 loss: 0.0016 lr: 0.02\n",
      "iteration: 111200 loss: 0.0014 lr: 0.02\n",
      "iteration: 111300 loss: 0.0013 lr: 0.02\n",
      "iteration: 111400 loss: 0.0014 lr: 0.02\n",
      "iteration: 111500 loss: 0.0014 lr: 0.02\n",
      "iteration: 111600 loss: 0.0014 lr: 0.02\n",
      "iteration: 111700 loss: 0.0015 lr: 0.02\n",
      "iteration: 111800 loss: 0.0015 lr: 0.02\n",
      "iteration: 111900 loss: 0.0012 lr: 0.02\n",
      "iteration: 112000 loss: 0.0014 lr: 0.02\n",
      "iteration: 112100 loss: 0.0013 lr: 0.02\n",
      "iteration: 112200 loss: 0.0015 lr: 0.02\n",
      "iteration: 112300 loss: 0.0012 lr: 0.02\n",
      "iteration: 112400 loss: 0.0012 lr: 0.02\n",
      "iteration: 112500 loss: 0.0013 lr: 0.02\n",
      "iteration: 112600 loss: 0.0015 lr: 0.02\n",
      "iteration: 112700 loss: 0.0014 lr: 0.02\n",
      "iteration: 112800 loss: 0.0013 lr: 0.02\n",
      "iteration: 112900 loss: 0.0014 lr: 0.02\n",
      "iteration: 113000 loss: 0.0015 lr: 0.02\n",
      "iteration: 113100 loss: 0.0013 lr: 0.02\n",
      "iteration: 113200 loss: 0.0014 lr: 0.02\n",
      "iteration: 113300 loss: 0.0015 lr: 0.02\n",
      "iteration: 113400 loss: 0.0015 lr: 0.02\n",
      "iteration: 113500 loss: 0.0014 lr: 0.02\n",
      "iteration: 113600 loss: 0.0014 lr: 0.02\n",
      "iteration: 113700 loss: 0.0014 lr: 0.02\n",
      "iteration: 113800 loss: 0.0012 lr: 0.02\n",
      "iteration: 113900 loss: 0.0016 lr: 0.02\n",
      "iteration: 114000 loss: 0.0015 lr: 0.02\n",
      "iteration: 114100 loss: 0.0012 lr: 0.02\n",
      "iteration: 114200 loss: 0.0014 lr: 0.02\n",
      "iteration: 114300 loss: 0.0012 lr: 0.02\n",
      "iteration: 114400 loss: 0.0013 lr: 0.02\n",
      "iteration: 114500 loss: 0.0015 lr: 0.02\n",
      "iteration: 114600 loss: 0.0014 lr: 0.02\n",
      "iteration: 114700 loss: 0.0013 lr: 0.02\n",
      "iteration: 114800 loss: 0.0016 lr: 0.02\n",
      "iteration: 114900 loss: 0.0013 lr: 0.02\n",
      "iteration: 115000 loss: 0.0014 lr: 0.02\n",
      "iteration: 115100 loss: 0.0014 lr: 0.02\n",
      "iteration: 115200 loss: 0.0014 lr: 0.02\n",
      "iteration: 115300 loss: 0.0014 lr: 0.02\n",
      "iteration: 115400 loss: 0.0013 lr: 0.02\n",
      "iteration: 115500 loss: 0.0014 lr: 0.02\n",
      "iteration: 115600 loss: 0.0014 lr: 0.02\n",
      "iteration: 115700 loss: 0.0016 lr: 0.02\n",
      "iteration: 115800 loss: 0.0016 lr: 0.02\n",
      "iteration: 115900 loss: 0.0016 lr: 0.02\n",
      "iteration: 116000 loss: 0.0013 lr: 0.02\n",
      "iteration: 116100 loss: 0.0016 lr: 0.02\n",
      "iteration: 116200 loss: 0.0014 lr: 0.02\n",
      "iteration: 116300 loss: 0.0015 lr: 0.02\n",
      "iteration: 116400 loss: 0.0016 lr: 0.02\n",
      "iteration: 116500 loss: 0.0016 lr: 0.02\n",
      "iteration: 116600 loss: 0.0015 lr: 0.02\n",
      "iteration: 116700 loss: 0.0013 lr: 0.02\n",
      "iteration: 116800 loss: 0.0013 lr: 0.02\n",
      "iteration: 116900 loss: 0.0016 lr: 0.02\n",
      "iteration: 117000 loss: 0.0014 lr: 0.02\n",
      "iteration: 117100 loss: 0.0014 lr: 0.02\n",
      "iteration: 117200 loss: 0.0012 lr: 0.02\n",
      "iteration: 117300 loss: 0.0014 lr: 0.02\n",
      "iteration: 117400 loss: 0.0012 lr: 0.02\n",
      "iteration: 117500 loss: 0.0016 lr: 0.02\n",
      "iteration: 117600 loss: 0.0015 lr: 0.02\n",
      "iteration: 117700 loss: 0.0018 lr: 0.02\n",
      "iteration: 117800 loss: 0.0013 lr: 0.02\n",
      "iteration: 117900 loss: 0.0015 lr: 0.02\n",
      "iteration: 118000 loss: 0.0014 lr: 0.02\n",
      "iteration: 118100 loss: 0.0015 lr: 0.02\n",
      "iteration: 118200 loss: 0.0017 lr: 0.02\n",
      "iteration: 118300 loss: 0.0015 lr: 0.02\n",
      "iteration: 118400 loss: 0.0013 lr: 0.02\n",
      "iteration: 118500 loss: 0.0016 lr: 0.02\n",
      "iteration: 118600 loss: 0.0015 lr: 0.02\n",
      "iteration: 118700 loss: 0.0012 lr: 0.02\n",
      "iteration: 118800 loss: 0.0015 lr: 0.02\n",
      "iteration: 118900 loss: 0.0015 lr: 0.02\n",
      "iteration: 119000 loss: 0.0014 lr: 0.02\n",
      "iteration: 119100 loss: 0.0013 lr: 0.02\n",
      "iteration: 119200 loss: 0.0015 lr: 0.02\n",
      "iteration: 119300 loss: 0.0015 lr: 0.02\n",
      "iteration: 119400 loss: 0.0013 lr: 0.02\n",
      "iteration: 119500 loss: 0.0013 lr: 0.02\n",
      "iteration: 119600 loss: 0.0014 lr: 0.02\n",
      "iteration: 119700 loss: 0.0014 lr: 0.02\n",
      "iteration: 119800 loss: 0.0014 lr: 0.02\n",
      "iteration: 119900 loss: 0.0015 lr: 0.02\n",
      "iteration: 120000 loss: 0.0014 lr: 0.02\n",
      "iteration: 120100 loss: 0.0015 lr: 0.02\n",
      "iteration: 120200 loss: 0.0012 lr: 0.02\n",
      "iteration: 120300 loss: 0.0013 lr: 0.02\n",
      "iteration: 120400 loss: 0.0013 lr: 0.02\n",
      "iteration: 120500 loss: 0.0014 lr: 0.02\n",
      "iteration: 120600 loss: 0.0014 lr: 0.02\n",
      "iteration: 120700 loss: 0.0013 lr: 0.02\n",
      "iteration: 120800 loss: 0.0013 lr: 0.02\n",
      "iteration: 120900 loss: 0.0013 lr: 0.02\n",
      "iteration: 121000 loss: 0.0014 lr: 0.02\n",
      "iteration: 121100 loss: 0.0014 lr: 0.02\n",
      "iteration: 121200 loss: 0.0014 lr: 0.02\n",
      "iteration: 121300 loss: 0.0014 lr: 0.02\n",
      "iteration: 121400 loss: 0.0014 lr: 0.02\n",
      "iteration: 121500 loss: 0.0015 lr: 0.02\n",
      "iteration: 121600 loss: 0.0013 lr: 0.02\n",
      "iteration: 121700 loss: 0.0014 lr: 0.02\n",
      "iteration: 121800 loss: 0.0015 lr: 0.02\n",
      "iteration: 121900 loss: 0.0014 lr: 0.02\n",
      "iteration: 122000 loss: 0.0014 lr: 0.02\n",
      "iteration: 122100 loss: 0.0014 lr: 0.02\n",
      "iteration: 122200 loss: 0.0013 lr: 0.02\n",
      "iteration: 122300 loss: 0.0016 lr: 0.02\n",
      "iteration: 122400 loss: 0.0014 lr: 0.02\n",
      "iteration: 122500 loss: 0.0015 lr: 0.02\n",
      "iteration: 122600 loss: 0.0012 lr: 0.02\n",
      "iteration: 122700 loss: 0.0014 lr: 0.02\n",
      "iteration: 122800 loss: 0.0013 lr: 0.02\n",
      "iteration: 122900 loss: 0.0013 lr: 0.02\n",
      "iteration: 123000 loss: 0.0013 lr: 0.02\n",
      "iteration: 123100 loss: 0.0016 lr: 0.02\n",
      "iteration: 123200 loss: 0.0014 lr: 0.02\n",
      "iteration: 123300 loss: 0.0015 lr: 0.02\n",
      "iteration: 123400 loss: 0.0014 lr: 0.02\n",
      "iteration: 123500 loss: 0.0015 lr: 0.02\n",
      "iteration: 123600 loss: 0.0014 lr: 0.02\n",
      "iteration: 123700 loss: 0.0015 lr: 0.02\n",
      "iteration: 123800 loss: 0.0015 lr: 0.02\n",
      "iteration: 123900 loss: 0.0011 lr: 0.02\n",
      "iteration: 124000 loss: 0.0014 lr: 0.02\n",
      "iteration: 124100 loss: 0.0016 lr: 0.02\n",
      "iteration: 124200 loss: 0.0013 lr: 0.02\n",
      "iteration: 124300 loss: 0.0014 lr: 0.02\n",
      "iteration: 124400 loss: 0.0015 lr: 0.02\n",
      "iteration: 124500 loss: 0.0017 lr: 0.02\n",
      "iteration: 124600 loss: 0.0013 lr: 0.02\n",
      "iteration: 124700 loss: 0.0015 lr: 0.02\n",
      "iteration: 124800 loss: 0.0015 lr: 0.02\n",
      "iteration: 124900 loss: 0.0015 lr: 0.02\n",
      "iteration: 125000 loss: 0.0015 lr: 0.02\n",
      "iteration: 125100 loss: 0.0012 lr: 0.02\n",
      "iteration: 125200 loss: 0.0017 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 125300 loss: 0.0015 lr: 0.02\n",
      "iteration: 125400 loss: 0.0014 lr: 0.02\n",
      "iteration: 125500 loss: 0.0013 lr: 0.02\n",
      "iteration: 125600 loss: 0.0014 lr: 0.02\n",
      "iteration: 125700 loss: 0.0014 lr: 0.02\n",
      "iteration: 125800 loss: 0.0014 lr: 0.02\n",
      "iteration: 125900 loss: 0.0016 lr: 0.02\n",
      "iteration: 126000 loss: 0.0013 lr: 0.02\n",
      "iteration: 126100 loss: 0.0012 lr: 0.02\n",
      "iteration: 126200 loss: 0.0014 lr: 0.02\n",
      "iteration: 126300 loss: 0.0015 lr: 0.02\n",
      "iteration: 126400 loss: 0.0014 lr: 0.02\n",
      "iteration: 126500 loss: 0.0014 lr: 0.02\n",
      "iteration: 126600 loss: 0.0015 lr: 0.02\n",
      "iteration: 126700 loss: 0.0014 lr: 0.02\n",
      "iteration: 126800 loss: 0.0015 lr: 0.02\n",
      "iteration: 126900 loss: 0.0015 lr: 0.02\n",
      "iteration: 127000 loss: 0.0014 lr: 0.02\n",
      "iteration: 127100 loss: 0.0013 lr: 0.02\n",
      "iteration: 127200 loss: 0.0013 lr: 0.02\n",
      "iteration: 127300 loss: 0.0015 lr: 0.02\n",
      "iteration: 127400 loss: 0.0015 lr: 0.02\n",
      "iteration: 127500 loss: 0.0014 lr: 0.02\n",
      "iteration: 127600 loss: 0.0016 lr: 0.02\n",
      "iteration: 127700 loss: 0.0013 lr: 0.02\n",
      "iteration: 127800 loss: 0.0014 lr: 0.02\n",
      "iteration: 127900 loss: 0.0012 lr: 0.02\n",
      "iteration: 128000 loss: 0.0014 lr: 0.02\n",
      "iteration: 128100 loss: 0.0015 lr: 0.02\n",
      "iteration: 128200 loss: 0.0013 lr: 0.02\n",
      "iteration: 128300 loss: 0.0014 lr: 0.02\n",
      "iteration: 128400 loss: 0.0012 lr: 0.02\n",
      "iteration: 128500 loss: 0.0015 lr: 0.02\n",
      "iteration: 128600 loss: 0.0012 lr: 0.02\n",
      "iteration: 128700 loss: 0.0014 lr: 0.02\n",
      "iteration: 128800 loss: 0.0015 lr: 0.02\n",
      "iteration: 128900 loss: 0.0015 lr: 0.02\n",
      "iteration: 129000 loss: 0.0014 lr: 0.02\n",
      "iteration: 129100 loss: 0.0015 lr: 0.02\n",
      "iteration: 129200 loss: 0.0012 lr: 0.02\n",
      "iteration: 129300 loss: 0.0014 lr: 0.02\n",
      "iteration: 129400 loss: 0.0014 lr: 0.02\n",
      "iteration: 129500 loss: 0.0014 lr: 0.02\n",
      "iteration: 129600 loss: 0.0016 lr: 0.02\n",
      "iteration: 129700 loss: 0.0015 lr: 0.02\n",
      "iteration: 129800 loss: 0.0013 lr: 0.02\n",
      "iteration: 129900 loss: 0.0016 lr: 0.02\n",
      "iteration: 130000 loss: 0.0014 lr: 0.02\n",
      "iteration: 130100 loss: 0.0014 lr: 0.02\n",
      "iteration: 130200 loss: 0.0014 lr: 0.02\n",
      "iteration: 130300 loss: 0.0013 lr: 0.02\n",
      "iteration: 130400 loss: 0.0014 lr: 0.02\n",
      "iteration: 130500 loss: 0.0013 lr: 0.02\n",
      "iteration: 130600 loss: 0.0014 lr: 0.02\n",
      "iteration: 130700 loss: 0.0013 lr: 0.02\n",
      "iteration: 130800 loss: 0.0014 lr: 0.02\n",
      "iteration: 130900 loss: 0.0014 lr: 0.02\n",
      "iteration: 131000 loss: 0.0013 lr: 0.02\n",
      "iteration: 131100 loss: 0.0014 lr: 0.02\n",
      "iteration: 131200 loss: 0.0014 lr: 0.02\n",
      "iteration: 131300 loss: 0.0015 lr: 0.02\n",
      "iteration: 131400 loss: 0.0013 lr: 0.02\n",
      "iteration: 131500 loss: 0.0015 lr: 0.02\n",
      "iteration: 131600 loss: 0.0013 lr: 0.02\n",
      "iteration: 131700 loss: 0.0014 lr: 0.02\n",
      "iteration: 131800 loss: 0.0015 lr: 0.02\n",
      "iteration: 131900 loss: 0.0013 lr: 0.02\n",
      "iteration: 132000 loss: 0.0014 lr: 0.02\n",
      "iteration: 132100 loss: 0.0012 lr: 0.02\n",
      "iteration: 132200 loss: 0.0017 lr: 0.02\n",
      "iteration: 132300 loss: 0.0014 lr: 0.02\n",
      "iteration: 132400 loss: 0.0014 lr: 0.02\n",
      "iteration: 132500 loss: 0.0015 lr: 0.02\n",
      "iteration: 132600 loss: 0.0013 lr: 0.02\n",
      "iteration: 132700 loss: 0.0014 lr: 0.02\n",
      "iteration: 132800 loss: 0.0012 lr: 0.02\n",
      "iteration: 132900 loss: 0.0015 lr: 0.02\n",
      "iteration: 133000 loss: 0.0013 lr: 0.02\n",
      "iteration: 133100 loss: 0.0014 lr: 0.02\n",
      "iteration: 133200 loss: 0.0015 lr: 0.02\n",
      "iteration: 133300 loss: 0.0014 lr: 0.02\n",
      "iteration: 133400 loss: 0.0014 lr: 0.02\n",
      "iteration: 133500 loss: 0.0015 lr: 0.02\n",
      "iteration: 133600 loss: 0.0016 lr: 0.02\n",
      "iteration: 133700 loss: 0.0014 lr: 0.02\n",
      "iteration: 133800 loss: 0.0013 lr: 0.02\n",
      "iteration: 133900 loss: 0.0016 lr: 0.02\n",
      "iteration: 134000 loss: 0.0016 lr: 0.02\n",
      "iteration: 134100 loss: 0.0014 lr: 0.02\n",
      "iteration: 134200 loss: 0.0013 lr: 0.02\n",
      "iteration: 134300 loss: 0.0013 lr: 0.02\n",
      "iteration: 134400 loss: 0.0014 lr: 0.02\n",
      "iteration: 134500 loss: 0.0016 lr: 0.02\n",
      "iteration: 134600 loss: 0.0014 lr: 0.02\n",
      "iteration: 134700 loss: 0.0014 lr: 0.02\n",
      "iteration: 134800 loss: 0.0016 lr: 0.02\n",
      "iteration: 134900 loss: 0.0012 lr: 0.02\n",
      "iteration: 135000 loss: 0.0014 lr: 0.02\n",
      "iteration: 135100 loss: 0.0013 lr: 0.02\n",
      "iteration: 135200 loss: 0.0013 lr: 0.02\n",
      "iteration: 135300 loss: 0.0014 lr: 0.02\n",
      "iteration: 135400 loss: 0.0014 lr: 0.02\n",
      "iteration: 135500 loss: 0.0015 lr: 0.02\n",
      "iteration: 135600 loss: 0.0012 lr: 0.02\n",
      "iteration: 135700 loss: 0.0017 lr: 0.02\n",
      "iteration: 135800 loss: 0.0014 lr: 0.02\n",
      "iteration: 135900 loss: 0.0016 lr: 0.02\n",
      "iteration: 136000 loss: 0.0016 lr: 0.02\n",
      "iteration: 136100 loss: 0.0013 lr: 0.02\n",
      "iteration: 136200 loss: 0.0011 lr: 0.02\n",
      "iteration: 136300 loss: 0.0012 lr: 0.02\n",
      "iteration: 136400 loss: 0.0013 lr: 0.02\n",
      "iteration: 136500 loss: 0.0014 lr: 0.02\n",
      "iteration: 136600 loss: 0.0014 lr: 0.02\n",
      "iteration: 136700 loss: 0.0015 lr: 0.02\n",
      "iteration: 136800 loss: 0.0015 lr: 0.02\n",
      "iteration: 136900 loss: 0.0013 lr: 0.02\n",
      "iteration: 137000 loss: 0.0016 lr: 0.02\n",
      "iteration: 137100 loss: 0.0013 lr: 0.02\n",
      "iteration: 137200 loss: 0.0015 lr: 0.02\n",
      "iteration: 137300 loss: 0.0014 lr: 0.02\n",
      "iteration: 137400 loss: 0.0014 lr: 0.02\n",
      "iteration: 137500 loss: 0.0015 lr: 0.02\n",
      "iteration: 137600 loss: 0.0013 lr: 0.02\n",
      "iteration: 137700 loss: 0.0015 lr: 0.02\n",
      "iteration: 137800 loss: 0.0013 lr: 0.02\n",
      "iteration: 137900 loss: 0.0012 lr: 0.02\n",
      "iteration: 138000 loss: 0.0013 lr: 0.02\n",
      "iteration: 138100 loss: 0.0016 lr: 0.02\n",
      "iteration: 138200 loss: 0.0014 lr: 0.02\n",
      "iteration: 138300 loss: 0.0012 lr: 0.02\n",
      "iteration: 138400 loss: 0.0012 lr: 0.02\n",
      "iteration: 138500 loss: 0.0015 lr: 0.02\n",
      "iteration: 138600 loss: 0.0013 lr: 0.02\n",
      "iteration: 138700 loss: 0.0013 lr: 0.02\n",
      "iteration: 138800 loss: 0.0014 lr: 0.02\n",
      "iteration: 138900 loss: 0.0015 lr: 0.02\n",
      "iteration: 139000 loss: 0.0014 lr: 0.02\n",
      "iteration: 139100 loss: 0.0015 lr: 0.02\n",
      "iteration: 139200 loss: 0.0013 lr: 0.02\n",
      "iteration: 139300 loss: 0.0012 lr: 0.02\n",
      "iteration: 139400 loss: 0.0013 lr: 0.02\n",
      "iteration: 139500 loss: 0.0013 lr: 0.02\n",
      "iteration: 139600 loss: 0.0012 lr: 0.02\n",
      "iteration: 139700 loss: 0.0015 lr: 0.02\n",
      "iteration: 139800 loss: 0.0014 lr: 0.02\n",
      "iteration: 139900 loss: 0.0014 lr: 0.02\n",
      "iteration: 140000 loss: 0.0014 lr: 0.02\n",
      "iteration: 140100 loss: 0.0012 lr: 0.02\n",
      "iteration: 140200 loss: 0.0015 lr: 0.02\n",
      "iteration: 140300 loss: 0.0014 lr: 0.02\n",
      "iteration: 140400 loss: 0.0013 lr: 0.02\n",
      "iteration: 140500 loss: 0.0014 lr: 0.02\n",
      "iteration: 140600 loss: 0.0015 lr: 0.02\n",
      "iteration: 140700 loss: 0.0013 lr: 0.02\n",
      "iteration: 140800 loss: 0.0014 lr: 0.02\n",
      "iteration: 140900 loss: 0.0014 lr: 0.02\n",
      "iteration: 141000 loss: 0.0013 lr: 0.02\n",
      "iteration: 141100 loss: 0.0013 lr: 0.02\n",
      "iteration: 141200 loss: 0.0015 lr: 0.02\n",
      "iteration: 141300 loss: 0.0014 lr: 0.02\n",
      "iteration: 141400 loss: 0.0013 lr: 0.02\n",
      "iteration: 141500 loss: 0.0015 lr: 0.02\n",
      "iteration: 141600 loss: 0.0012 lr: 0.02\n",
      "iteration: 141700 loss: 0.0014 lr: 0.02\n",
      "iteration: 141800 loss: 0.0013 lr: 0.02\n",
      "iteration: 141900 loss: 0.0015 lr: 0.02\n",
      "iteration: 142000 loss: 0.0013 lr: 0.02\n",
      "iteration: 142100 loss: 0.0013 lr: 0.02\n",
      "iteration: 142200 loss: 0.0015 lr: 0.02\n",
      "iteration: 142300 loss: 0.0011 lr: 0.02\n",
      "iteration: 142400 loss: 0.0013 lr: 0.02\n",
      "iteration: 142500 loss: 0.0014 lr: 0.02\n",
      "iteration: 142600 loss: 0.0015 lr: 0.02\n",
      "iteration: 142700 loss: 0.0015 lr: 0.02\n",
      "iteration: 142800 loss: 0.0014 lr: 0.02\n",
      "iteration: 142900 loss: 0.0015 lr: 0.02\n",
      "iteration: 143000 loss: 0.0014 lr: 0.02\n",
      "iteration: 143100 loss: 0.0013 lr: 0.02\n",
      "iteration: 143200 loss: 0.0013 lr: 0.02\n",
      "iteration: 143300 loss: 0.0013 lr: 0.02\n",
      "iteration: 143400 loss: 0.0013 lr: 0.02\n",
      "iteration: 143500 loss: 0.0015 lr: 0.02\n",
      "iteration: 143600 loss: 0.0015 lr: 0.02\n",
      "iteration: 143700 loss: 0.0013 lr: 0.02\n",
      "iteration: 143800 loss: 0.0013 lr: 0.02\n",
      "iteration: 143900 loss: 0.0013 lr: 0.02\n",
      "iteration: 144000 loss: 0.0016 lr: 0.02\n",
      "iteration: 144100 loss: 0.0012 lr: 0.02\n",
      "iteration: 144200 loss: 0.0015 lr: 0.02\n",
      "iteration: 144300 loss: 0.0015 lr: 0.02\n",
      "iteration: 144400 loss: 0.0014 lr: 0.02\n",
      "iteration: 144500 loss: 0.0012 lr: 0.02\n",
      "iteration: 144600 loss: 0.0015 lr: 0.02\n",
      "iteration: 144700 loss: 0.0013 lr: 0.02\n",
      "iteration: 144800 loss: 0.0015 lr: 0.02\n",
      "iteration: 144900 loss: 0.0015 lr: 0.02\n",
      "iteration: 145000 loss: 0.0014 lr: 0.02\n",
      "iteration: 145100 loss: 0.0016 lr: 0.02\n",
      "iteration: 145200 loss: 0.0015 lr: 0.02\n",
      "iteration: 145300 loss: 0.0012 lr: 0.02\n",
      "iteration: 145400 loss: 0.0014 lr: 0.02\n",
      "iteration: 145500 loss: 0.0013 lr: 0.02\n",
      "iteration: 145600 loss: 0.0013 lr: 0.02\n",
      "iteration: 145700 loss: 0.0014 lr: 0.02\n",
      "iteration: 145800 loss: 0.0013 lr: 0.02\n",
      "iteration: 145900 loss: 0.0015 lr: 0.02\n",
      "iteration: 146000 loss: 0.0013 lr: 0.02\n",
      "iteration: 146100 loss: 0.0013 lr: 0.02\n",
      "iteration: 146200 loss: 0.0012 lr: 0.02\n",
      "iteration: 146300 loss: 0.0012 lr: 0.02\n",
      "iteration: 146400 loss: 0.0014 lr: 0.02\n",
      "iteration: 146500 loss: 0.0011 lr: 0.02\n",
      "iteration: 146600 loss: 0.0014 lr: 0.02\n",
      "iteration: 146700 loss: 0.0013 lr: 0.02\n",
      "iteration: 146800 loss: 0.0013 lr: 0.02\n",
      "iteration: 146900 loss: 0.0013 lr: 0.02\n",
      "iteration: 147000 loss: 0.0012 lr: 0.02\n",
      "iteration: 147100 loss: 0.0014 lr: 0.02\n",
      "iteration: 147200 loss: 0.0013 lr: 0.02\n",
      "iteration: 147300 loss: 0.0014 lr: 0.02\n",
      "iteration: 147400 loss: 0.0014 lr: 0.02\n",
      "iteration: 147500 loss: 0.0013 lr: 0.02\n",
      "iteration: 147600 loss: 0.0013 lr: 0.02\n",
      "iteration: 147700 loss: 0.0012 lr: 0.02\n",
      "iteration: 147800 loss: 0.0015 lr: 0.02\n",
      "iteration: 147900 loss: 0.0013 lr: 0.02\n",
      "iteration: 148000 loss: 0.0013 lr: 0.02\n",
      "iteration: 148100 loss: 0.0015 lr: 0.02\n",
      "iteration: 148200 loss: 0.0013 lr: 0.02\n",
      "iteration: 148300 loss: 0.0014 lr: 0.02\n",
      "iteration: 148400 loss: 0.0015 lr: 0.02\n",
      "iteration: 148500 loss: 0.0014 lr: 0.02\n",
      "iteration: 148600 loss: 0.0014 lr: 0.02\n",
      "iteration: 148700 loss: 0.0013 lr: 0.02\n",
      "iteration: 148800 loss: 0.0013 lr: 0.02\n",
      "iteration: 148900 loss: 0.0014 lr: 0.02\n",
      "iteration: 149000 loss: 0.0013 lr: 0.02\n",
      "iteration: 149100 loss: 0.0014 lr: 0.02\n",
      "iteration: 149200 loss: 0.0014 lr: 0.02\n",
      "iteration: 149300 loss: 0.0013 lr: 0.02\n",
      "iteration: 149400 loss: 0.0014 lr: 0.02\n",
      "iteration: 149500 loss: 0.0014 lr: 0.02\n",
      "iteration: 149600 loss: 0.0014 lr: 0.02\n",
      "iteration: 149700 loss: 0.0014 lr: 0.02\n",
      "iteration: 149800 loss: 0.0013 lr: 0.02\n",
      "iteration: 149900 loss: 0.0013 lr: 0.02\n",
      "iteration: 150000 loss: 0.0014 lr: 0.02\n",
      "iteration: 150100 loss: 0.0013 lr: 0.02\n",
      "iteration: 150200 loss: 0.0014 lr: 0.02\n",
      "iteration: 150300 loss: 0.0013 lr: 0.02\n",
      "iteration: 150400 loss: 0.0014 lr: 0.02\n",
      "iteration: 150500 loss: 0.0014 lr: 0.02\n",
      "iteration: 150600 loss: 0.0014 lr: 0.02\n",
      "iteration: 150700 loss: 0.0014 lr: 0.02\n",
      "iteration: 150800 loss: 0.0012 lr: 0.02\n",
      "iteration: 150900 loss: 0.0015 lr: 0.02\n",
      "iteration: 151000 loss: 0.0015 lr: 0.02\n",
      "iteration: 151100 loss: 0.0012 lr: 0.02\n",
      "iteration: 151200 loss: 0.0013 lr: 0.02\n",
      "iteration: 151300 loss: 0.0015 lr: 0.02\n",
      "iteration: 151400 loss: 0.0015 lr: 0.02\n",
      "iteration: 151500 loss: 0.0012 lr: 0.02\n",
      "iteration: 151600 loss: 0.0012 lr: 0.02\n",
      "iteration: 151700 loss: 0.0012 lr: 0.02\n",
      "iteration: 151800 loss: 0.0013 lr: 0.02\n",
      "iteration: 151900 loss: 0.0013 lr: 0.02\n",
      "iteration: 152000 loss: 0.0013 lr: 0.02\n",
      "iteration: 152100 loss: 0.0013 lr: 0.02\n",
      "iteration: 152200 loss: 0.0012 lr: 0.02\n",
      "iteration: 152300 loss: 0.0013 lr: 0.02\n",
      "iteration: 152400 loss: 0.0014 lr: 0.02\n",
      "iteration: 152500 loss: 0.0015 lr: 0.02\n",
      "iteration: 152600 loss: 0.0014 lr: 0.02\n",
      "iteration: 152700 loss: 0.0014 lr: 0.02\n",
      "iteration: 152800 loss: 0.0012 lr: 0.02\n",
      "iteration: 152900 loss: 0.0012 lr: 0.02\n",
      "iteration: 153000 loss: 0.0013 lr: 0.02\n",
      "iteration: 153100 loss: 0.0013 lr: 0.02\n",
      "iteration: 153200 loss: 0.0014 lr: 0.02\n",
      "iteration: 153300 loss: 0.0013 lr: 0.02\n",
      "iteration: 153400 loss: 0.0013 lr: 0.02\n",
      "iteration: 153500 loss: 0.0013 lr: 0.02\n",
      "iteration: 153600 loss: 0.0013 lr: 0.02\n",
      "iteration: 153700 loss: 0.0013 lr: 0.02\n",
      "iteration: 153800 loss: 0.0014 lr: 0.02\n",
      "iteration: 153900 loss: 0.0013 lr: 0.02\n",
      "iteration: 154000 loss: 0.0014 lr: 0.02\n",
      "iteration: 154100 loss: 0.0013 lr: 0.02\n",
      "iteration: 154200 loss: 0.0013 lr: 0.02\n",
      "iteration: 154300 loss: 0.0012 lr: 0.02\n",
      "iteration: 154400 loss: 0.0012 lr: 0.02\n",
      "iteration: 154500 loss: 0.0013 lr: 0.02\n",
      "iteration: 154600 loss: 0.0013 lr: 0.02\n",
      "iteration: 154700 loss: 0.0014 lr: 0.02\n",
      "iteration: 154800 loss: 0.0015 lr: 0.02\n",
      "iteration: 154900 loss: 0.0014 lr: 0.02\n",
      "iteration: 155000 loss: 0.0013 lr: 0.02\n",
      "iteration: 155100 loss: 0.0013 lr: 0.02\n",
      "iteration: 155200 loss: 0.0014 lr: 0.02\n",
      "iteration: 155300 loss: 0.0014 lr: 0.02\n",
      "iteration: 155400 loss: 0.0014 lr: 0.02\n",
      "iteration: 155500 loss: 0.0013 lr: 0.02\n",
      "iteration: 155600 loss: 0.0012 lr: 0.02\n",
      "iteration: 155700 loss: 0.0013 lr: 0.02\n",
      "iteration: 155800 loss: 0.0012 lr: 0.02\n",
      "iteration: 155900 loss: 0.0013 lr: 0.02\n",
      "iteration: 156000 loss: 0.0014 lr: 0.02\n",
      "iteration: 156100 loss: 0.0014 lr: 0.02\n",
      "iteration: 156200 loss: 0.0014 lr: 0.02\n",
      "iteration: 156300 loss: 0.0012 lr: 0.02\n",
      "iteration: 156400 loss: 0.0014 lr: 0.02\n",
      "iteration: 156500 loss: 0.0012 lr: 0.02\n",
      "iteration: 156600 loss: 0.0012 lr: 0.02\n",
      "iteration: 156700 loss: 0.0014 lr: 0.02\n",
      "iteration: 156800 loss: 0.0011 lr: 0.02\n",
      "iteration: 156900 loss: 0.0012 lr: 0.02\n",
      "iteration: 157000 loss: 0.0013 lr: 0.02\n",
      "iteration: 157100 loss: 0.0013 lr: 0.02\n",
      "iteration: 157200 loss: 0.0014 lr: 0.02\n",
      "iteration: 157300 loss: 0.0013 lr: 0.02\n",
      "iteration: 157400 loss: 0.0013 lr: 0.02\n",
      "iteration: 157500 loss: 0.0014 lr: 0.02\n",
      "iteration: 157600 loss: 0.0013 lr: 0.02\n",
      "iteration: 157700 loss: 0.0013 lr: 0.02\n",
      "iteration: 157800 loss: 0.0013 lr: 0.02\n",
      "iteration: 157900 loss: 0.0013 lr: 0.02\n",
      "iteration: 158000 loss: 0.0012 lr: 0.02\n",
      "iteration: 158100 loss: 0.0013 lr: 0.02\n",
      "iteration: 158200 loss: 0.0015 lr: 0.02\n",
      "iteration: 158300 loss: 0.0014 lr: 0.02\n",
      "iteration: 158400 loss: 0.0013 lr: 0.02\n",
      "iteration: 158500 loss: 0.0014 lr: 0.02\n",
      "iteration: 158600 loss: 0.0012 lr: 0.02\n",
      "iteration: 158700 loss: 0.0012 lr: 0.02\n",
      "iteration: 158800 loss: 0.0014 lr: 0.02\n",
      "iteration: 158900 loss: 0.0015 lr: 0.02\n",
      "iteration: 159000 loss: 0.0014 lr: 0.02\n",
      "iteration: 159100 loss: 0.0013 lr: 0.02\n",
      "iteration: 159200 loss: 0.0014 lr: 0.02\n",
      "iteration: 159300 loss: 0.0016 lr: 0.02\n",
      "iteration: 159400 loss: 0.0016 lr: 0.02\n",
      "iteration: 159500 loss: 0.0016 lr: 0.02\n",
      "iteration: 159600 loss: 0.0013 lr: 0.02\n",
      "iteration: 159700 loss: 0.0012 lr: 0.02\n",
      "iteration: 159800 loss: 0.0013 lr: 0.02\n",
      "iteration: 159900 loss: 0.0014 lr: 0.02\n",
      "iteration: 160000 loss: 0.0013 lr: 0.02\n",
      "iteration: 160100 loss: 0.0013 lr: 0.02\n",
      "iteration: 160200 loss: 0.0015 lr: 0.02\n",
      "iteration: 160300 loss: 0.0014 lr: 0.02\n",
      "iteration: 160400 loss: 0.0012 lr: 0.02\n",
      "iteration: 160500 loss: 0.0014 lr: 0.02\n",
      "iteration: 160600 loss: 0.0012 lr: 0.02\n",
      "iteration: 160700 loss: 0.0012 lr: 0.02\n",
      "iteration: 160800 loss: 0.0012 lr: 0.02\n",
      "iteration: 160900 loss: 0.0014 lr: 0.02\n",
      "iteration: 161000 loss: 0.0013 lr: 0.02\n",
      "iteration: 161100 loss: 0.0013 lr: 0.02\n",
      "iteration: 161200 loss: 0.0015 lr: 0.02\n",
      "iteration: 161300 loss: 0.0013 lr: 0.02\n",
      "iteration: 161400 loss: 0.0015 lr: 0.02\n",
      "iteration: 161500 loss: 0.0014 lr: 0.02\n",
      "iteration: 161600 loss: 0.0015 lr: 0.02\n",
      "iteration: 161700 loss: 0.0012 lr: 0.02\n",
      "iteration: 161800 loss: 0.0013 lr: 0.02\n",
      "iteration: 161900 loss: 0.0014 lr: 0.02\n",
      "iteration: 162000 loss: 0.0011 lr: 0.02\n",
      "iteration: 162100 loss: 0.0014 lr: 0.02\n",
      "iteration: 162200 loss: 0.0014 lr: 0.02\n",
      "iteration: 162300 loss: 0.0015 lr: 0.02\n",
      "iteration: 162400 loss: 0.0013 lr: 0.02\n",
      "iteration: 162500 loss: 0.0013 lr: 0.02\n",
      "iteration: 162600 loss: 0.0012 lr: 0.02\n",
      "iteration: 162700 loss: 0.0014 lr: 0.02\n",
      "iteration: 162800 loss: 0.0014 lr: 0.02\n",
      "iteration: 162900 loss: 0.0014 lr: 0.02\n",
      "iteration: 163000 loss: 0.0013 lr: 0.02\n",
      "iteration: 163100 loss: 0.0012 lr: 0.02\n",
      "iteration: 163200 loss: 0.0013 lr: 0.02\n",
      "iteration: 163300 loss: 0.0013 lr: 0.02\n",
      "iteration: 163400 loss: 0.0013 lr: 0.02\n",
      "iteration: 163500 loss: 0.0012 lr: 0.02\n",
      "iteration: 163600 loss: 0.0014 lr: 0.02\n",
      "iteration: 163700 loss: 0.0016 lr: 0.02\n",
      "iteration: 163800 loss: 0.0015 lr: 0.02\n",
      "iteration: 163900 loss: 0.0014 lr: 0.02\n",
      "iteration: 164000 loss: 0.0013 lr: 0.02\n",
      "iteration: 164100 loss: 0.0012 lr: 0.02\n",
      "iteration: 164200 loss: 0.0012 lr: 0.02\n",
      "iteration: 164300 loss: 0.0014 lr: 0.02\n",
      "iteration: 164400 loss: 0.0013 lr: 0.02\n",
      "iteration: 164500 loss: 0.0013 lr: 0.02\n",
      "iteration: 164600 loss: 0.0013 lr: 0.02\n",
      "iteration: 164700 loss: 0.0014 lr: 0.02\n",
      "iteration: 164800 loss: 0.0012 lr: 0.02\n",
      "iteration: 164900 loss: 0.0012 lr: 0.02\n",
      "iteration: 165000 loss: 0.0014 lr: 0.02\n",
      "iteration: 165100 loss: 0.0014 lr: 0.02\n",
      "iteration: 165200 loss: 0.0012 lr: 0.02\n",
      "iteration: 165300 loss: 0.0015 lr: 0.02\n",
      "iteration: 165400 loss: 0.0013 lr: 0.02\n",
      "iteration: 165500 loss: 0.0013 lr: 0.02\n",
      "iteration: 165600 loss: 0.0013 lr: 0.02\n",
      "iteration: 165700 loss: 0.0012 lr: 0.02\n",
      "iteration: 165800 loss: 0.0012 lr: 0.02\n",
      "iteration: 165900 loss: 0.0014 lr: 0.02\n",
      "iteration: 166000 loss: 0.0013 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 166100 loss: 0.0013 lr: 0.02\n",
      "iteration: 166200 loss: 0.0011 lr: 0.02\n",
      "iteration: 166300 loss: 0.0011 lr: 0.02\n",
      "iteration: 166400 loss: 0.0010 lr: 0.02\n",
      "iteration: 166500 loss: 0.0012 lr: 0.02\n",
      "iteration: 166600 loss: 0.0013 lr: 0.02\n",
      "iteration: 166700 loss: 0.0014 lr: 0.02\n",
      "iteration: 166800 loss: 0.0012 lr: 0.02\n",
      "iteration: 166900 loss: 0.0014 lr: 0.02\n",
      "iteration: 167000 loss: 0.0012 lr: 0.02\n",
      "iteration: 167100 loss: 0.0015 lr: 0.02\n",
      "iteration: 167200 loss: 0.0013 lr: 0.02\n",
      "iteration: 167300 loss: 0.0013 lr: 0.02\n",
      "iteration: 167400 loss: 0.0014 lr: 0.02\n",
      "iteration: 167500 loss: 0.0013 lr: 0.02\n",
      "iteration: 167600 loss: 0.0012 lr: 0.02\n",
      "iteration: 167700 loss: 0.0013 lr: 0.02\n",
      "iteration: 167800 loss: 0.0013 lr: 0.02\n",
      "iteration: 167900 loss: 0.0013 lr: 0.02\n",
      "iteration: 168000 loss: 0.0013 lr: 0.02\n",
      "iteration: 168100 loss: 0.0014 lr: 0.02\n",
      "iteration: 168200 loss: 0.0013 lr: 0.02\n",
      "iteration: 168300 loss: 0.0011 lr: 0.02\n",
      "iteration: 168400 loss: 0.0013 lr: 0.02\n",
      "iteration: 168500 loss: 0.0014 lr: 0.02\n",
      "iteration: 168600 loss: 0.0013 lr: 0.02\n",
      "iteration: 168700 loss: 0.0015 lr: 0.02\n",
      "iteration: 168800 loss: 0.0012 lr: 0.02\n",
      "iteration: 168900 loss: 0.0013 lr: 0.02\n",
      "iteration: 169000 loss: 0.0013 lr: 0.02\n",
      "iteration: 169100 loss: 0.0013 lr: 0.02\n",
      "iteration: 169200 loss: 0.0012 lr: 0.02\n",
      "iteration: 169300 loss: 0.0014 lr: 0.02\n",
      "iteration: 169400 loss: 0.0014 lr: 0.02\n",
      "iteration: 169500 loss: 0.0012 lr: 0.02\n",
      "iteration: 169600 loss: 0.0012 lr: 0.02\n",
      "iteration: 169700 loss: 0.0014 lr: 0.02\n",
      "iteration: 169800 loss: 0.0014 lr: 0.02\n",
      "iteration: 169900 loss: 0.0013 lr: 0.02\n",
      "iteration: 170000 loss: 0.0013 lr: 0.02\n",
      "iteration: 170100 loss: 0.0013 lr: 0.02\n",
      "iteration: 170200 loss: 0.0013 lr: 0.02\n",
      "iteration: 170300 loss: 0.0011 lr: 0.02\n",
      "iteration: 170400 loss: 0.0011 lr: 0.02\n",
      "iteration: 170500 loss: 0.0012 lr: 0.02\n",
      "iteration: 170600 loss: 0.0015 lr: 0.02\n",
      "iteration: 170700 loss: 0.0011 lr: 0.02\n",
      "iteration: 170800 loss: 0.0014 lr: 0.02\n",
      "iteration: 170900 loss: 0.0014 lr: 0.02\n",
      "iteration: 171000 loss: 0.0014 lr: 0.02\n",
      "iteration: 171100 loss: 0.0012 lr: 0.02\n",
      "iteration: 171200 loss: 0.0015 lr: 0.02\n",
      "iteration: 171300 loss: 0.0013 lr: 0.02\n",
      "iteration: 171400 loss: 0.0013 lr: 0.02\n",
      "iteration: 171500 loss: 0.0014 lr: 0.02\n",
      "iteration: 171600 loss: 0.0015 lr: 0.02\n",
      "iteration: 171700 loss: 0.0012 lr: 0.02\n",
      "iteration: 171800 loss: 0.0013 lr: 0.02\n",
      "iteration: 171900 loss: 0.0014 lr: 0.02\n",
      "iteration: 172000 loss: 0.0013 lr: 0.02\n",
      "iteration: 172100 loss: 0.0014 lr: 0.02\n",
      "iteration: 172200 loss: 0.0012 lr: 0.02\n",
      "iteration: 172300 loss: 0.0013 lr: 0.02\n",
      "iteration: 172400 loss: 0.0014 lr: 0.02\n",
      "iteration: 172500 loss: 0.0013 lr: 0.02\n",
      "iteration: 172600 loss: 0.0013 lr: 0.02\n",
      "iteration: 172700 loss: 0.0015 lr: 0.02\n",
      "iteration: 172800 loss: 0.0013 lr: 0.02\n",
      "iteration: 172900 loss: 0.0012 lr: 0.02\n",
      "iteration: 173000 loss: 0.0013 lr: 0.02\n",
      "iteration: 173100 loss: 0.0014 lr: 0.02\n",
      "iteration: 173200 loss: 0.0012 lr: 0.02\n",
      "iteration: 173300 loss: 0.0014 lr: 0.02\n",
      "iteration: 173400 loss: 0.0015 lr: 0.02\n",
      "iteration: 173500 loss: 0.0012 lr: 0.02\n",
      "iteration: 173600 loss: 0.0012 lr: 0.02\n",
      "iteration: 173700 loss: 0.0012 lr: 0.02\n",
      "iteration: 173800 loss: 0.0012 lr: 0.02\n",
      "iteration: 173900 loss: 0.0012 lr: 0.02\n",
      "iteration: 174000 loss: 0.0012 lr: 0.02\n",
      "iteration: 174100 loss: 0.0014 lr: 0.02\n",
      "iteration: 174200 loss: 0.0012 lr: 0.02\n",
      "iteration: 174300 loss: 0.0016 lr: 0.02\n",
      "iteration: 174400 loss: 0.0012 lr: 0.02\n",
      "iteration: 174500 loss: 0.0013 lr: 0.02\n",
      "iteration: 174600 loss: 0.0013 lr: 0.02\n",
      "iteration: 174700 loss: 0.0012 lr: 0.02\n",
      "iteration: 174800 loss: 0.0015 lr: 0.02\n",
      "iteration: 174900 loss: 0.0016 lr: 0.02\n",
      "iteration: 175000 loss: 0.0013 lr: 0.02\n",
      "iteration: 175100 loss: 0.0012 lr: 0.02\n",
      "iteration: 175200 loss: 0.0014 lr: 0.02\n",
      "iteration: 175300 loss: 0.0013 lr: 0.02\n",
      "iteration: 175400 loss: 0.0012 lr: 0.02\n",
      "iteration: 175500 loss: 0.0013 lr: 0.02\n",
      "iteration: 175600 loss: 0.0012 lr: 0.02\n",
      "iteration: 175700 loss: 0.0013 lr: 0.02\n",
      "iteration: 175800 loss: 0.0012 lr: 0.02\n",
      "iteration: 175900 loss: 0.0014 lr: 0.02\n",
      "iteration: 176000 loss: 0.0013 lr: 0.02\n",
      "iteration: 176100 loss: 0.0012 lr: 0.02\n",
      "iteration: 176200 loss: 0.0011 lr: 0.02\n",
      "iteration: 176300 loss: 0.0014 lr: 0.02\n",
      "iteration: 176400 loss: 0.0012 lr: 0.02\n",
      "iteration: 176500 loss: 0.0011 lr: 0.02\n",
      "iteration: 176600 loss: 0.0013 lr: 0.02\n",
      "iteration: 176700 loss: 0.0013 lr: 0.02\n",
      "iteration: 176800 loss: 0.0014 lr: 0.02\n",
      "iteration: 176900 loss: 0.0012 lr: 0.02\n",
      "iteration: 177000 loss: 0.0013 lr: 0.02\n",
      "iteration: 177100 loss: 0.0014 lr: 0.02\n",
      "iteration: 177200 loss: 0.0012 lr: 0.02\n",
      "iteration: 177300 loss: 0.0014 lr: 0.02\n",
      "iteration: 177400 loss: 0.0012 lr: 0.02\n",
      "iteration: 177500 loss: 0.0012 lr: 0.02\n",
      "iteration: 177600 loss: 0.0014 lr: 0.02\n",
      "iteration: 177700 loss: 0.0011 lr: 0.02\n",
      "iteration: 177800 loss: 0.0014 lr: 0.02\n",
      "iteration: 177900 loss: 0.0012 lr: 0.02\n",
      "iteration: 178000 loss: 0.0012 lr: 0.02\n",
      "iteration: 178100 loss: 0.0012 lr: 0.02\n",
      "iteration: 178200 loss: 0.0012 lr: 0.02\n",
      "iteration: 178300 loss: 0.0015 lr: 0.02\n",
      "iteration: 178400 loss: 0.0013 lr: 0.02\n",
      "iteration: 178500 loss: 0.0013 lr: 0.02\n",
      "iteration: 178600 loss: 0.0011 lr: 0.02\n",
      "iteration: 178700 loss: 0.0014 lr: 0.02\n",
      "iteration: 178800 loss: 0.0014 lr: 0.02\n",
      "iteration: 178900 loss: 0.0015 lr: 0.02\n",
      "iteration: 179000 loss: 0.0013 lr: 0.02\n",
      "iteration: 179100 loss: 0.0013 lr: 0.02\n",
      "iteration: 179200 loss: 0.0012 lr: 0.02\n",
      "iteration: 179300 loss: 0.0012 lr: 0.02\n",
      "iteration: 179400 loss: 0.0012 lr: 0.02\n",
      "iteration: 179500 loss: 0.0014 lr: 0.02\n",
      "iteration: 179600 loss: 0.0012 lr: 0.02\n",
      "iteration: 179700 loss: 0.0013 lr: 0.02\n",
      "iteration: 179800 loss: 0.0013 lr: 0.02\n",
      "iteration: 179900 loss: 0.0014 lr: 0.02\n",
      "iteration: 180000 loss: 0.0013 lr: 0.02\n",
      "iteration: 180100 loss: 0.0013 lr: 0.02\n",
      "iteration: 180200 loss: 0.0015 lr: 0.02\n",
      "iteration: 180300 loss: 0.0013 lr: 0.02\n",
      "iteration: 180400 loss: 0.0012 lr: 0.02\n",
      "iteration: 180500 loss: 0.0014 lr: 0.02\n",
      "iteration: 180600 loss: 0.0012 lr: 0.02\n",
      "iteration: 180700 loss: 0.0014 lr: 0.02\n",
      "iteration: 180800 loss: 0.0012 lr: 0.02\n",
      "iteration: 180900 loss: 0.0013 lr: 0.02\n",
      "iteration: 181000 loss: 0.0013 lr: 0.02\n",
      "iteration: 181100 loss: 0.0012 lr: 0.02\n",
      "iteration: 181200 loss: 0.0014 lr: 0.02\n",
      "iteration: 181300 loss: 0.0011 lr: 0.02\n",
      "iteration: 181400 loss: 0.0012 lr: 0.02\n",
      "iteration: 181500 loss: 0.0012 lr: 0.02\n",
      "iteration: 181600 loss: 0.0014 lr: 0.02\n",
      "iteration: 181700 loss: 0.0011 lr: 0.02\n",
      "iteration: 181800 loss: 0.0014 lr: 0.02\n",
      "iteration: 181900 loss: 0.0014 lr: 0.02\n",
      "iteration: 182000 loss: 0.0015 lr: 0.02\n",
      "iteration: 182100 loss: 0.0013 lr: 0.02\n",
      "iteration: 182200 loss: 0.0013 lr: 0.02\n",
      "iteration: 182300 loss: 0.0012 lr: 0.02\n",
      "iteration: 182400 loss: 0.0012 lr: 0.02\n",
      "iteration: 182500 loss: 0.0013 lr: 0.02\n",
      "iteration: 182600 loss: 0.0013 lr: 0.02\n",
      "iteration: 182700 loss: 0.0012 lr: 0.02\n",
      "iteration: 182800 loss: 0.0013 lr: 0.02\n",
      "iteration: 182900 loss: 0.0012 lr: 0.02\n",
      "iteration: 183000 loss: 0.0013 lr: 0.02\n",
      "iteration: 183100 loss: 0.0013 lr: 0.02\n",
      "iteration: 183200 loss: 0.0013 lr: 0.02\n",
      "iteration: 183300 loss: 0.0011 lr: 0.02\n",
      "iteration: 183400 loss: 0.0012 lr: 0.02\n",
      "iteration: 183500 loss: 0.0013 lr: 0.02\n",
      "iteration: 183600 loss: 0.0013 lr: 0.02\n",
      "iteration: 183700 loss: 0.0013 lr: 0.02\n",
      "iteration: 183800 loss: 0.0014 lr: 0.02\n",
      "iteration: 183900 loss: 0.0014 lr: 0.02\n",
      "iteration: 184000 loss: 0.0013 lr: 0.02\n",
      "iteration: 184100 loss: 0.0012 lr: 0.02\n",
      "iteration: 184200 loss: 0.0012 lr: 0.02\n",
      "iteration: 184300 loss: 0.0012 lr: 0.02\n",
      "iteration: 184400 loss: 0.0012 lr: 0.02\n",
      "iteration: 184500 loss: 0.0013 lr: 0.02\n",
      "iteration: 184600 loss: 0.0012 lr: 0.02\n",
      "iteration: 184700 loss: 0.0012 lr: 0.02\n",
      "iteration: 184800 loss: 0.0013 lr: 0.02\n",
      "iteration: 184900 loss: 0.0012 lr: 0.02\n",
      "iteration: 185000 loss: 0.0013 lr: 0.02\n",
      "iteration: 185100 loss: 0.0013 lr: 0.02\n",
      "iteration: 185200 loss: 0.0014 lr: 0.02\n",
      "iteration: 185300 loss: 0.0013 lr: 0.02\n",
      "iteration: 185400 loss: 0.0014 lr: 0.02\n",
      "iteration: 185500 loss: 0.0013 lr: 0.02\n",
      "iteration: 185600 loss: 0.0012 lr: 0.02\n",
      "iteration: 185700 loss: 0.0015 lr: 0.02\n",
      "iteration: 185800 loss: 0.0013 lr: 0.02\n",
      "iteration: 185900 loss: 0.0012 lr: 0.02\n",
      "iteration: 186000 loss: 0.0012 lr: 0.02\n",
      "iteration: 186100 loss: 0.0012 lr: 0.02\n",
      "iteration: 186200 loss: 0.0013 lr: 0.02\n",
      "iteration: 186300 loss: 0.0014 lr: 0.02\n",
      "iteration: 186400 loss: 0.0013 lr: 0.02\n",
      "iteration: 186500 loss: 0.0013 lr: 0.02\n",
      "iteration: 186600 loss: 0.0014 lr: 0.02\n",
      "iteration: 186700 loss: 0.0011 lr: 0.02\n",
      "iteration: 186800 loss: 0.0014 lr: 0.02\n",
      "iteration: 186900 loss: 0.0011 lr: 0.02\n",
      "iteration: 187000 loss: 0.0013 lr: 0.02\n",
      "iteration: 187100 loss: 0.0014 lr: 0.02\n",
      "iteration: 187200 loss: 0.0014 lr: 0.02\n",
      "iteration: 187300 loss: 0.0012 lr: 0.02\n",
      "iteration: 187400 loss: 0.0014 lr: 0.02\n",
      "iteration: 187500 loss: 0.0013 lr: 0.02\n",
      "iteration: 187600 loss: 0.0013 lr: 0.02\n",
      "iteration: 187700 loss: 0.0012 lr: 0.02\n",
      "iteration: 187800 loss: 0.0011 lr: 0.02\n",
      "iteration: 187900 loss: 0.0013 lr: 0.02\n",
      "iteration: 188000 loss: 0.0013 lr: 0.02\n",
      "iteration: 188100 loss: 0.0012 lr: 0.02\n",
      "iteration: 188200 loss: 0.0012 lr: 0.02\n",
      "iteration: 188300 loss: 0.0013 lr: 0.02\n",
      "iteration: 188400 loss: 0.0012 lr: 0.02\n",
      "iteration: 188500 loss: 0.0014 lr: 0.02\n",
      "iteration: 188600 loss: 0.0012 lr: 0.02\n",
      "iteration: 188700 loss: 0.0012 lr: 0.02\n",
      "iteration: 188800 loss: 0.0011 lr: 0.02\n",
      "iteration: 188900 loss: 0.0011 lr: 0.02\n",
      "iteration: 189000 loss: 0.0012 lr: 0.02\n",
      "iteration: 189100 loss: 0.0013 lr: 0.02\n",
      "iteration: 189200 loss: 0.0013 lr: 0.02\n",
      "iteration: 189300 loss: 0.0015 lr: 0.02\n",
      "iteration: 189400 loss: 0.0013 lr: 0.02\n",
      "iteration: 189500 loss: 0.0016 lr: 0.02\n",
      "iteration: 189600 loss: 0.0012 lr: 0.02\n",
      "iteration: 189700 loss: 0.0012 lr: 0.02\n",
      "iteration: 189800 loss: 0.0013 lr: 0.02\n",
      "iteration: 189900 loss: 0.0013 lr: 0.02\n",
      "iteration: 190000 loss: 0.0013 lr: 0.02\n",
      "iteration: 190100 loss: 0.0012 lr: 0.02\n",
      "iteration: 190200 loss: 0.0014 lr: 0.02\n",
      "iteration: 190300 loss: 0.0013 lr: 0.02\n",
      "iteration: 190400 loss: 0.0013 lr: 0.02\n",
      "iteration: 190500 loss: 0.0012 lr: 0.02\n",
      "iteration: 190600 loss: 0.0013 lr: 0.02\n",
      "iteration: 190700 loss: 0.0013 lr: 0.02\n",
      "iteration: 190800 loss: 0.0012 lr: 0.02\n",
      "iteration: 190900 loss: 0.0015 lr: 0.02\n",
      "iteration: 191000 loss: 0.0013 lr: 0.02\n",
      "iteration: 191100 loss: 0.0011 lr: 0.02\n",
      "iteration: 191200 loss: 0.0012 lr: 0.02\n",
      "iteration: 191300 loss: 0.0014 lr: 0.02\n",
      "iteration: 191400 loss: 0.0013 lr: 0.02\n",
      "iteration: 191500 loss: 0.0014 lr: 0.02\n",
      "iteration: 191600 loss: 0.0012 lr: 0.02\n",
      "iteration: 191700 loss: 0.0012 lr: 0.02\n",
      "iteration: 191800 loss: 0.0012 lr: 0.02\n",
      "iteration: 191900 loss: 0.0014 lr: 0.02\n",
      "iteration: 192000 loss: 0.0011 lr: 0.02\n",
      "iteration: 192100 loss: 0.0013 lr: 0.02\n",
      "iteration: 192200 loss: 0.0013 lr: 0.02\n",
      "iteration: 192300 loss: 0.0013 lr: 0.02\n",
      "iteration: 192400 loss: 0.0012 lr: 0.02\n",
      "iteration: 192500 loss: 0.0014 lr: 0.02\n",
      "iteration: 192600 loss: 0.0012 lr: 0.02\n",
      "iteration: 192700 loss: 0.0012 lr: 0.02\n",
      "iteration: 192800 loss: 0.0011 lr: 0.02\n",
      "iteration: 192900 loss: 0.0013 lr: 0.02\n",
      "iteration: 193000 loss: 0.0012 lr: 0.02\n",
      "iteration: 193100 loss: 0.0012 lr: 0.02\n",
      "iteration: 193200 loss: 0.0011 lr: 0.02\n",
      "iteration: 193300 loss: 0.0013 lr: 0.02\n",
      "iteration: 193400 loss: 0.0011 lr: 0.02\n",
      "iteration: 193500 loss: 0.0012 lr: 0.02\n",
      "iteration: 193600 loss: 0.0013 lr: 0.02\n",
      "iteration: 193700 loss: 0.0013 lr: 0.02\n",
      "iteration: 193800 loss: 0.0012 lr: 0.02\n",
      "iteration: 193900 loss: 0.0011 lr: 0.02\n",
      "iteration: 194000 loss: 0.0013 lr: 0.02\n",
      "iteration: 194100 loss: 0.0013 lr: 0.02\n",
      "iteration: 194200 loss: 0.0012 lr: 0.02\n",
      "iteration: 194300 loss: 0.0013 lr: 0.02\n",
      "iteration: 194400 loss: 0.0013 lr: 0.02\n",
      "iteration: 194500 loss: 0.0012 lr: 0.02\n",
      "iteration: 194600 loss: 0.0013 lr: 0.02\n",
      "iteration: 194700 loss: 0.0012 lr: 0.02\n",
      "iteration: 194800 loss: 0.0013 lr: 0.02\n",
      "iteration: 194900 loss: 0.0012 lr: 0.02\n",
      "iteration: 195000 loss: 0.0012 lr: 0.02\n",
      "iteration: 195100 loss: 0.0012 lr: 0.02\n",
      "iteration: 195200 loss: 0.0013 lr: 0.02\n",
      "iteration: 195300 loss: 0.0012 lr: 0.02\n",
      "iteration: 195400 loss: 0.0012 lr: 0.02\n",
      "iteration: 195500 loss: 0.0014 lr: 0.02\n",
      "iteration: 195600 loss: 0.0012 lr: 0.02\n",
      "iteration: 195700 loss: 0.0013 lr: 0.02\n",
      "iteration: 195800 loss: 0.0012 lr: 0.02\n",
      "iteration: 195900 loss: 0.0011 lr: 0.02\n",
      "iteration: 196000 loss: 0.0012 lr: 0.02\n",
      "iteration: 196100 loss: 0.0012 lr: 0.02\n",
      "iteration: 196200 loss: 0.0011 lr: 0.02\n",
      "iteration: 196300 loss: 0.0011 lr: 0.02\n",
      "iteration: 196400 loss: 0.0012 lr: 0.02\n",
      "iteration: 196500 loss: 0.0014 lr: 0.02\n",
      "iteration: 196600 loss: 0.0012 lr: 0.02\n",
      "iteration: 196700 loss: 0.0013 lr: 0.02\n",
      "iteration: 196800 loss: 0.0012 lr: 0.02\n",
      "iteration: 196900 loss: 0.0012 lr: 0.02\n",
      "iteration: 197000 loss: 0.0013 lr: 0.02\n",
      "iteration: 197100 loss: 0.0013 lr: 0.02\n",
      "iteration: 197200 loss: 0.0012 lr: 0.02\n",
      "iteration: 197300 loss: 0.0012 lr: 0.02\n",
      "iteration: 197400 loss: 0.0011 lr: 0.02\n",
      "iteration: 197500 loss: 0.0011 lr: 0.02\n",
      "iteration: 197600 loss: 0.0013 lr: 0.02\n",
      "iteration: 197700 loss: 0.0013 lr: 0.02\n",
      "iteration: 197800 loss: 0.0014 lr: 0.02\n",
      "iteration: 197900 loss: 0.0012 lr: 0.02\n",
      "iteration: 198000 loss: 0.0013 lr: 0.02\n",
      "iteration: 198100 loss: 0.0013 lr: 0.02\n",
      "iteration: 198200 loss: 0.0012 lr: 0.02\n",
      "iteration: 198300 loss: 0.0014 lr: 0.02\n",
      "iteration: 198400 loss: 0.0012 lr: 0.02\n",
      "iteration: 198500 loss: 0.0014 lr: 0.02\n",
      "iteration: 198600 loss: 0.0013 lr: 0.02\n",
      "iteration: 198700 loss: 0.0013 lr: 0.02\n",
      "iteration: 198800 loss: 0.0013 lr: 0.02\n",
      "iteration: 198900 loss: 0.0012 lr: 0.02\n",
      "iteration: 199000 loss: 0.0014 lr: 0.02\n",
      "iteration: 199100 loss: 0.0012 lr: 0.02\n",
      "iteration: 199200 loss: 0.0013 lr: 0.02\n",
      "iteration: 199300 loss: 0.0013 lr: 0.02\n",
      "iteration: 199400 loss: 0.0011 lr: 0.02\n",
      "iteration: 199500 loss: 0.0014 lr: 0.02\n",
      "iteration: 199600 loss: 0.0012 lr: 0.02\n",
      "iteration: 199700 loss: 0.0013 lr: 0.02\n",
      "iteration: 199800 loss: 0.0015 lr: 0.02\n",
      "iteration: 199900 loss: 0.0013 lr: 0.02\n",
      "iteration: 200000 loss: 0.0013 lr: 0.02\n",
      "iteration: 200100 loss: 0.0011 lr: 0.02\n",
      "iteration: 200200 loss: 0.0013 lr: 0.02\n",
      "iteration: 200300 loss: 0.0012 lr: 0.02\n",
      "iteration: 200400 loss: 0.0012 lr: 0.02\n",
      "iteration: 200500 loss: 0.0012 lr: 0.02\n",
      "iteration: 200600 loss: 0.0013 lr: 0.02\n",
      "iteration: 200700 loss: 0.0014 lr: 0.02\n",
      "iteration: 200800 loss: 0.0013 lr: 0.02\n",
      "iteration: 200900 loss: 0.0013 lr: 0.02\n",
      "iteration: 201000 loss: 0.0011 lr: 0.02\n",
      "iteration: 201100 loss: 0.0012 lr: 0.02\n",
      "iteration: 201200 loss: 0.0012 lr: 0.02\n",
      "iteration: 201300 loss: 0.0011 lr: 0.02\n",
      "iteration: 201400 loss: 0.0014 lr: 0.02\n",
      "iteration: 201500 loss: 0.0013 lr: 0.02\n",
      "iteration: 201600 loss: 0.0012 lr: 0.02\n",
      "iteration: 201700 loss: 0.0012 lr: 0.02\n",
      "iteration: 201800 loss: 0.0013 lr: 0.02\n",
      "iteration: 201900 loss: 0.0012 lr: 0.02\n",
      "iteration: 202000 loss: 0.0011 lr: 0.02\n",
      "iteration: 202100 loss: 0.0013 lr: 0.02\n",
      "iteration: 202200 loss: 0.0013 lr: 0.02\n",
      "iteration: 202300 loss: 0.0013 lr: 0.02\n",
      "iteration: 202400 loss: 0.0014 lr: 0.02\n",
      "iteration: 202500 loss: 0.0013 lr: 0.02\n",
      "iteration: 202600 loss: 0.0013 lr: 0.02\n",
      "iteration: 202700 loss: 0.0013 lr: 0.02\n",
      "iteration: 202800 loss: 0.0011 lr: 0.02\n",
      "iteration: 202900 loss: 0.0012 lr: 0.02\n",
      "iteration: 203000 loss: 0.0012 lr: 0.02\n",
      "iteration: 203100 loss: 0.0012 lr: 0.02\n",
      "iteration: 203200 loss: 0.0012 lr: 0.02\n",
      "iteration: 203300 loss: 0.0014 lr: 0.02\n",
      "iteration: 203400 loss: 0.0012 lr: 0.02\n",
      "iteration: 203500 loss: 0.0012 lr: 0.02\n",
      "iteration: 203600 loss: 0.0012 lr: 0.02\n",
      "iteration: 203700 loss: 0.0012 lr: 0.02\n",
      "iteration: 203800 loss: 0.0012 lr: 0.02\n",
      "iteration: 203900 loss: 0.0012 lr: 0.02\n",
      "iteration: 204000 loss: 0.0012 lr: 0.02\n",
      "iteration: 204100 loss: 0.0012 lr: 0.02\n",
      "iteration: 204200 loss: 0.0012 lr: 0.02\n",
      "iteration: 204300 loss: 0.0012 lr: 0.02\n",
      "iteration: 204400 loss: 0.0012 lr: 0.02\n",
      "iteration: 204500 loss: 0.0013 lr: 0.02\n",
      "iteration: 204600 loss: 0.0013 lr: 0.02\n",
      "iteration: 204700 loss: 0.0013 lr: 0.02\n",
      "iteration: 204800 loss: 0.0013 lr: 0.02\n",
      "iteration: 204900 loss: 0.0013 lr: 0.02\n",
      "iteration: 205000 loss: 0.0012 lr: 0.02\n",
      "iteration: 205100 loss: 0.0013 lr: 0.02\n",
      "iteration: 205200 loss: 0.0012 lr: 0.02\n",
      "iteration: 205300 loss: 0.0011 lr: 0.02\n",
      "iteration: 205400 loss: 0.0012 lr: 0.02\n",
      "iteration: 205500 loss: 0.0013 lr: 0.02\n",
      "iteration: 205600 loss: 0.0013 lr: 0.02\n",
      "iteration: 205700 loss: 0.0013 lr: 0.02\n",
      "iteration: 205800 loss: 0.0012 lr: 0.02\n",
      "iteration: 205900 loss: 0.0012 lr: 0.02\n",
      "iteration: 206000 loss: 0.0012 lr: 0.02\n",
      "iteration: 206100 loss: 0.0012 lr: 0.02\n",
      "iteration: 206200 loss: 0.0012 lr: 0.02\n",
      "iteration: 206300 loss: 0.0013 lr: 0.02\n",
      "iteration: 206400 loss: 0.0011 lr: 0.02\n",
      "iteration: 206500 loss: 0.0012 lr: 0.02\n",
      "iteration: 206600 loss: 0.0013 lr: 0.02\n",
      "iteration: 206700 loss: 0.0011 lr: 0.02\n",
      "iteration: 206800 loss: 0.0011 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 206900 loss: 0.0012 lr: 0.02\n",
      "iteration: 207000 loss: 0.0013 lr: 0.02\n",
      "iteration: 207100 loss: 0.0012 lr: 0.02\n",
      "iteration: 207200 loss: 0.0013 lr: 0.02\n",
      "iteration: 207300 loss: 0.0012 lr: 0.02\n",
      "iteration: 207400 loss: 0.0011 lr: 0.02\n",
      "iteration: 207500 loss: 0.0014 lr: 0.02\n",
      "iteration: 207600 loss: 0.0014 lr: 0.02\n",
      "iteration: 207700 loss: 0.0014 lr: 0.02\n",
      "iteration: 207800 loss: 0.0011 lr: 0.02\n",
      "iteration: 207900 loss: 0.0012 lr: 0.02\n",
      "iteration: 208000 loss: 0.0013 lr: 0.02\n",
      "iteration: 208100 loss: 0.0013 lr: 0.02\n",
      "iteration: 208200 loss: 0.0012 lr: 0.02\n",
      "iteration: 208300 loss: 0.0014 lr: 0.02\n",
      "iteration: 208400 loss: 0.0012 lr: 0.02\n",
      "iteration: 208500 loss: 0.0015 lr: 0.02\n",
      "iteration: 208600 loss: 0.0012 lr: 0.02\n",
      "iteration: 208700 loss: 0.0011 lr: 0.02\n",
      "iteration: 208800 loss: 0.0013 lr: 0.02\n",
      "iteration: 208900 loss: 0.0012 lr: 0.02\n",
      "iteration: 209000 loss: 0.0011 lr: 0.02\n",
      "iteration: 209100 loss: 0.0012 lr: 0.02\n",
      "iteration: 209200 loss: 0.0012 lr: 0.02\n",
      "iteration: 209300 loss: 0.0013 lr: 0.02\n",
      "iteration: 209400 loss: 0.0013 lr: 0.02\n",
      "iteration: 209500 loss: 0.0011 lr: 0.02\n",
      "iteration: 209600 loss: 0.0012 lr: 0.02\n",
      "iteration: 209700 loss: 0.0011 lr: 0.02\n",
      "iteration: 209800 loss: 0.0012 lr: 0.02\n",
      "iteration: 209900 loss: 0.0013 lr: 0.02\n",
      "iteration: 210000 loss: 0.0014 lr: 0.02\n",
      "iteration: 210100 loss: 0.0014 lr: 0.02\n",
      "iteration: 210200 loss: 0.0011 lr: 0.02\n",
      "iteration: 210300 loss: 0.0013 lr: 0.02\n",
      "iteration: 210400 loss: 0.0012 lr: 0.02\n",
      "iteration: 210500 loss: 0.0011 lr: 0.02\n",
      "iteration: 210600 loss: 0.0012 lr: 0.02\n",
      "iteration: 210700 loss: 0.0012 lr: 0.02\n",
      "iteration: 210800 loss: 0.0012 lr: 0.02\n",
      "iteration: 210900 loss: 0.0012 lr: 0.02\n",
      "iteration: 211000 loss: 0.0013 lr: 0.02\n",
      "iteration: 211100 loss: 0.0012 lr: 0.02\n",
      "iteration: 211200 loss: 0.0012 lr: 0.02\n",
      "iteration: 211300 loss: 0.0011 lr: 0.02\n",
      "iteration: 211400 loss: 0.0012 lr: 0.02\n",
      "iteration: 211500 loss: 0.0012 lr: 0.02\n",
      "iteration: 211600 loss: 0.0011 lr: 0.02\n",
      "iteration: 211700 loss: 0.0012 lr: 0.02\n",
      "iteration: 211800 loss: 0.0012 lr: 0.02\n",
      "iteration: 211900 loss: 0.0013 lr: 0.02\n",
      "iteration: 212000 loss: 0.0012 lr: 0.02\n",
      "iteration: 212100 loss: 0.0012 lr: 0.02\n",
      "iteration: 212200 loss: 0.0012 lr: 0.02\n",
      "iteration: 212300 loss: 0.0012 lr: 0.02\n",
      "iteration: 212400 loss: 0.0013 lr: 0.02\n",
      "iteration: 212500 loss: 0.0012 lr: 0.02\n",
      "iteration: 212600 loss: 0.0012 lr: 0.02\n",
      "iteration: 212700 loss: 0.0013 lr: 0.02\n",
      "iteration: 212800 loss: 0.0011 lr: 0.02\n",
      "iteration: 212900 loss: 0.0013 lr: 0.02\n",
      "iteration: 213000 loss: 0.0012 lr: 0.02\n",
      "iteration: 213100 loss: 0.0014 lr: 0.02\n",
      "iteration: 213200 loss: 0.0013 lr: 0.02\n",
      "iteration: 213300 loss: 0.0012 lr: 0.02\n",
      "iteration: 213400 loss: 0.0012 lr: 0.02\n",
      "iteration: 213500 loss: 0.0013 lr: 0.02\n",
      "iteration: 213600 loss: 0.0012 lr: 0.02\n",
      "iteration: 213700 loss: 0.0012 lr: 0.02\n",
      "iteration: 213800 loss: 0.0012 lr: 0.02\n",
      "iteration: 213900 loss: 0.0013 lr: 0.02\n",
      "iteration: 214000 loss: 0.0012 lr: 0.02\n",
      "iteration: 214100 loss: 0.0011 lr: 0.02\n",
      "iteration: 214200 loss: 0.0013 lr: 0.02\n",
      "iteration: 214300 loss: 0.0012 lr: 0.02\n",
      "iteration: 214400 loss: 0.0014 lr: 0.02\n",
      "iteration: 214500 loss: 0.0012 lr: 0.02\n",
      "iteration: 214600 loss: 0.0012 lr: 0.02\n",
      "iteration: 214700 loss: 0.0012 lr: 0.02\n",
      "iteration: 214800 loss: 0.0012 lr: 0.02\n",
      "iteration: 214900 loss: 0.0012 lr: 0.02\n",
      "iteration: 215000 loss: 0.0012 lr: 0.02\n",
      "iteration: 215100 loss: 0.0012 lr: 0.02\n",
      "iteration: 215200 loss: 0.0011 lr: 0.02\n",
      "iteration: 215300 loss: 0.0013 lr: 0.02\n",
      "iteration: 215400 loss: 0.0011 lr: 0.02\n",
      "iteration: 215500 loss: 0.0012 lr: 0.02\n",
      "iteration: 215600 loss: 0.0012 lr: 0.02\n",
      "iteration: 215700 loss: 0.0012 lr: 0.02\n",
      "iteration: 215800 loss: 0.0013 lr: 0.02\n",
      "iteration: 215900 loss: 0.0011 lr: 0.02\n",
      "iteration: 216000 loss: 0.0012 lr: 0.02\n",
      "iteration: 216100 loss: 0.0011 lr: 0.02\n",
      "iteration: 216200 loss: 0.0012 lr: 0.02\n",
      "iteration: 216300 loss: 0.0014 lr: 0.02\n",
      "iteration: 216400 loss: 0.0013 lr: 0.02\n",
      "iteration: 216500 loss: 0.0012 lr: 0.02\n",
      "iteration: 216600 loss: 0.0011 lr: 0.02\n",
      "iteration: 216700 loss: 0.0012 lr: 0.02\n",
      "iteration: 216800 loss: 0.0012 lr: 0.02\n",
      "iteration: 216900 loss: 0.0010 lr: 0.02\n",
      "iteration: 217000 loss: 0.0013 lr: 0.02\n",
      "iteration: 217100 loss: 0.0012 lr: 0.02\n",
      "iteration: 217200 loss: 0.0012 lr: 0.02\n",
      "iteration: 217300 loss: 0.0013 lr: 0.02\n",
      "iteration: 217400 loss: 0.0011 lr: 0.02\n",
      "iteration: 217500 loss: 0.0013 lr: 0.02\n",
      "iteration: 217600 loss: 0.0013 lr: 0.02\n",
      "iteration: 217700 loss: 0.0014 lr: 0.02\n",
      "iteration: 217800 loss: 0.0012 lr: 0.02\n",
      "iteration: 217900 loss: 0.0012 lr: 0.02\n",
      "iteration: 218000 loss: 0.0012 lr: 0.02\n",
      "iteration: 218100 loss: 0.0013 lr: 0.02\n",
      "iteration: 218200 loss: 0.0012 lr: 0.02\n",
      "iteration: 218300 loss: 0.0014 lr: 0.02\n",
      "iteration: 218400 loss: 0.0011 lr: 0.02\n",
      "iteration: 218500 loss: 0.0012 lr: 0.02\n",
      "iteration: 218600 loss: 0.0012 lr: 0.02\n",
      "iteration: 218700 loss: 0.0011 lr: 0.02\n",
      "iteration: 218800 loss: 0.0012 lr: 0.02\n",
      "iteration: 218900 loss: 0.0012 lr: 0.02\n",
      "iteration: 219000 loss: 0.0013 lr: 0.02\n",
      "iteration: 219100 loss: 0.0013 lr: 0.02\n",
      "iteration: 219200 loss: 0.0012 lr: 0.02\n",
      "iteration: 219300 loss: 0.0012 lr: 0.02\n",
      "iteration: 219400 loss: 0.0011 lr: 0.02\n",
      "iteration: 219500 loss: 0.0011 lr: 0.02\n",
      "iteration: 219600 loss: 0.0012 lr: 0.02\n",
      "iteration: 219700 loss: 0.0012 lr: 0.02\n",
      "iteration: 219800 loss: 0.0011 lr: 0.02\n",
      "iteration: 219900 loss: 0.0015 lr: 0.02\n",
      "iteration: 220000 loss: 0.0011 lr: 0.02\n",
      "iteration: 220100 loss: 0.0012 lr: 0.02\n",
      "iteration: 220200 loss: 0.0012 lr: 0.02\n",
      "iteration: 220300 loss: 0.0013 lr: 0.02\n",
      "iteration: 220400 loss: 0.0011 lr: 0.02\n",
      "iteration: 220500 loss: 0.0012 lr: 0.02\n",
      "iteration: 220600 loss: 0.0012 lr: 0.02\n",
      "iteration: 220700 loss: 0.0013 lr: 0.02\n",
      "iteration: 220800 loss: 0.0011 lr: 0.02\n",
      "iteration: 220900 loss: 0.0013 lr: 0.02\n",
      "iteration: 221000 loss: 0.0012 lr: 0.02\n",
      "iteration: 221100 loss: 0.0013 lr: 0.02\n",
      "iteration: 221200 loss: 0.0012 lr: 0.02\n",
      "iteration: 221300 loss: 0.0012 lr: 0.02\n",
      "iteration: 221400 loss: 0.0011 lr: 0.02\n",
      "iteration: 221500 loss: 0.0012 lr: 0.02\n",
      "iteration: 221600 loss: 0.0013 lr: 0.02\n",
      "iteration: 221700 loss: 0.0012 lr: 0.02\n",
      "iteration: 221800 loss: 0.0013 lr: 0.02\n",
      "iteration: 221900 loss: 0.0012 lr: 0.02\n",
      "iteration: 222000 loss: 0.0014 lr: 0.02\n",
      "iteration: 222100 loss: 0.0013 lr: 0.02\n",
      "iteration: 222200 loss: 0.0012 lr: 0.02\n",
      "iteration: 222300 loss: 0.0012 lr: 0.02\n",
      "iteration: 222400 loss: 0.0012 lr: 0.02\n",
      "iteration: 222500 loss: 0.0015 lr: 0.02\n",
      "iteration: 222600 loss: 0.0014 lr: 0.02\n",
      "iteration: 222700 loss: 0.0015 lr: 0.02\n",
      "iteration: 222800 loss: 0.0012 lr: 0.02\n",
      "iteration: 222900 loss: 0.0011 lr: 0.02\n",
      "iteration: 223000 loss: 0.0014 lr: 0.02\n",
      "iteration: 223100 loss: 0.0013 lr: 0.02\n",
      "iteration: 223200 loss: 0.0012 lr: 0.02\n",
      "iteration: 223300 loss: 0.0012 lr: 0.02\n",
      "iteration: 223400 loss: 0.0013 lr: 0.02\n",
      "iteration: 223500 loss: 0.0013 lr: 0.02\n",
      "iteration: 223600 loss: 0.0012 lr: 0.02\n",
      "iteration: 223700 loss: 0.0011 lr: 0.02\n",
      "iteration: 223800 loss: 0.0014 lr: 0.02\n",
      "iteration: 223900 loss: 0.0013 lr: 0.02\n",
      "iteration: 224000 loss: 0.0011 lr: 0.02\n",
      "iteration: 224100 loss: 0.0011 lr: 0.02\n",
      "iteration: 224200 loss: 0.0012 lr: 0.02\n",
      "iteration: 224300 loss: 0.0012 lr: 0.02\n",
      "iteration: 224400 loss: 0.0012 lr: 0.02\n",
      "iteration: 224500 loss: 0.0013 lr: 0.02\n",
      "iteration: 224600 loss: 0.0013 lr: 0.02\n",
      "iteration: 224700 loss: 0.0012 lr: 0.02\n",
      "iteration: 224800 loss: 0.0012 lr: 0.02\n",
      "iteration: 224900 loss: 0.0011 lr: 0.02\n",
      "iteration: 225000 loss: 0.0013 lr: 0.02\n",
      "iteration: 225100 loss: 0.0012 lr: 0.02\n",
      "iteration: 225200 loss: 0.0012 lr: 0.02\n",
      "iteration: 225300 loss: 0.0012 lr: 0.02\n",
      "iteration: 225400 loss: 0.0011 lr: 0.02\n",
      "iteration: 225500 loss: 0.0012 lr: 0.02\n",
      "iteration: 225600 loss: 0.0012 lr: 0.02\n",
      "iteration: 225700 loss: 0.0012 lr: 0.02\n",
      "iteration: 225800 loss: 0.0012 lr: 0.02\n",
      "iteration: 225900 loss: 0.0012 lr: 0.02\n",
      "iteration: 226000 loss: 0.0012 lr: 0.02\n",
      "iteration: 226100 loss: 0.0011 lr: 0.02\n",
      "iteration: 226200 loss: 0.0011 lr: 0.02\n",
      "iteration: 226300 loss: 0.0013 lr: 0.02\n",
      "iteration: 226400 loss: 0.0010 lr: 0.02\n",
      "iteration: 226500 loss: 0.0011 lr: 0.02\n",
      "iteration: 226600 loss: 0.0013 lr: 0.02\n",
      "iteration: 226700 loss: 0.0012 lr: 0.02\n",
      "iteration: 226800 loss: 0.0014 lr: 0.02\n",
      "iteration: 226900 loss: 0.0012 lr: 0.02\n",
      "iteration: 227000 loss: 0.0013 lr: 0.02\n",
      "iteration: 227100 loss: 0.0012 lr: 0.02\n",
      "iteration: 227200 loss: 0.0012 lr: 0.02\n",
      "iteration: 227300 loss: 0.0012 lr: 0.02\n",
      "iteration: 227400 loss: 0.0013 lr: 0.02\n",
      "iteration: 227500 loss: 0.0011 lr: 0.02\n",
      "iteration: 227600 loss: 0.0012 lr: 0.02\n",
      "iteration: 227700 loss: 0.0012 lr: 0.02\n",
      "iteration: 227800 loss: 0.0012 lr: 0.02\n",
      "iteration: 227900 loss: 0.0011 lr: 0.02\n",
      "iteration: 228000 loss: 0.0015 lr: 0.02\n",
      "iteration: 228100 loss: 0.0013 lr: 0.02\n",
      "iteration: 228200 loss: 0.0013 lr: 0.02\n",
      "iteration: 228300 loss: 0.0012 lr: 0.02\n",
      "iteration: 228400 loss: 0.0012 lr: 0.02\n",
      "iteration: 228500 loss: 0.0012 lr: 0.02\n",
      "iteration: 228600 loss: 0.0011 lr: 0.02\n",
      "iteration: 228700 loss: 0.0014 lr: 0.02\n",
      "iteration: 228800 loss: 0.0013 lr: 0.02\n",
      "iteration: 228900 loss: 0.0013 lr: 0.02\n",
      "iteration: 229000 loss: 0.0012 lr: 0.02\n",
      "iteration: 229100 loss: 0.0013 lr: 0.02\n",
      "iteration: 229200 loss: 0.0013 lr: 0.02\n",
      "iteration: 229300 loss: 0.0012 lr: 0.02\n",
      "iteration: 229400 loss: 0.0012 lr: 0.02\n",
      "iteration: 229500 loss: 0.0013 lr: 0.02\n",
      "iteration: 229600 loss: 0.0010 lr: 0.02\n",
      "iteration: 229700 loss: 0.0013 lr: 0.02\n",
      "iteration: 229800 loss: 0.0012 lr: 0.02\n",
      "iteration: 229900 loss: 0.0013 lr: 0.02\n",
      "iteration: 230000 loss: 0.0012 lr: 0.02\n",
      "iteration: 230100 loss: 0.0012 lr: 0.02\n",
      "iteration: 230200 loss: 0.0012 lr: 0.02\n",
      "iteration: 230300 loss: 0.0012 lr: 0.02\n",
      "iteration: 230400 loss: 0.0012 lr: 0.02\n",
      "iteration: 230500 loss: 0.0011 lr: 0.02\n",
      "iteration: 230600 loss: 0.0012 lr: 0.02\n",
      "iteration: 230700 loss: 0.0012 lr: 0.02\n",
      "iteration: 230800 loss: 0.0012 lr: 0.02\n",
      "iteration: 230900 loss: 0.0011 lr: 0.02\n",
      "iteration: 231000 loss: 0.0012 lr: 0.02\n",
      "iteration: 231100 loss: 0.0013 lr: 0.02\n",
      "iteration: 231200 loss: 0.0012 lr: 0.02\n",
      "iteration: 231300 loss: 0.0011 lr: 0.02\n",
      "iteration: 231400 loss: 0.0011 lr: 0.02\n",
      "iteration: 231500 loss: 0.0012 lr: 0.02\n",
      "iteration: 231600 loss: 0.0010 lr: 0.02\n",
      "iteration: 231700 loss: 0.0010 lr: 0.02\n",
      "iteration: 231800 loss: 0.0011 lr: 0.02\n",
      "iteration: 231900 loss: 0.0012 lr: 0.02\n",
      "iteration: 232000 loss: 0.0011 lr: 0.02\n",
      "iteration: 232100 loss: 0.0013 lr: 0.02\n",
      "iteration: 232200 loss: 0.0012 lr: 0.02\n",
      "iteration: 232300 loss: 0.0012 lr: 0.02\n",
      "iteration: 232400 loss: 0.0011 lr: 0.02\n",
      "iteration: 232500 loss: 0.0012 lr: 0.02\n",
      "iteration: 232600 loss: 0.0012 lr: 0.02\n",
      "iteration: 232700 loss: 0.0012 lr: 0.02\n",
      "iteration: 232800 loss: 0.0013 lr: 0.02\n",
      "iteration: 232900 loss: 0.0012 lr: 0.02\n",
      "iteration: 233000 loss: 0.0012 lr: 0.02\n",
      "iteration: 233100 loss: 0.0012 lr: 0.02\n",
      "iteration: 233200 loss: 0.0013 lr: 0.02\n",
      "iteration: 233300 loss: 0.0012 lr: 0.02\n",
      "iteration: 233400 loss: 0.0011 lr: 0.02\n",
      "iteration: 233500 loss: 0.0013 lr: 0.02\n",
      "iteration: 233600 loss: 0.0012 lr: 0.02\n",
      "iteration: 233700 loss: 0.0014 lr: 0.02\n",
      "iteration: 233800 loss: 0.0011 lr: 0.02\n",
      "iteration: 233900 loss: 0.0014 lr: 0.02\n",
      "iteration: 234000 loss: 0.0012 lr: 0.02\n",
      "iteration: 234100 loss: 0.0013 lr: 0.02\n",
      "iteration: 234200 loss: 0.0012 lr: 0.02\n",
      "iteration: 234300 loss: 0.0011 lr: 0.02\n",
      "iteration: 234400 loss: 0.0011 lr: 0.02\n",
      "iteration: 234500 loss: 0.0011 lr: 0.02\n",
      "iteration: 234600 loss: 0.0011 lr: 0.02\n",
      "iteration: 234700 loss: 0.0012 lr: 0.02\n",
      "iteration: 234800 loss: 0.0012 lr: 0.02\n",
      "iteration: 234900 loss: 0.0012 lr: 0.02\n",
      "iteration: 235000 loss: 0.0012 lr: 0.02\n",
      "iteration: 235100 loss: 0.0012 lr: 0.02\n",
      "iteration: 235200 loss: 0.0012 lr: 0.02\n",
      "iteration: 235300 loss: 0.0012 lr: 0.02\n",
      "iteration: 235400 loss: 0.0010 lr: 0.02\n",
      "iteration: 235500 loss: 0.0010 lr: 0.02\n",
      "iteration: 235600 loss: 0.0012 lr: 0.02\n",
      "iteration: 235700 loss: 0.0013 lr: 0.02\n",
      "iteration: 235800 loss: 0.0011 lr: 0.02\n",
      "iteration: 235900 loss: 0.0012 lr: 0.02\n",
      "iteration: 236000 loss: 0.0012 lr: 0.02\n",
      "iteration: 236100 loss: 0.0012 lr: 0.02\n",
      "iteration: 236200 loss: 0.0011 lr: 0.02\n",
      "iteration: 236300 loss: 0.0011 lr: 0.02\n",
      "iteration: 236400 loss: 0.0012 lr: 0.02\n",
      "iteration: 236500 loss: 0.0011 lr: 0.02\n",
      "iteration: 236600 loss: 0.0012 lr: 0.02\n",
      "iteration: 236700 loss: 0.0014 lr: 0.02\n",
      "iteration: 236800 loss: 0.0012 lr: 0.02\n",
      "iteration: 236900 loss: 0.0013 lr: 0.02\n",
      "iteration: 237000 loss: 0.0012 lr: 0.02\n",
      "iteration: 237100 loss: 0.0013 lr: 0.02\n",
      "iteration: 237200 loss: 0.0013 lr: 0.02\n",
      "iteration: 237300 loss: 0.0014 lr: 0.02\n",
      "iteration: 237400 loss: 0.0011 lr: 0.02\n",
      "iteration: 237500 loss: 0.0011 lr: 0.02\n",
      "iteration: 237600 loss: 0.0012 lr: 0.02\n",
      "iteration: 237700 loss: 0.0012 lr: 0.02\n",
      "iteration: 237800 loss: 0.0013 lr: 0.02\n",
      "iteration: 237900 loss: 0.0012 lr: 0.02\n",
      "iteration: 238000 loss: 0.0015 lr: 0.02\n",
      "iteration: 238100 loss: 0.0012 lr: 0.02\n",
      "iteration: 238200 loss: 0.0013 lr: 0.02\n",
      "iteration: 238300 loss: 0.0010 lr: 0.02\n",
      "iteration: 238400 loss: 0.0011 lr: 0.02\n",
      "iteration: 238500 loss: 0.0012 lr: 0.02\n",
      "iteration: 238600 loss: 0.0012 lr: 0.02\n",
      "iteration: 238700 loss: 0.0012 lr: 0.02\n",
      "iteration: 238800 loss: 0.0012 lr: 0.02\n",
      "iteration: 238900 loss: 0.0011 lr: 0.02\n",
      "iteration: 239000 loss: 0.0011 lr: 0.02\n",
      "iteration: 239100 loss: 0.0012 lr: 0.02\n",
      "iteration: 239200 loss: 0.0011 lr: 0.02\n",
      "iteration: 239300 loss: 0.0011 lr: 0.02\n",
      "iteration: 239400 loss: 0.0013 lr: 0.02\n",
      "iteration: 239500 loss: 0.0014 lr: 0.02\n",
      "iteration: 239600 loss: 0.0012 lr: 0.02\n",
      "iteration: 239700 loss: 0.0012 lr: 0.02\n",
      "iteration: 239800 loss: 0.0012 lr: 0.02\n",
      "iteration: 239900 loss: 0.0010 lr: 0.02\n",
      "iteration: 240000 loss: 0.0012 lr: 0.02\n",
      "iteration: 240100 loss: 0.0011 lr: 0.02\n",
      "iteration: 240200 loss: 0.0011 lr: 0.02\n",
      "iteration: 240300 loss: 0.0013 lr: 0.02\n",
      "iteration: 240400 loss: 0.0013 lr: 0.02\n",
      "iteration: 240500 loss: 0.0012 lr: 0.02\n",
      "iteration: 240600 loss: 0.0010 lr: 0.02\n",
      "iteration: 240700 loss: 0.0013 lr: 0.02\n",
      "iteration: 240800 loss: 0.0011 lr: 0.02\n",
      "iteration: 240900 loss: 0.0013 lr: 0.02\n",
      "iteration: 241000 loss: 0.0010 lr: 0.02\n",
      "iteration: 241100 loss: 0.0012 lr: 0.02\n",
      "iteration: 241200 loss: 0.0013 lr: 0.02\n",
      "iteration: 241300 loss: 0.0011 lr: 0.02\n",
      "iteration: 241400 loss: 0.0012 lr: 0.02\n",
      "iteration: 241500 loss: 0.0010 lr: 0.02\n",
      "iteration: 241600 loss: 0.0013 lr: 0.02\n",
      "iteration: 241700 loss: 0.0012 lr: 0.02\n",
      "iteration: 241800 loss: 0.0012 lr: 0.02\n",
      "iteration: 241900 loss: 0.0012 lr: 0.02\n",
      "iteration: 242000 loss: 0.0011 lr: 0.02\n",
      "iteration: 242100 loss: 0.0012 lr: 0.02\n",
      "iteration: 242200 loss: 0.0012 lr: 0.02\n",
      "iteration: 242300 loss: 0.0013 lr: 0.02\n",
      "iteration: 242400 loss: 0.0013 lr: 0.02\n",
      "iteration: 242500 loss: 0.0011 lr: 0.02\n",
      "iteration: 242600 loss: 0.0012 lr: 0.02\n",
      "iteration: 242700 loss: 0.0013 lr: 0.02\n",
      "iteration: 242800 loss: 0.0012 lr: 0.02\n",
      "iteration: 242900 loss: 0.0013 lr: 0.02\n",
      "iteration: 243000 loss: 0.0013 lr: 0.02\n",
      "iteration: 243100 loss: 0.0012 lr: 0.02\n",
      "iteration: 243200 loss: 0.0011 lr: 0.02\n",
      "iteration: 243300 loss: 0.0011 lr: 0.02\n",
      "iteration: 243400 loss: 0.0012 lr: 0.02\n",
      "iteration: 243500 loss: 0.0011 lr: 0.02\n",
      "iteration: 243600 loss: 0.0013 lr: 0.02\n",
      "iteration: 243700 loss: 0.0012 lr: 0.02\n",
      "iteration: 243800 loss: 0.0012 lr: 0.02\n",
      "iteration: 243900 loss: 0.0012 lr: 0.02\n",
      "iteration: 244000 loss: 0.0012 lr: 0.02\n",
      "iteration: 244100 loss: 0.0012 lr: 0.02\n",
      "iteration: 244200 loss: 0.0011 lr: 0.02\n",
      "iteration: 244300 loss: 0.0012 lr: 0.02\n",
      "iteration: 244400 loss: 0.0012 lr: 0.02\n",
      "iteration: 244500 loss: 0.0012 lr: 0.02\n",
      "iteration: 244600 loss: 0.0010 lr: 0.02\n",
      "iteration: 244700 loss: 0.0012 lr: 0.02\n",
      "iteration: 244800 loss: 0.0010 lr: 0.02\n",
      "iteration: 244900 loss: 0.0011 lr: 0.02\n",
      "iteration: 245000 loss: 0.0011 lr: 0.02\n",
      "iteration: 245100 loss: 0.0011 lr: 0.02\n",
      "iteration: 245200 loss: 0.0012 lr: 0.02\n",
      "iteration: 245300 loss: 0.0013 lr: 0.02\n",
      "iteration: 245400 loss: 0.0012 lr: 0.02\n",
      "iteration: 245500 loss: 0.0013 lr: 0.02\n",
      "iteration: 245600 loss: 0.0012 lr: 0.02\n",
      "iteration: 245700 loss: 0.0012 lr: 0.02\n",
      "iteration: 245800 loss: 0.0012 lr: 0.02\n",
      "iteration: 245900 loss: 0.0013 lr: 0.02\n",
      "iteration: 246000 loss: 0.0012 lr: 0.02\n",
      "iteration: 246100 loss: 0.0013 lr: 0.02\n",
      "iteration: 246200 loss: 0.0012 lr: 0.02\n",
      "iteration: 246300 loss: 0.0013 lr: 0.02\n",
      "iteration: 246400 loss: 0.0010 lr: 0.02\n",
      "iteration: 246500 loss: 0.0012 lr: 0.02\n",
      "iteration: 246600 loss: 0.0011 lr: 0.02\n",
      "iteration: 246700 loss: 0.0014 lr: 0.02\n",
      "iteration: 246800 loss: 0.0011 lr: 0.02\n",
      "iteration: 246900 loss: 0.0013 lr: 0.02\n",
      "iteration: 247000 loss: 0.0012 lr: 0.02\n",
      "iteration: 247100 loss: 0.0011 lr: 0.02\n",
      "iteration: 247200 loss: 0.0010 lr: 0.02\n",
      "iteration: 247300 loss: 0.0013 lr: 0.02\n",
      "iteration: 247400 loss: 0.0011 lr: 0.02\n",
      "iteration: 247500 loss: 0.0011 lr: 0.02\n",
      "iteration: 247600 loss: 0.0012 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 247700 loss: 0.0012 lr: 0.02\n",
      "iteration: 247800 loss: 0.0011 lr: 0.02\n",
      "iteration: 247900 loss: 0.0013 lr: 0.02\n",
      "iteration: 248000 loss: 0.0011 lr: 0.02\n",
      "iteration: 248100 loss: 0.0011 lr: 0.02\n",
      "iteration: 248200 loss: 0.0013 lr: 0.02\n",
      "iteration: 248300 loss: 0.0011 lr: 0.02\n",
      "iteration: 248400 loss: 0.0011 lr: 0.02\n",
      "iteration: 248500 loss: 0.0012 lr: 0.02\n",
      "iteration: 248600 loss: 0.0012 lr: 0.02\n",
      "iteration: 248700 loss: 0.0013 lr: 0.02\n",
      "iteration: 248800 loss: 0.0011 lr: 0.02\n",
      "iteration: 248900 loss: 0.0011 lr: 0.02\n",
      "iteration: 249000 loss: 0.0011 lr: 0.02\n",
      "iteration: 249100 loss: 0.0012 lr: 0.02\n",
      "iteration: 249200 loss: 0.0012 lr: 0.02\n",
      "iteration: 249300 loss: 0.0011 lr: 0.02\n",
      "iteration: 249400 loss: 0.0012 lr: 0.02\n",
      "iteration: 249500 loss: 0.0013 lr: 0.02\n",
      "iteration: 249600 loss: 0.0012 lr: 0.02\n",
      "iteration: 249700 loss: 0.0013 lr: 0.02\n",
      "iteration: 249800 loss: 0.0011 lr: 0.02\n",
      "iteration: 249900 loss: 0.0013 lr: 0.02\n",
      "iteration: 250000 loss: 0.0011 lr: 0.02\n",
      "iteration: 250100 loss: 0.0011 lr: 0.02\n",
      "iteration: 250200 loss: 0.0012 lr: 0.02\n",
      "iteration: 250300 loss: 0.0011 lr: 0.02\n",
      "iteration: 250400 loss: 0.0012 lr: 0.02\n",
      "iteration: 250500 loss: 0.0012 lr: 0.02\n",
      "iteration: 250600 loss: 0.0011 lr: 0.02\n",
      "iteration: 250700 loss: 0.0012 lr: 0.02\n",
      "iteration: 250800 loss: 0.0012 lr: 0.02\n",
      "iteration: 250900 loss: 0.0011 lr: 0.02\n",
      "iteration: 251000 loss: 0.0012 lr: 0.02\n",
      "iteration: 251100 loss: 0.0013 lr: 0.02\n",
      "iteration: 251200 loss: 0.0013 lr: 0.02\n",
      "iteration: 251300 loss: 0.0014 lr: 0.02\n",
      "iteration: 251400 loss: 0.0011 lr: 0.02\n",
      "iteration: 251500 loss: 0.0012 lr: 0.02\n",
      "iteration: 251600 loss: 0.0012 lr: 0.02\n",
      "iteration: 251700 loss: 0.0010 lr: 0.02\n",
      "iteration: 251800 loss: 0.0011 lr: 0.02\n",
      "iteration: 251900 loss: 0.0012 lr: 0.02\n",
      "iteration: 252000 loss: 0.0013 lr: 0.02\n",
      "iteration: 252100 loss: 0.0012 lr: 0.02\n",
      "iteration: 252200 loss: 0.0010 lr: 0.02\n",
      "iteration: 252300 loss: 0.0012 lr: 0.02\n",
      "iteration: 252400 loss: 0.0012 lr: 0.02\n",
      "iteration: 252500 loss: 0.0011 lr: 0.02\n",
      "iteration: 252600 loss: 0.0010 lr: 0.02\n",
      "iteration: 252700 loss: 0.0012 lr: 0.02\n",
      "iteration: 252800 loss: 0.0012 lr: 0.02\n",
      "iteration: 252900 loss: 0.0011 lr: 0.02\n",
      "iteration: 253000 loss: 0.0011 lr: 0.02\n",
      "iteration: 253100 loss: 0.0012 lr: 0.02\n",
      "iteration: 253200 loss: 0.0011 lr: 0.02\n",
      "iteration: 253300 loss: 0.0012 lr: 0.02\n",
      "iteration: 253400 loss: 0.0012 lr: 0.02\n",
      "iteration: 253500 loss: 0.0011 lr: 0.02\n",
      "iteration: 253600 loss: 0.0012 lr: 0.02\n",
      "iteration: 253700 loss: 0.0011 lr: 0.02\n",
      "iteration: 253800 loss: 0.0011 lr: 0.02\n",
      "iteration: 253900 loss: 0.0011 lr: 0.02\n",
      "iteration: 254000 loss: 0.0012 lr: 0.02\n",
      "iteration: 254100 loss: 0.0011 lr: 0.02\n",
      "iteration: 254200 loss: 0.0011 lr: 0.02\n",
      "iteration: 254300 loss: 0.0011 lr: 0.02\n",
      "iteration: 254400 loss: 0.0014 lr: 0.02\n",
      "iteration: 254500 loss: 0.0012 lr: 0.02\n",
      "iteration: 254600 loss: 0.0011 lr: 0.02\n",
      "iteration: 254700 loss: 0.0012 lr: 0.02\n",
      "iteration: 254800 loss: 0.0011 lr: 0.02\n",
      "iteration: 254900 loss: 0.0011 lr: 0.02\n",
      "iteration: 255000 loss: 0.0013 lr: 0.02\n",
      "iteration: 255100 loss: 0.0012 lr: 0.02\n",
      "iteration: 255200 loss: 0.0010 lr: 0.02\n",
      "iteration: 255300 loss: 0.0011 lr: 0.02\n",
      "iteration: 255400 loss: 0.0012 lr: 0.02\n",
      "iteration: 255500 loss: 0.0011 lr: 0.02\n",
      "iteration: 255600 loss: 0.0011 lr: 0.02\n",
      "iteration: 255700 loss: 0.0011 lr: 0.02\n",
      "iteration: 255800 loss: 0.0012 lr: 0.02\n",
      "iteration: 255900 loss: 0.0012 lr: 0.02\n",
      "iteration: 256000 loss: 0.0013 lr: 0.02\n",
      "iteration: 256100 loss: 0.0011 lr: 0.02\n",
      "iteration: 256200 loss: 0.0011 lr: 0.02\n",
      "iteration: 256300 loss: 0.0010 lr: 0.02\n",
      "iteration: 256400 loss: 0.0012 lr: 0.02\n",
      "iteration: 256500 loss: 0.0012 lr: 0.02\n",
      "iteration: 256600 loss: 0.0012 lr: 0.02\n",
      "iteration: 256700 loss: 0.0012 lr: 0.02\n",
      "iteration: 256800 loss: 0.0012 lr: 0.02\n",
      "iteration: 256900 loss: 0.0012 lr: 0.02\n",
      "iteration: 257000 loss: 0.0011 lr: 0.02\n",
      "iteration: 257100 loss: 0.0012 lr: 0.02\n",
      "iteration: 257200 loss: 0.0014 lr: 0.02\n",
      "iteration: 257300 loss: 0.0012 lr: 0.02\n",
      "iteration: 257400 loss: 0.0013 lr: 0.02\n",
      "iteration: 257500 loss: 0.0012 lr: 0.02\n",
      "iteration: 257600 loss: 0.0011 lr: 0.02\n",
      "iteration: 257700 loss: 0.0012 lr: 0.02\n",
      "iteration: 257800 loss: 0.0011 lr: 0.02\n",
      "iteration: 257900 loss: 0.0011 lr: 0.02\n",
      "iteration: 258000 loss: 0.0011 lr: 0.02\n",
      "iteration: 258100 loss: 0.0011 lr: 0.02\n",
      "iteration: 258200 loss: 0.0013 lr: 0.02\n",
      "iteration: 258300 loss: 0.0013 lr: 0.02\n",
      "iteration: 258400 loss: 0.0012 lr: 0.02\n",
      "iteration: 258500 loss: 0.0011 lr: 0.02\n",
      "iteration: 258600 loss: 0.0012 lr: 0.02\n",
      "iteration: 258700 loss: 0.0011 lr: 0.02\n",
      "iteration: 258800 loss: 0.0011 lr: 0.02\n",
      "iteration: 258900 loss: 0.0012 lr: 0.02\n",
      "iteration: 259000 loss: 0.0011 lr: 0.02\n",
      "iteration: 259100 loss: 0.0012 lr: 0.02\n",
      "iteration: 259200 loss: 0.0009 lr: 0.02\n",
      "iteration: 259300 loss: 0.0012 lr: 0.02\n",
      "iteration: 259400 loss: 0.0010 lr: 0.02\n",
      "iteration: 259500 loss: 0.0012 lr: 0.02\n",
      "iteration: 259600 loss: 0.0012 lr: 0.02\n",
      "iteration: 259700 loss: 0.0011 lr: 0.02\n",
      "iteration: 259800 loss: 0.0013 lr: 0.02\n",
      "iteration: 259900 loss: 0.0011 lr: 0.02\n",
      "iteration: 260000 loss: 0.0012 lr: 0.02\n",
      "iteration: 260100 loss: 0.0012 lr: 0.02\n",
      "iteration: 260200 loss: 0.0011 lr: 0.02\n",
      "iteration: 260300 loss: 0.0012 lr: 0.02\n",
      "iteration: 260400 loss: 0.0010 lr: 0.02\n",
      "iteration: 260500 loss: 0.0012 lr: 0.02\n",
      "iteration: 260600 loss: 0.0012 lr: 0.02\n",
      "iteration: 260700 loss: 0.0012 lr: 0.02\n",
      "iteration: 260800 loss: 0.0011 lr: 0.02\n",
      "iteration: 260900 loss: 0.0012 lr: 0.02\n",
      "iteration: 261000 loss: 0.0011 lr: 0.02\n",
      "iteration: 261100 loss: 0.0013 lr: 0.02\n",
      "iteration: 261200 loss: 0.0012 lr: 0.02\n",
      "iteration: 261300 loss: 0.0013 lr: 0.02\n",
      "iteration: 261400 loss: 0.0012 lr: 0.02\n",
      "iteration: 261500 loss: 0.0011 lr: 0.02\n",
      "iteration: 261600 loss: 0.0012 lr: 0.02\n",
      "iteration: 261700 loss: 0.0011 lr: 0.02\n",
      "iteration: 261800 loss: 0.0011 lr: 0.02\n",
      "iteration: 261900 loss: 0.0011 lr: 0.02\n",
      "iteration: 262000 loss: 0.0012 lr: 0.02\n",
      "iteration: 262100 loss: 0.0012 lr: 0.02\n",
      "iteration: 262200 loss: 0.0012 lr: 0.02\n",
      "iteration: 262300 loss: 0.0013 lr: 0.02\n",
      "iteration: 262400 loss: 0.0012 lr: 0.02\n",
      "iteration: 262500 loss: 0.0011 lr: 0.02\n",
      "iteration: 262600 loss: 0.0012 lr: 0.02\n",
      "iteration: 262700 loss: 0.0011 lr: 0.02\n",
      "iteration: 262800 loss: 0.0012 lr: 0.02\n",
      "iteration: 262900 loss: 0.0010 lr: 0.02\n",
      "iteration: 263000 loss: 0.0011 lr: 0.02\n",
      "iteration: 263100 loss: 0.0011 lr: 0.02\n",
      "iteration: 263200 loss: 0.0013 lr: 0.02\n",
      "iteration: 263300 loss: 0.0011 lr: 0.02\n",
      "iteration: 263400 loss: 0.0011 lr: 0.02\n",
      "iteration: 263500 loss: 0.0011 lr: 0.02\n",
      "iteration: 263600 loss: 0.0011 lr: 0.02\n",
      "iteration: 263700 loss: 0.0013 lr: 0.02\n",
      "iteration: 263800 loss: 0.0012 lr: 0.02\n",
      "iteration: 263900 loss: 0.0014 lr: 0.02\n",
      "iteration: 264000 loss: 0.0013 lr: 0.02\n",
      "iteration: 264100 loss: 0.0014 lr: 0.02\n",
      "iteration: 264200 loss: 0.0011 lr: 0.02\n",
      "iteration: 264300 loss: 0.0011 lr: 0.02\n",
      "iteration: 264400 loss: 0.0011 lr: 0.02\n",
      "iteration: 264500 loss: 0.0013 lr: 0.02\n",
      "iteration: 264600 loss: 0.0011 lr: 0.02\n",
      "iteration: 264700 loss: 0.0012 lr: 0.02\n",
      "iteration: 264800 loss: 0.0011 lr: 0.02\n",
      "iteration: 264900 loss: 0.0011 lr: 0.02\n",
      "iteration: 265000 loss: 0.0012 lr: 0.02\n",
      "iteration: 265100 loss: 0.0010 lr: 0.02\n",
      "iteration: 265200 loss: 0.0012 lr: 0.02\n",
      "iteration: 265300 loss: 0.0011 lr: 0.02\n",
      "iteration: 265400 loss: 0.0012 lr: 0.02\n",
      "iteration: 265500 loss: 0.0012 lr: 0.02\n",
      "iteration: 265600 loss: 0.0012 lr: 0.02\n",
      "iteration: 265700 loss: 0.0011 lr: 0.02\n",
      "iteration: 265800 loss: 0.0011 lr: 0.02\n",
      "iteration: 265900 loss: 0.0012 lr: 0.02\n",
      "iteration: 266000 loss: 0.0012 lr: 0.02\n",
      "iteration: 266100 loss: 0.0013 lr: 0.02\n",
      "iteration: 266200 loss: 0.0011 lr: 0.02\n",
      "iteration: 266300 loss: 0.0009 lr: 0.02\n",
      "iteration: 266400 loss: 0.0012 lr: 0.02\n",
      "iteration: 266500 loss: 0.0011 lr: 0.02\n",
      "iteration: 266600 loss: 0.0012 lr: 0.02\n",
      "iteration: 266700 loss: 0.0012 lr: 0.02\n",
      "iteration: 266800 loss: 0.0011 lr: 0.02\n",
      "iteration: 266900 loss: 0.0009 lr: 0.02\n",
      "iteration: 267000 loss: 0.0012 lr: 0.02\n",
      "iteration: 267100 loss: 0.0012 lr: 0.02\n",
      "iteration: 267200 loss: 0.0013 lr: 0.02\n",
      "iteration: 267300 loss: 0.0012 lr: 0.02\n",
      "iteration: 267400 loss: 0.0011 lr: 0.02\n",
      "iteration: 267500 loss: 0.0010 lr: 0.02\n",
      "iteration: 267600 loss: 0.0010 lr: 0.02\n",
      "iteration: 267700 loss: 0.0014 lr: 0.02\n",
      "iteration: 267800 loss: 0.0012 lr: 0.02\n",
      "iteration: 267900 loss: 0.0011 lr: 0.02\n",
      "iteration: 268000 loss: 0.0012 lr: 0.02\n",
      "iteration: 268100 loss: 0.0013 lr: 0.02\n",
      "iteration: 268200 loss: 0.0012 lr: 0.02\n",
      "iteration: 268300 loss: 0.0013 lr: 0.02\n",
      "iteration: 268400 loss: 0.0012 lr: 0.02\n",
      "iteration: 268500 loss: 0.0012 lr: 0.02\n",
      "iteration: 268600 loss: 0.0012 lr: 0.02\n",
      "iteration: 268700 loss: 0.0011 lr: 0.02\n",
      "iteration: 268800 loss: 0.0011 lr: 0.02\n",
      "iteration: 268900 loss: 0.0012 lr: 0.02\n",
      "iteration: 269000 loss: 0.0012 lr: 0.02\n",
      "iteration: 269100 loss: 0.0012 lr: 0.02\n",
      "iteration: 269200 loss: 0.0012 lr: 0.02\n",
      "iteration: 269300 loss: 0.0012 lr: 0.02\n",
      "iteration: 269400 loss: 0.0012 lr: 0.02\n",
      "iteration: 269500 loss: 0.0012 lr: 0.02\n",
      "iteration: 269600 loss: 0.0011 lr: 0.02\n",
      "iteration: 269700 loss: 0.0010 lr: 0.02\n",
      "iteration: 269800 loss: 0.0011 lr: 0.02\n",
      "iteration: 269900 loss: 0.0011 lr: 0.02\n",
      "iteration: 270000 loss: 0.0012 lr: 0.02\n",
      "iteration: 270100 loss: 0.0011 lr: 0.02\n",
      "iteration: 270200 loss: 0.0010 lr: 0.02\n",
      "iteration: 270300 loss: 0.0014 lr: 0.02\n",
      "iteration: 270400 loss: 0.0010 lr: 0.02\n",
      "iteration: 270500 loss: 0.0012 lr: 0.02\n",
      "iteration: 270600 loss: 0.0011 lr: 0.02\n",
      "iteration: 270700 loss: 0.0011 lr: 0.02\n",
      "iteration: 270800 loss: 0.0012 lr: 0.02\n",
      "iteration: 270900 loss: 0.0011 lr: 0.02\n",
      "iteration: 271000 loss: 0.0011 lr: 0.02\n",
      "iteration: 271100 loss: 0.0012 lr: 0.02\n",
      "iteration: 271200 loss: 0.0011 lr: 0.02\n",
      "iteration: 271300 loss: 0.0012 lr: 0.02\n",
      "iteration: 271400 loss: 0.0011 lr: 0.02\n",
      "iteration: 271500 loss: 0.0011 lr: 0.02\n",
      "iteration: 271600 loss: 0.0012 lr: 0.02\n",
      "iteration: 271700 loss: 0.0013 lr: 0.02\n",
      "iteration: 271800 loss: 0.0012 lr: 0.02\n",
      "iteration: 271900 loss: 0.0011 lr: 0.02\n",
      "iteration: 272000 loss: 0.0013 lr: 0.02\n",
      "iteration: 272100 loss: 0.0011 lr: 0.02\n",
      "iteration: 272200 loss: 0.0013 lr: 0.02\n",
      "iteration: 272300 loss: 0.0011 lr: 0.02\n",
      "iteration: 272400 loss: 0.0013 lr: 0.02\n",
      "iteration: 272500 loss: 0.0012 lr: 0.02\n",
      "iteration: 272600 loss: 0.0012 lr: 0.02\n",
      "iteration: 272700 loss: 0.0011 lr: 0.02\n",
      "iteration: 272800 loss: 0.0012 lr: 0.02\n",
      "iteration: 272900 loss: 0.0013 lr: 0.02\n",
      "iteration: 273000 loss: 0.0012 lr: 0.02\n",
      "iteration: 273100 loss: 0.0012 lr: 0.02\n",
      "iteration: 273200 loss: 0.0011 lr: 0.02\n",
      "iteration: 273300 loss: 0.0012 lr: 0.02\n",
      "iteration: 273400 loss: 0.0011 lr: 0.02\n",
      "iteration: 273500 loss: 0.0011 lr: 0.02\n",
      "iteration: 273600 loss: 0.0010 lr: 0.02\n",
      "iteration: 273700 loss: 0.0011 lr: 0.02\n",
      "iteration: 273800 loss: 0.0012 lr: 0.02\n",
      "iteration: 273900 loss: 0.0012 lr: 0.02\n",
      "iteration: 274000 loss: 0.0011 lr: 0.02\n",
      "iteration: 274100 loss: 0.0014 lr: 0.02\n",
      "iteration: 274200 loss: 0.0011 lr: 0.02\n",
      "iteration: 274300 loss: 0.0012 lr: 0.02\n",
      "iteration: 274400 loss: 0.0011 lr: 0.02\n",
      "iteration: 274500 loss: 0.0011 lr: 0.02\n",
      "iteration: 274600 loss: 0.0013 lr: 0.02\n",
      "iteration: 274700 loss: 0.0011 lr: 0.02\n",
      "iteration: 274800 loss: 0.0011 lr: 0.02\n",
      "iteration: 274900 loss: 0.0011 lr: 0.02\n",
      "iteration: 275000 loss: 0.0016 lr: 0.02\n",
      "iteration: 275100 loss: 0.0012 lr: 0.02\n",
      "iteration: 275200 loss: 0.0011 lr: 0.02\n",
      "iteration: 275300 loss: 0.0011 lr: 0.02\n",
      "iteration: 275400 loss: 0.0011 lr: 0.02\n",
      "iteration: 275500 loss: 0.0012 lr: 0.02\n",
      "iteration: 275600 loss: 0.0011 lr: 0.02\n",
      "iteration: 275700 loss: 0.0012 lr: 0.02\n",
      "iteration: 275800 loss: 0.0012 lr: 0.02\n",
      "iteration: 275900 loss: 0.0011 lr: 0.02\n",
      "iteration: 276000 loss: 0.0012 lr: 0.02\n",
      "iteration: 276100 loss: 0.0011 lr: 0.02\n",
      "iteration: 276200 loss: 0.0011 lr: 0.02\n",
      "iteration: 276300 loss: 0.0011 lr: 0.02\n",
      "iteration: 276400 loss: 0.0013 lr: 0.02\n",
      "iteration: 276500 loss: 0.0011 lr: 0.02\n",
      "iteration: 276600 loss: 0.0010 lr: 0.02\n",
      "iteration: 276700 loss: 0.0012 lr: 0.02\n",
      "iteration: 276800 loss: 0.0011 lr: 0.02\n",
      "iteration: 276900 loss: 0.0012 lr: 0.02\n",
      "iteration: 277000 loss: 0.0011 lr: 0.02\n",
      "iteration: 277100 loss: 0.0011 lr: 0.02\n",
      "iteration: 277200 loss: 0.0012 lr: 0.02\n",
      "iteration: 277300 loss: 0.0011 lr: 0.02\n",
      "iteration: 277400 loss: 0.0012 lr: 0.02\n",
      "iteration: 277500 loss: 0.0012 lr: 0.02\n",
      "iteration: 277600 loss: 0.0011 lr: 0.02\n",
      "iteration: 277700 loss: 0.0012 lr: 0.02\n",
      "iteration: 277800 loss: 0.0012 lr: 0.02\n",
      "iteration: 277900 loss: 0.0011 lr: 0.02\n",
      "iteration: 278000 loss: 0.0012 lr: 0.02\n",
      "iteration: 278100 loss: 0.0010 lr: 0.02\n",
      "iteration: 278200 loss: 0.0012 lr: 0.02\n",
      "iteration: 278300 loss: 0.0011 lr: 0.02\n",
      "iteration: 278400 loss: 0.0012 lr: 0.02\n",
      "iteration: 278500 loss: 0.0011 lr: 0.02\n",
      "iteration: 278600 loss: 0.0011 lr: 0.02\n",
      "iteration: 278700 loss: 0.0011 lr: 0.02\n",
      "iteration: 278800 loss: 0.0012 lr: 0.02\n",
      "iteration: 278900 loss: 0.0012 lr: 0.02\n",
      "iteration: 279000 loss: 0.0012 lr: 0.02\n",
      "iteration: 279100 loss: 0.0010 lr: 0.02\n",
      "iteration: 279200 loss: 0.0011 lr: 0.02\n",
      "iteration: 279300 loss: 0.0012 lr: 0.02\n",
      "iteration: 279400 loss: 0.0011 lr: 0.02\n",
      "iteration: 279500 loss: 0.0012 lr: 0.02\n",
      "iteration: 279600 loss: 0.0011 lr: 0.02\n",
      "iteration: 279700 loss: 0.0010 lr: 0.02\n",
      "iteration: 279800 loss: 0.0011 lr: 0.02\n",
      "iteration: 279900 loss: 0.0013 lr: 0.02\n",
      "iteration: 280000 loss: 0.0013 lr: 0.02\n",
      "iteration: 280100 loss: 0.0013 lr: 0.02\n",
      "iteration: 280200 loss: 0.0012 lr: 0.02\n",
      "iteration: 280300 loss: 0.0011 lr: 0.02\n",
      "iteration: 280400 loss: 0.0011 lr: 0.02\n",
      "iteration: 280500 loss: 0.0010 lr: 0.02\n",
      "iteration: 280600 loss: 0.0011 lr: 0.02\n",
      "iteration: 280700 loss: 0.0012 lr: 0.02\n",
      "iteration: 280800 loss: 0.0011 lr: 0.02\n",
      "iteration: 280900 loss: 0.0011 lr: 0.02\n",
      "iteration: 281000 loss: 0.0011 lr: 0.02\n",
      "iteration: 281100 loss: 0.0011 lr: 0.02\n",
      "iteration: 281200 loss: 0.0011 lr: 0.02\n",
      "iteration: 281300 loss: 0.0011 lr: 0.02\n",
      "iteration: 281400 loss: 0.0011 lr: 0.02\n",
      "iteration: 281500 loss: 0.0010 lr: 0.02\n",
      "iteration: 281600 loss: 0.0012 lr: 0.02\n",
      "iteration: 281700 loss: 0.0012 lr: 0.02\n",
      "iteration: 281800 loss: 0.0010 lr: 0.02\n",
      "iteration: 281900 loss: 0.0012 lr: 0.02\n",
      "iteration: 282000 loss: 0.0010 lr: 0.02\n",
      "iteration: 282100 loss: 0.0011 lr: 0.02\n",
      "iteration: 282200 loss: 0.0011 lr: 0.02\n",
      "iteration: 282300 loss: 0.0012 lr: 0.02\n",
      "iteration: 282400 loss: 0.0011 lr: 0.02\n",
      "iteration: 282500 loss: 0.0011 lr: 0.02\n",
      "iteration: 282600 loss: 0.0011 lr: 0.02\n",
      "iteration: 282700 loss: 0.0012 lr: 0.02\n",
      "iteration: 282800 loss: 0.0010 lr: 0.02\n",
      "iteration: 282900 loss: 0.0011 lr: 0.02\n",
      "iteration: 283000 loss: 0.0013 lr: 0.02\n",
      "iteration: 283100 loss: 0.0011 lr: 0.02\n",
      "iteration: 283200 loss: 0.0010 lr: 0.02\n",
      "iteration: 283300 loss: 0.0013 lr: 0.02\n",
      "iteration: 283400 loss: 0.0011 lr: 0.02\n",
      "iteration: 283500 loss: 0.0011 lr: 0.02\n",
      "iteration: 283600 loss: 0.0013 lr: 0.02\n",
      "iteration: 283700 loss: 0.0012 lr: 0.02\n",
      "iteration: 283800 loss: 0.0011 lr: 0.02\n",
      "iteration: 283900 loss: 0.0011 lr: 0.02\n",
      "iteration: 284000 loss: 0.0012 lr: 0.02\n",
      "iteration: 284100 loss: 0.0011 lr: 0.02\n",
      "iteration: 284200 loss: 0.0011 lr: 0.02\n",
      "iteration: 284300 loss: 0.0011 lr: 0.02\n",
      "iteration: 284400 loss: 0.0011 lr: 0.02\n",
      "iteration: 284500 loss: 0.0011 lr: 0.02\n",
      "iteration: 284600 loss: 0.0011 lr: 0.02\n",
      "iteration: 284700 loss: 0.0012 lr: 0.02\n",
      "iteration: 284800 loss: 0.0010 lr: 0.02\n",
      "iteration: 284900 loss: 0.0011 lr: 0.02\n",
      "iteration: 285000 loss: 0.0011 lr: 0.02\n",
      "iteration: 285100 loss: 0.0010 lr: 0.02\n",
      "iteration: 285200 loss: 0.0010 lr: 0.02\n",
      "iteration: 285300 loss: 0.0012 lr: 0.02\n",
      "iteration: 285400 loss: 0.0013 lr: 0.02\n",
      "iteration: 285500 loss: 0.0013 lr: 0.02\n",
      "iteration: 285600 loss: 0.0012 lr: 0.02\n",
      "iteration: 285700 loss: 0.0012 lr: 0.02\n",
      "iteration: 285800 loss: 0.0013 lr: 0.02\n",
      "iteration: 285900 loss: 0.0012 lr: 0.02\n",
      "iteration: 286000 loss: 0.0012 lr: 0.02\n",
      "iteration: 286100 loss: 0.0011 lr: 0.02\n",
      "iteration: 286200 loss: 0.0012 lr: 0.02\n",
      "iteration: 286300 loss: 0.0011 lr: 0.02\n",
      "iteration: 286400 loss: 0.0012 lr: 0.02\n",
      "iteration: 286500 loss: 0.0012 lr: 0.02\n",
      "iteration: 286600 loss: 0.0012 lr: 0.02\n",
      "iteration: 286700 loss: 0.0012 lr: 0.02\n",
      "iteration: 286800 loss: 0.0012 lr: 0.02\n",
      "iteration: 286900 loss: 0.0012 lr: 0.02\n",
      "iteration: 287000 loss: 0.0012 lr: 0.02\n",
      "iteration: 287100 loss: 0.0010 lr: 0.02\n",
      "iteration: 287200 loss: 0.0010 lr: 0.02\n",
      "iteration: 287300 loss: 0.0010 lr: 0.02\n",
      "iteration: 287400 loss: 0.0012 lr: 0.02\n",
      "iteration: 287500 loss: 0.0013 lr: 0.02\n",
      "iteration: 287600 loss: 0.0011 lr: 0.02\n",
      "iteration: 287700 loss: 0.0012 lr: 0.02\n",
      "iteration: 287800 loss: 0.0012 lr: 0.02\n",
      "iteration: 287900 loss: 0.0013 lr: 0.02\n",
      "iteration: 288000 loss: 0.0012 lr: 0.02\n",
      "iteration: 288100 loss: 0.0012 lr: 0.02\n",
      "iteration: 288200 loss: 0.0012 lr: 0.02\n",
      "iteration: 288300 loss: 0.0012 lr: 0.02\n",
      "iteration: 288400 loss: 0.0011 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 288500 loss: 0.0011 lr: 0.02\n",
      "iteration: 288600 loss: 0.0010 lr: 0.02\n",
      "iteration: 288700 loss: 0.0010 lr: 0.02\n",
      "iteration: 288800 loss: 0.0013 lr: 0.02\n",
      "iteration: 288900 loss: 0.0012 lr: 0.02\n",
      "iteration: 289000 loss: 0.0011 lr: 0.02\n",
      "iteration: 289100 loss: 0.0012 lr: 0.02\n",
      "iteration: 289200 loss: 0.0012 lr: 0.02\n",
      "iteration: 289300 loss: 0.0013 lr: 0.02\n",
      "iteration: 289400 loss: 0.0013 lr: 0.02\n",
      "iteration: 289500 loss: 0.0012 lr: 0.02\n",
      "iteration: 289600 loss: 0.0011 lr: 0.02\n",
      "iteration: 289700 loss: 0.0011 lr: 0.02\n",
      "iteration: 289800 loss: 0.0011 lr: 0.02\n",
      "iteration: 289900 loss: 0.0013 lr: 0.02\n",
      "iteration: 290000 loss: 0.0011 lr: 0.02\n",
      "iteration: 290100 loss: 0.0010 lr: 0.02\n",
      "iteration: 290200 loss: 0.0011 lr: 0.02\n",
      "iteration: 290300 loss: 0.0011 lr: 0.02\n",
      "iteration: 290400 loss: 0.0011 lr: 0.02\n",
      "iteration: 290500 loss: 0.0011 lr: 0.02\n",
      "iteration: 290600 loss: 0.0011 lr: 0.02\n",
      "iteration: 290700 loss: 0.0011 lr: 0.02\n",
      "iteration: 290800 loss: 0.0013 lr: 0.02\n",
      "iteration: 290900 loss: 0.0011 lr: 0.02\n",
      "iteration: 291000 loss: 0.0012 lr: 0.02\n",
      "iteration: 291100 loss: 0.0011 lr: 0.02\n",
      "iteration: 291200 loss: 0.0012 lr: 0.02\n",
      "iteration: 291300 loss: 0.0011 lr: 0.02\n",
      "iteration: 291400 loss: 0.0011 lr: 0.02\n",
      "iteration: 291500 loss: 0.0011 lr: 0.02\n",
      "iteration: 291600 loss: 0.0010 lr: 0.02\n",
      "iteration: 291700 loss: 0.0011 lr: 0.02\n",
      "iteration: 291800 loss: 0.0012 lr: 0.02\n",
      "iteration: 291900 loss: 0.0010 lr: 0.02\n",
      "iteration: 292000 loss: 0.0011 lr: 0.02\n",
      "iteration: 292100 loss: 0.0010 lr: 0.02\n",
      "iteration: 292200 loss: 0.0011 lr: 0.02\n",
      "iteration: 292300 loss: 0.0011 lr: 0.02\n",
      "iteration: 292400 loss: 0.0012 lr: 0.02\n",
      "iteration: 292500 loss: 0.0011 lr: 0.02\n",
      "iteration: 292600 loss: 0.0012 lr: 0.02\n",
      "iteration: 292700 loss: 0.0011 lr: 0.02\n",
      "iteration: 292800 loss: 0.0010 lr: 0.02\n",
      "iteration: 292900 loss: 0.0011 lr: 0.02\n",
      "iteration: 293000 loss: 0.0011 lr: 0.02\n",
      "iteration: 293100 loss: 0.0011 lr: 0.02\n",
      "iteration: 293200 loss: 0.0010 lr: 0.02\n",
      "iteration: 293300 loss: 0.0011 lr: 0.02\n",
      "iteration: 293400 loss: 0.0011 lr: 0.02\n",
      "iteration: 293500 loss: 0.0013 lr: 0.02\n",
      "iteration: 293600 loss: 0.0011 lr: 0.02\n",
      "iteration: 293700 loss: 0.0012 lr: 0.02\n",
      "iteration: 293800 loss: 0.0011 lr: 0.02\n",
      "iteration: 293900 loss: 0.0011 lr: 0.02\n",
      "iteration: 294000 loss: 0.0012 lr: 0.02\n",
      "iteration: 294100 loss: 0.0012 lr: 0.02\n",
      "iteration: 294200 loss: 0.0011 lr: 0.02\n",
      "iteration: 294300 loss: 0.0011 lr: 0.02\n",
      "iteration: 294400 loss: 0.0010 lr: 0.02\n",
      "iteration: 294500 loss: 0.0011 lr: 0.02\n",
      "iteration: 294600 loss: 0.0012 lr: 0.02\n",
      "iteration: 294700 loss: 0.0012 lr: 0.02\n",
      "iteration: 294800 loss: 0.0012 lr: 0.02\n",
      "iteration: 294900 loss: 0.0011 lr: 0.02\n",
      "iteration: 295000 loss: 0.0011 lr: 0.02\n",
      "iteration: 295100 loss: 0.0011 lr: 0.02\n",
      "iteration: 295200 loss: 0.0012 lr: 0.02\n",
      "iteration: 295300 loss: 0.0011 lr: 0.02\n",
      "iteration: 295400 loss: 0.0012 lr: 0.02\n",
      "iteration: 295500 loss: 0.0012 lr: 0.02\n",
      "iteration: 295600 loss: 0.0011 lr: 0.02\n",
      "iteration: 295700 loss: 0.0010 lr: 0.02\n",
      "iteration: 295800 loss: 0.0012 lr: 0.02\n",
      "iteration: 295900 loss: 0.0011 lr: 0.02\n",
      "iteration: 296000 loss: 0.0010 lr: 0.02\n",
      "iteration: 296100 loss: 0.0012 lr: 0.02\n",
      "iteration: 296200 loss: 0.0011 lr: 0.02\n",
      "iteration: 296300 loss: 0.0013 lr: 0.02\n",
      "iteration: 296400 loss: 0.0012 lr: 0.02\n",
      "iteration: 296500 loss: 0.0012 lr: 0.02\n",
      "iteration: 296600 loss: 0.0011 lr: 0.02\n",
      "iteration: 296700 loss: 0.0010 lr: 0.02\n",
      "iteration: 296800 loss: 0.0014 lr: 0.02\n",
      "iteration: 296900 loss: 0.0012 lr: 0.02\n",
      "iteration: 297000 loss: 0.0014 lr: 0.02\n",
      "iteration: 297100 loss: 0.0012 lr: 0.02\n",
      "iteration: 297200 loss: 0.0012 lr: 0.02\n",
      "iteration: 297300 loss: 0.0010 lr: 0.02\n",
      "iteration: 297400 loss: 0.0011 lr: 0.02\n",
      "iteration: 297500 loss: 0.0010 lr: 0.02\n",
      "iteration: 297600 loss: 0.0011 lr: 0.02\n",
      "iteration: 297700 loss: 0.0011 lr: 0.02\n",
      "iteration: 297800 loss: 0.0014 lr: 0.02\n",
      "iteration: 297900 loss: 0.0013 lr: 0.02\n",
      "iteration: 298000 loss: 0.0012 lr: 0.02\n",
      "iteration: 298100 loss: 0.0010 lr: 0.02\n",
      "iteration: 298200 loss: 0.0011 lr: 0.02\n",
      "iteration: 298300 loss: 0.0012 lr: 0.02\n",
      "iteration: 298400 loss: 0.0011 lr: 0.02\n",
      "iteration: 298500 loss: 0.0010 lr: 0.02\n",
      "iteration: 298600 loss: 0.0012 lr: 0.02\n",
      "iteration: 298700 loss: 0.0012 lr: 0.02\n",
      "iteration: 298800 loss: 0.0011 lr: 0.02\n",
      "iteration: 298900 loss: 0.0011 lr: 0.02\n",
      "iteration: 299000 loss: 0.0012 lr: 0.02\n",
      "iteration: 299100 loss: 0.0011 lr: 0.02\n",
      "iteration: 299200 loss: 0.0011 lr: 0.02\n",
      "iteration: 299300 loss: 0.0012 lr: 0.02\n",
      "iteration: 299400 loss: 0.0010 lr: 0.02\n",
      "iteration: 299500 loss: 0.0011 lr: 0.02\n",
      "iteration: 299600 loss: 0.0013 lr: 0.02\n",
      "iteration: 299700 loss: 0.0012 lr: 0.02\n",
      "iteration: 299800 loss: 0.0012 lr: 0.02\n",
      "iteration: 299900 loss: 0.0011 lr: 0.02\n",
      "iteration: 300000 loss: 0.0011 lr: 0.02\n",
      "Exception in thread Thread-21:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1334, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1319, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1407, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\n",
      "\t [[{{node fifo_queue_enqueue}} = QueueEnqueueV2[Tcomponents=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fifo_queue, _arg_Placeholder_0_0, _arg_Placeholder_1_0_1, _arg_Placeholder_2_0_2, _arg_Placeholder_3_0_3, _arg_Placeholder_4_0_4)]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py\", line 53, in load_and_enqueue\n",
      "    sess.run(enqueue_op, feed_dict=food)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1152, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1328, in _do_run\n",
      "    run_metadata)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1348, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\n",
      "\t [[node fifo_queue_enqueue (defined at C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py:39)  = QueueEnqueueV2[Tcomponents=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fifo_queue, _arg_Placeholder_0_0, _arg_Placeholder_1_0_1, _arg_Placeholder_2_0_2, _arg_Placeholder_3_0_3, _arg_Placeholder_4_0_4)]]\n",
      "\n",
      "Caused by op 'fifo_queue_enqueue', defined at:\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\asyncio\\events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2683, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2787, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2847, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-36-c796e1806e5d>\", line 5, in <module>\n",
      "    deeplabcut.train_network(path_config_file, saveiters=1000, displayiters=100, maxiters=300001)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\training.py\", line 87, in train_network\n",
      "    train(str(poseconfigfile),displayiters,saveiters,maxiters,max_to_keep=max_snapshots_to_keep) #pass on path and file name for pose_cfg.yaml!\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py\", line 88, in train\n",
      "    batch, enqueue_op, placeholders = setup_preloading(batch_spec)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py\", line 39, in setup_preloading\n",
      "    enqueue_op = q.enqueue(placeholders_list)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\ops\\data_flow_ops.py\", line 341, in enqueue\n",
      "    self._queue_ref, vals, name=scope)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\ops\\gen_data_flow_ops.py\", line 3984, in queue_enqueue_v2\n",
      "    timeout_ms=timeout_ms, name=name)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "CancelledError (see above for traceback): Enqueue operation was cancelled\n",
      "\t [[node fifo_queue_enqueue (defined at C:\\Users\\vjj14\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py:39)  = QueueEnqueueV2[Tcomponents=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fifo_queue, _arg_Placeholder_0_0, _arg_Placeholder_1_0_1, _arg_Placeholder_2_0_2, _arg_Placeholder_3_0_3, _arg_Placeholder_4_0_4)]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network.\n",
      "log written\n",
      "log written\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "write_log(str(start) + \"***train_network start\")\n",
    "write_log(\"   DESCRIPTION:\" + prompt_with_timeout(\"Training description(Network, Iterations, etc) \"))\n",
    "\n",
    "deeplabcut.train_network(path_config_file, saveiters=1000, displayiters=100, maxiters=300001)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "write_log(str(end) + \"***train_network end\")\n",
    "write_log(\"   time elapsed:\" + str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iter0  ~ 1 hour and 20 minutes\n",
    "#iter1 start-1:07pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "?deeplabcut.train_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xZygsb2DoEJc"
   },
   "source": [
    "## Start evaluating\n",
    "This funtion evaluates a trained model for a specific shuffle/shuffles at a particular state or all the states on the data set (images)\n",
    "and stores the results as .csv file in a subdirectory under **evaluation-results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nv4zlbrnoEJg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log written\n",
      "C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-05/evaluation-results/  already exists!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3]],\n",
      " 'all_joints_names': ['LeftHand', 'RightHand', 'Nose', 'Pellet'],\n",
      " 'batch_size': 4,\n",
      " 'bottomheight': 400,\n",
      " 'crop': True,\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets\\\\iteration-1\\\\UnaugmentedDataSet_frontslowmoJun5\\\\frontslowmo_vj95shuffle1.mat',\n",
      " 'dataset_type': 'default',\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': 'C:\\\\Users\\\\vjj14\\\\.conda\\\\envs\\\\dlc-windowsGPU\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'leftwidth': 400,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets\\\\iteration-1\\\\UnaugmentedDataSet_frontslowmoJun5\\\\Documentation_data-frontslowmo_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'minsize': 100,\n",
      " 'mirror': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 4,\n",
      " 'optimizer': 'sgd',\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': 'C:\\\\Users\\\\vjj14\\\\Desktop\\\\DeepLabCut\\\\frontslowmo-vj-2019-06-05',\n",
      " 'regularize': False,\n",
      " 'rightwidth': 400,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'C:\\\\Users\\\\vjj14\\\\Desktop\\\\DeepLabCut\\\\frontslowmo-vj-2019-06-05\\\\dlc-models\\\\iteration-1\\\\frontslowmoJun5-trainset95shuffle1\\\\test\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'topheight': 400,\n",
      " 'use_gt_segm': False,\n",
      " 'video': False,\n",
      " 'video_batch': False,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-05\\evaluation-results\\iteration-1\\frontslowmoJun5-trainset95shuffle1  already exists!\n",
      "Running  DeepCut_resnet50_frontslowmoJun5shuffle1_297000  with # of trainingiterations: 297000\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-05\\dlc-models\\iteration-1\\frontslowmoJun5-trainset95shuffle1\\train\\snapshot-297000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-05\\dlc-models\\iteration-1\\frontslowmoJun5-trainset95shuffle1\\train\\snapshot-297000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "204it [00:37,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done and results stored for snapshot:  snapshot-297000\n",
      "Results for 297000  training iterations: 95 1 train error: 1.72 pixels. Test error: 21.37  pixels.\n",
      "With pcutoff of 0.1  train error: 1.72 pixels. Test error: 13.19 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
      "If it generalizes well, choose the best model for prediction and update the config file with the appropriate index for the 'snapshotindex'.\n",
      "Use the function 'analyze_video' to make predictions on new videos.\n",
      "Otherwise consider retraining the network (see DeepLabCut workflow Fig 2)\n",
      "log written\n",
      "log written\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "write_log(str(start) + \"***evaluate_network start\")\n",
    "\n",
    "deeplabcut.evaluate_network(path_config_file)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "write_log(str(end) + \"***evalaute_network end\")\n",
    "write_log(\"   time elapsed:\" + str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVFLSKKfoEJk"
   },
   "source": [
    "## Start Analyzing videos\n",
    "This function analyzes the new video. The user can choose the best model from the evaluation results and specify the correct snapshot index for the variable **snapshotindex** in the **config.yaml** file. Otherwise, by default the most recent snapshot is used to analyse the video.\n",
    "\n",
    "The results are stored in hd5 file in the same directory where the video resides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_LZiS_0oEJl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log written\n",
      "log written\n",
      "Using snapshot-28500 for model C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-05\\dlc-models\\iteration-0\\frontslowmoJun5-trainset95shuffle1\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-05\\dlc-models\\iteration-0\\frontslowmoJun5-trainset95shuffle1\\train\\snapshot-28500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-05\\dlc-models\\iteration-0\\frontslowmoJun5-trainset95shuffle1\\train\\snapshot-28500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  C:\\Users\\vjj14\\Desktop\\DeepLabCut\\paperfrontslowmo-vj-2019-06-10\\videos\\paperfrontslowmo_Trim.mp4\n",
      "Loading  C:\\Users\\vjj14\\Desktop\\DeepLabCut\\paperfrontslowmo-vj-2019-06-10\\videos\\paperfrontslowmo_Trim.mp4\n",
      "Duration of video [s]:  52.7 , recorded with  30.0 fps!\n",
      "Overall # of frames:  1581  found with (before cropping) frame dimensions:  1080 1920\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1590it [02:09, 12.83it/s]                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  1581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in C:\\Users\\vjj14\\Desktop\\DeepLabCut\\paperfrontslowmo-vj-2019-06-10\\videos...\n",
      "Saving csv poses!\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!\n",
      "log written\n",
      "log written\n"
     ]
    }
   ],
   "source": [
    "# videofile_path = [r\"C:\\Users\\vjj14\\Downloads\\compressed25.mp4\"] #Enter the list of videos to analyze.\n",
    "# videos=[[r\"C:\\Users\\vjj14\\Downloads\\frontslowmoCOMPRESSED\\compressed50.mp4\"], [r\"C:\\Users\\vjj14\\Downloads\\frontslowmoCOMPRESSED\\compressed30.mp4\"], [r\"C:\\Users\\vjj14\\Downloads\\frontslowmoCOMPRESSED\\compressed27.mp4\"], [r\"C:\\Users\\vjj14\\Downloads\\frontslowmoCOMPRESSED\\compressed25.mp4\"], [r\"C:\\Users\\vjj14\\Downloads\\frontslowmoCOMPRESSED\\compressed20.mp4\"]]\n",
    "videos=[[r\"C:\\Users\\vjj14\\Desktop\\DeepLabCut\\paperfrontslowmo-vj-2019-06-10\\videos\\paperfrontslowmo_Trim.mp4\"]]\n",
    "for videofile_path in videos:\n",
    "    start = datetime.datetime.now()\n",
    "    write_log(str(start) + \"***analyze_videos start\")\n",
    "    write_log(\"   VIDEOS: \" + str(videofile_path))\n",
    "\n",
    "    deeplabcut.analyze_videos(path_config_file,videofile_path, save_as_csv=True)\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    write_log(str(end) + \"***analyze_videos end\")\n",
    "    write_log(\"   time elapsed:\" + str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep Track of time for analyzing videos\n",
    "#Original: 940 mb\n",
    "#25:134 mb, took 20:55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|████████████▎                                                                | 2480/15561 [03:30<17:57, 12.14it/s]"
     ]
    }
   ],
   "source": [
    "?deeplabcut.analyze_videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iGu_PdTWoEJr"
   },
   "source": [
    "## Extract outlier frames [optional step]\n",
    "This is an optional step and is used only when the evaluation results are poor i.e. the labels are incorrectly predicted. In such a case, the user can use the following function to extract frames where the labels are incorrectly predicted. Make sure to provide the correct value of the \"iterations\" as it will be used to create the unique directory where the extracted frames will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gkbaBOJVoEJs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network parameters: DeepCut_resnet50_frontslowmoJun5shuffle1_28000\n",
      "Method  uncertain  found  12  putative outlier frames.\n",
      "Do you want to proceed with extracting  200  of those?\n",
      "If this list is very large, perhaps consider changing the paramters (start, stop, p_bound, comparisonbodyparts) or use a different method.\n",
      "yes/noyes\n",
      "Frames from video 1080p  already extracted (more will be added)!\n",
      "Loading video...\n",
      "Duration of video [s]:  518.6666666666666 , recorded @  30.0 fps!\n",
      "Overall # of frames:  15560 with (cropped) frame dimensions: \n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 518.7  seconds.\n",
      "Let's select frames indices: [113, 154, 155, 156, 157, 158, 160, 161, 162, 13116, 13117, 13126]\n",
      "New video was added to the project! Use the function 'extract_frames' to select frames for labeling.\n",
      "The outlier frames are extracted. They are stored in the subdirectory labeled-data\\1080p.\n",
      "Once you extracted frames for all videos, use 'refine_labels' to manually correct the labels.\n"
     ]
    }
   ],
   "source": [
    "alg = 'uncertain'\n",
    "param = 0.0002 #epsilon or p_bound\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "write_log(str(start) + \"***extract_outlier_frames start\")\n",
    "\n",
    "deeplabcut.extract_outlier_frames(path_config_file, videofile_path, outlieralgorithm=alg, p_bound=param)\n",
    "num_frames = prompt_with_timeout(\"number of frames selected?\")\n",
    "write_log(\"   VIDEOS: {0}, ALGORITHM: {1}, PARAMETER: {2}, NUM FRAMES: {3}\".format(str(videofile_path), alg, param, num_frames))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "write_log(str(end) + \"***analyze_videos end\")\n",
    "write_log(\"   time elapsed:\" + str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\vjj14\\\\Desktop\\\\DeepLabCut\\\\frontslowmo-vj-2019-06-05\\\\videos\\\\1080p.MOV']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videofile_path = [r\"C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-05\\videos\\1080p.MOV\"]\n",
    "videofile_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "?deeplabcut.extract_outlier_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ib0uvhaoEJx"
   },
   "source": [
    "## Refine Labels [optional step]\n",
    "Following the extraction of outlier frames, the user can use the following function to move the predicted labels to the correct location. Thus augmenting the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_FpEXtyoEJy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Cannot activate multiple GUI eventloops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows\n",
      "Checking labels if they are outside the image\n",
      "A training dataset file is already found for this video. The refined machine labels are merged to this data!\n",
      "Closing... The refined labels are stored in a subdirectory under labeled-data. Use the function 'merge_datasets' to augment the training dataset, and then re-train a network using create_training_dataset followed by train_network!\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "write_log(str(start) + \"***refine_frames start\")\n",
    "\n",
    "%gui wx\n",
    "deeplabcut.refine_labels(path_config_file)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "write_log(str(end) + \"***refine_frames end\")\n",
    "write_log(\"   time elapsed:\" + str(end-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CHzstWr8oEJ2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data sets and updated refinement iteration to 2.\n",
      "Now you can create a new training set for the expanded annotated images (use create_training_dataset).\n"
     ]
    }
   ],
   "source": [
    "#Once all folders are relabeled, check them and advance. See how to check labels, above!\n",
    "start = datetime.datetime.now()\n",
    "write_log(str(start) + \"***refine_frames start\")\n",
    "\n",
    "deeplabcut.merge_datasets(path_config_file)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "write_log(str(end) + \"***refine_frames end\")\n",
    "write_log(\"   time elapsed:\" + str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCHj7qyboEJ6"
   },
   "source": [
    "## Create a new iteration of training dataset [optional step]\n",
    "Following the refine labels, append these frames to the original dataset to create a new iteration of training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytQoxIldoEJ7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "write_log(str(start) + \"***create_training_dataset start\")\n",
    "\n",
    "deeplabcut.create_training_dataset(path_config_file)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "write_log(str(end) + \"***create_training_dataset end\")\n",
    "write_log(\"   time elapsed:\" + str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCrUvQIvoEKD"
   },
   "source": [
    "## Create labeled video\n",
    "This funtion is for visualiztion purpose and can be used to create a video in .mp4 format with labels predicted by the network. This video is saved in the same directory where the original video resides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aDF7Q7KoEKE",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log written\n",
      "log written\n",
      "Starting %  C:\\Users\\vjj14\\Desktop\\DeepLabCut\\paperfrontslowmo-vj-2019-06-10\\videos ['C:\\\\Users\\\\vjj14\\\\Desktop\\\\DeepLabCut\\\\paperfrontslowmo-vj-2019-06-10\\\\videos\\\\paperfrontslowmo_Trim.mp4']\n",
      "Loading  C:\\Users\\vjj14\\Desktop\\DeepLabCut\\paperfrontslowmo-vj-2019-06-10\\videos\\paperfrontslowmo_Trim.mp4 and data.\n",
      "False 0 1080 0 1920\n",
      "1581\n",
      "Duration of video [s]:  52.7 , recorded with  30.0 fps!\n",
      "Overall # of frames:  1581 with cropped frame dimensions:  1080 1920\n",
      "Generating frames and creating video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1581/1581 [00:26<00:00, 59.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log written\n",
      "log written\n"
     ]
    }
   ],
   "source": [
    "# videos=[[r\"C:\\Users\\vjj14\\Downloads\\frontslowmoCOMPRESSED\\compressed50.mp4\"], [r\"C:\\Users\\vjj14\\Downloads\\frontslowmoCOMPRESSED\\compressed30.mp4\"], [r\"C:\\Users\\vjj14\\Downloads\\frontslowmoCOMPRESSED\\compressed27.mp4\"], [r\"C:\\Users\\vjj14\\Downloads\\frontslowmoCOMPRESSED\\compressed25.mp4\"], [r\"C:\\Users\\vjj14\\Downloads\\frontslowmoCOMPRESSED\\compressed20.mp4\"]]\n",
    "videos=[[r\"C:\\Users\\vjj14\\Desktop\\DeepLabCut\\paperfrontslowmo-vj-2019-06-10\\videos\\paperfrontslowmo_Trim.mp4\"]]\n",
    "for videofile_path in videos:\n",
    "    start = datetime.datetime.now()\n",
    "    write_log(str(start) + \"***create_labeled_video start\")\n",
    "    write_log(\"   VIDEOS: {}\".format(str(videofile_path)))\n",
    "\n",
    "    deeplabcut.create_labeled_video(path_config_file,videofile_path)\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    write_log(str(end) + \"***create_labeled_video end\")\n",
    "    write_log(\"   time elapsed:\" + str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "?deeplabcut.create_labeled_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GTiuJESoEKH"
   },
   "source": [
    "## Plot the trajectories of the analyzed videos\n",
    "This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\vjj14\\\\Desktop\\\\DeepLabCut\\\\frontslowmo-vj-2019-06-05\\\\videos\\\\1080p.MOV']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videofile_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gX21zZbXoEKJ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "#for making interactive plots.\n",
    "deeplabcut.plot_trajectories(path_config_file,videofile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "?deeplabcut.plot_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\vjj14\\\\Desktop\\\\DeepLabCut\\\\frontslowmo-vj-2019-06-05\\\\config.yaml'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_config_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>scorer</th>\n",
       "      <th colspan=\"12\" halign=\"left\">DeepCut_resnet50_frontslowmoJun5shuffle1_30000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bodyparts</th>\n",
       "      <th colspan=\"3\" halign=\"left\">LeftHand</th>\n",
       "      <th colspan=\"3\" halign=\"left\">RightHand</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Nose</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Pellet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coords</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15560.000000</td>\n",
       "      <td>15560.000000</td>\n",
       "      <td>15560.000000</td>\n",
       "      <td>15560.000000</td>\n",
       "      <td>15560.000000</td>\n",
       "      <td>15560.000000</td>\n",
       "      <td>15560.000000</td>\n",
       "      <td>15560.000000</td>\n",
       "      <td>15560.000000</td>\n",
       "      <td>15560.000000</td>\n",
       "      <td>15560.000000</td>\n",
       "      <td>15560.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>971.243753</td>\n",
       "      <td>725.853752</td>\n",
       "      <td>0.699717</td>\n",
       "      <td>906.089441</td>\n",
       "      <td>738.766505</td>\n",
       "      <td>0.503567</td>\n",
       "      <td>931.766309</td>\n",
       "      <td>656.488581</td>\n",
       "      <td>0.671245</td>\n",
       "      <td>957.103689</td>\n",
       "      <td>811.306070</td>\n",
       "      <td>0.322458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>171.896374</td>\n",
       "      <td>139.390499</td>\n",
       "      <td>0.357561</td>\n",
       "      <td>156.673818</td>\n",
       "      <td>166.830793</td>\n",
       "      <td>0.400728</td>\n",
       "      <td>169.306891</td>\n",
       "      <td>191.976523</td>\n",
       "      <td>0.363396</td>\n",
       "      <td>159.386299</td>\n",
       "      <td>177.523694</td>\n",
       "      <td>0.418997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.870381</td>\n",
       "      <td>0.415488</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>319.477097</td>\n",
       "      <td>1.081914</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>298.748627</td>\n",
       "      <td>-0.059892</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>-5.040360</td>\n",
       "      <td>0.012989</td>\n",
       "      <td>0.000098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>914.894261</td>\n",
       "      <td>649.265245</td>\n",
       "      <td>0.397388</td>\n",
       "      <td>850.580156</td>\n",
       "      <td>651.611459</td>\n",
       "      <td>0.060324</td>\n",
       "      <td>909.805872</td>\n",
       "      <td>595.394240</td>\n",
       "      <td>0.306699</td>\n",
       "      <td>922.235798</td>\n",
       "      <td>671.503496</td>\n",
       "      <td>0.027885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1024.541650</td>\n",
       "      <td>783.569860</td>\n",
       "      <td>0.910440</td>\n",
       "      <td>905.468754</td>\n",
       "      <td>778.514918</td>\n",
       "      <td>0.466857</td>\n",
       "      <td>918.396317</td>\n",
       "      <td>703.317179</td>\n",
       "      <td>0.875269</td>\n",
       "      <td>939.903313</td>\n",
       "      <td>847.006353</td>\n",
       "      <td>0.048265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1064.598903</td>\n",
       "      <td>821.210082</td>\n",
       "      <td>0.986921</td>\n",
       "      <td>940.134475</td>\n",
       "      <td>858.174915</td>\n",
       "      <td>0.949224</td>\n",
       "      <td>928.343641</td>\n",
       "      <td>794.728728</td>\n",
       "      <td>0.960582</td>\n",
       "      <td>953.739810</td>\n",
       "      <td>940.497040</td>\n",
       "      <td>0.954270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1611.307688</td>\n",
       "      <td>1088.421771</td>\n",
       "      <td>0.999329</td>\n",
       "      <td>1609.412701</td>\n",
       "      <td>1092.391703</td>\n",
       "      <td>0.999041</td>\n",
       "      <td>1623.930874</td>\n",
       "      <td>1088.734671</td>\n",
       "      <td>0.998151</td>\n",
       "      <td>1613.409633</td>\n",
       "      <td>1089.468144</td>\n",
       "      <td>0.997180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "scorer    DeepCut_resnet50_frontslowmoJun5shuffle1_30000                \\\n",
       "bodyparts                                       LeftHand                 \n",
       "coords                                                 x             y   \n",
       "count                                       15560.000000  15560.000000   \n",
       "mean                                          971.243753    725.853752   \n",
       "std                                           171.896374    139.390499   \n",
       "min                                            -3.870381      0.415488   \n",
       "25%                                           914.894261    649.265245   \n",
       "50%                                          1024.541650    783.569860   \n",
       "75%                                          1064.598903    821.210082   \n",
       "max                                          1611.307688   1088.421771   \n",
       "\n",
       "scorer                                                             \\\n",
       "bodyparts                   RightHand                               \n",
       "coords       likelihood             x             y    likelihood   \n",
       "count      15560.000000  15560.000000  15560.000000  15560.000000   \n",
       "mean           0.699717    906.089441    738.766505      0.503567   \n",
       "std            0.357561    156.673818    166.830793      0.400728   \n",
       "min            0.000027    319.477097      1.081914      0.000040   \n",
       "25%            0.397388    850.580156    651.611459      0.060324   \n",
       "50%            0.910440    905.468754    778.514918      0.466857   \n",
       "75%            0.986921    940.134475    858.174915      0.949224   \n",
       "max            0.999329   1609.412701   1092.391703      0.999041   \n",
       "\n",
       "scorer                                                             \\\n",
       "bodyparts          Nose                                    Pellet   \n",
       "coords                x             y    likelihood             x   \n",
       "count      15560.000000  15560.000000  15560.000000  15560.000000   \n",
       "mean         931.766309    656.488581      0.671245    957.103689   \n",
       "std          169.306891    191.976523      0.363396    159.386299   \n",
       "min          298.748627     -0.059892      0.000104     -5.040360   \n",
       "25%          909.805872    595.394240      0.306699    922.235798   \n",
       "50%          918.396317    703.317179      0.875269    939.903313   \n",
       "75%          928.343641    794.728728      0.960582    953.739810   \n",
       "max         1623.930874   1088.734671      0.998151   1613.409633   \n",
       "\n",
       "scorer                                 \n",
       "bodyparts                              \n",
       "coords                y    likelihood  \n",
       "count      15560.000000  15560.000000  \n",
       "mean         811.306070      0.322458  \n",
       "std          177.523694      0.418997  \n",
       "min            0.012989      0.000098  \n",
       "25%          671.503496      0.027885  \n",
       "50%          847.006353      0.048265  \n",
       "75%          940.497040      0.954270  \n",
       "max         1089.468144      0.997180  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_hdf(r\"C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-05\\videos\\1080pDeepCut_resnet50_frontslowmoJun5shuffle1_30000.h5\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>scorer</th>\n",
       "      <th colspan=\"12\" halign=\"left\">DeepCut_resnet50_front4k60fpszoom3.0_TrimJun3shuffle1_286000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bodyparts</th>\n",
       "      <th colspan=\"3\" halign=\"left\">LeftHand</th>\n",
       "      <th colspan=\"3\" halign=\"left\">RightHand</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Nose</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Pellet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coords</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>612.000000</td>\n",
       "      <td>612.000000</td>\n",
       "      <td>612.000000</td>\n",
       "      <td>612.000000</td>\n",
       "      <td>612.000000</td>\n",
       "      <td>612.000000</td>\n",
       "      <td>612.000000</td>\n",
       "      <td>612.000000</td>\n",
       "      <td>612.000000</td>\n",
       "      <td>612.000000</td>\n",
       "      <td>612.000000</td>\n",
       "      <td>612.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1294.587569</td>\n",
       "      <td>2455.822662</td>\n",
       "      <td>0.042246</td>\n",
       "      <td>609.627308</td>\n",
       "      <td>2811.817150</td>\n",
       "      <td>0.078364</td>\n",
       "      <td>1000.140640</td>\n",
       "      <td>2138.825481</td>\n",
       "      <td>0.222379</td>\n",
       "      <td>1161.420468</td>\n",
       "      <td>3607.459208</td>\n",
       "      <td>0.364173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>566.489027</td>\n",
       "      <td>890.589556</td>\n",
       "      <td>0.138305</td>\n",
       "      <td>523.969844</td>\n",
       "      <td>763.555947</td>\n",
       "      <td>0.202453</td>\n",
       "      <td>249.147238</td>\n",
       "      <td>615.631056</td>\n",
       "      <td>0.313997</td>\n",
       "      <td>635.214989</td>\n",
       "      <td>435.197877</td>\n",
       "      <td>0.308807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.173059</td>\n",
       "      <td>1.988913</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.665848</td>\n",
       "      <td>76.401595</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.269251</td>\n",
       "      <td>865.347540</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>2.184450</td>\n",
       "      <td>1940.036492</td>\n",
       "      <td>0.000428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>955.338525</td>\n",
       "      <td>1989.080632</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>3.883312</td>\n",
       "      <td>2168.396138</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>990.724893</td>\n",
       "      <td>1820.097459</td>\n",
       "      <td>0.004228</td>\n",
       "      <td>736.731850</td>\n",
       "      <td>3637.395378</td>\n",
       "      <td>0.070970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1458.549845</td>\n",
       "      <td>2089.116435</td>\n",
       "      <td>0.002397</td>\n",
       "      <td>764.576838</td>\n",
       "      <td>2395.076544</td>\n",
       "      <td>0.003136</td>\n",
       "      <td>1030.763698</td>\n",
       "      <td>1986.446780</td>\n",
       "      <td>0.050586</td>\n",
       "      <td>963.540944</td>\n",
       "      <td>3822.212681</td>\n",
       "      <td>0.276980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1644.293059</td>\n",
       "      <td>3421.296761</td>\n",
       "      <td>0.011128</td>\n",
       "      <td>970.543810</td>\n",
       "      <td>3556.132724</td>\n",
       "      <td>0.019450</td>\n",
       "      <td>1069.846140</td>\n",
       "      <td>2166.098999</td>\n",
       "      <td>0.300523</td>\n",
       "      <td>1822.975545</td>\n",
       "      <td>3837.997508</td>\n",
       "      <td>0.658898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2161.044888</td>\n",
       "      <td>3840.645847</td>\n",
       "      <td>0.977641</td>\n",
       "      <td>2139.572770</td>\n",
       "      <td>3840.410774</td>\n",
       "      <td>0.992057</td>\n",
       "      <td>2152.967607</td>\n",
       "      <td>3841.093842</td>\n",
       "      <td>0.998524</td>\n",
       "      <td>2158.995823</td>\n",
       "      <td>3841.865475</td>\n",
       "      <td>0.989263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "scorer    DeepCut_resnet50_front4k60fpszoom3.0_TrimJun3shuffle1_286000  \\\n",
       "bodyparts                                                     LeftHand   \n",
       "coords                                                               x   \n",
       "count                                             612.000000             \n",
       "mean                                             1294.587569             \n",
       "std                                               566.489027             \n",
       "min                                                -0.173059             \n",
       "25%                                               955.338525             \n",
       "50%                                              1458.549845             \n",
       "75%                                              1644.293059             \n",
       "max                                              2161.044888             \n",
       "\n",
       "scorer                                                                    \\\n",
       "bodyparts                             RightHand                            \n",
       "coords               y  likelihood            x            y  likelihood   \n",
       "count       612.000000  612.000000   612.000000   612.000000  612.000000   \n",
       "mean       2455.822662    0.042246   609.627308  2811.817150    0.078364   \n",
       "std         890.589556    0.138305   523.969844   763.555947    0.202453   \n",
       "min           1.988913    0.000032     0.665848    76.401595    0.000153   \n",
       "25%        1989.080632    0.000861     3.883312  2168.396138    0.001087   \n",
       "50%        2089.116435    0.002397   764.576838  2395.076544    0.003136   \n",
       "75%        3421.296761    0.011128   970.543810  3556.132724    0.019450   \n",
       "max        3840.645847    0.977641  2139.572770  3840.410774    0.992057   \n",
       "\n",
       "scorer                                                                     \\\n",
       "bodyparts         Nose                                Pellet                \n",
       "coords               x            y  likelihood            x            y   \n",
       "count       612.000000   612.000000  612.000000   612.000000   612.000000   \n",
       "mean       1000.140640  2138.825481    0.222379  1161.420468  3607.459208   \n",
       "std         249.147238   615.631056    0.313997   635.214989   435.197877   \n",
       "min           0.269251   865.347540    0.000097     2.184450  1940.036492   \n",
       "25%         990.724893  1820.097459    0.004228   736.731850  3637.395378   \n",
       "50%        1030.763698  1986.446780    0.050586   963.540944  3822.212681   \n",
       "75%        1069.846140  2166.098999    0.300523  1822.975545  3837.997508   \n",
       "max        2152.967607  3841.093842    0.998524  2158.995823  3841.865475   \n",
       "\n",
       "scorer                 \n",
       "bodyparts              \n",
       "coords     likelihood  \n",
       "count      612.000000  \n",
       "mean         0.364173  \n",
       "std          0.308807  \n",
       "min          0.000428  \n",
       "25%          0.070970  \n",
       "50%          0.276980  \n",
       "75%          0.658898  \n",
       "max          0.989263  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_hdf(r\"C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-05\\videos\\1080pDeepCut_resnet50_frontslowmoJun5shuffle1_30000.h5\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>scorer</th>\n",
       "      <th colspan=\"12\" halign=\"left\">DeepCut_resnet50_front4k60fpszoom3.0_TrimJun3shuffle1_286000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bodyparts</th>\n",
       "      <th colspan=\"3\" halign=\"left\">LeftHand</th>\n",
       "      <th colspan=\"3\" halign=\"left\">RightHand</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Nose</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Pellet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coords</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>396.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1081.024990</td>\n",
       "      <td>1915.892243</td>\n",
       "      <td>0.028620</td>\n",
       "      <td>507.260805</td>\n",
       "      <td>2028.133410</td>\n",
       "      <td>0.074291</td>\n",
       "      <td>832.015528</td>\n",
       "      <td>2042.639285</td>\n",
       "      <td>0.122190</td>\n",
       "      <td>1359.938273</td>\n",
       "      <td>3524.089973</td>\n",
       "      <td>0.122058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>801.865156</td>\n",
       "      <td>925.669503</td>\n",
       "      <td>0.089883</td>\n",
       "      <td>570.154718</td>\n",
       "      <td>789.584933</td>\n",
       "      <td>0.167471</td>\n",
       "      <td>477.821564</td>\n",
       "      <td>905.199187</td>\n",
       "      <td>0.237828</td>\n",
       "      <td>693.038635</td>\n",
       "      <td>567.594545</td>\n",
       "      <td>0.173181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.075624</td>\n",
       "      <td>4.104054</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>1.491787</td>\n",
       "      <td>0.110741</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>-1.233883</td>\n",
       "      <td>1.529313</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>1.956876</td>\n",
       "      <td>1154.500621</td>\n",
       "      <td>0.000364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>306.234073</td>\n",
       "      <td>1363.246884</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>6.579469</td>\n",
       "      <td>1460.181150</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>795.323176</td>\n",
       "      <td>1530.546586</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>649.109436</td>\n",
       "      <td>3461.410198</td>\n",
       "      <td>0.009609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1099.686393</td>\n",
       "      <td>1541.223487</td>\n",
       "      <td>0.004285</td>\n",
       "      <td>137.076892</td>\n",
       "      <td>1945.901693</td>\n",
       "      <td>0.008166</td>\n",
       "      <td>980.203261</td>\n",
       "      <td>1763.745589</td>\n",
       "      <td>0.006032</td>\n",
       "      <td>1651.501211</td>\n",
       "      <td>3830.206299</td>\n",
       "      <td>0.044030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1978.196430</td>\n",
       "      <td>2062.913943</td>\n",
       "      <td>0.013648</td>\n",
       "      <td>941.707332</td>\n",
       "      <td>2366.580038</td>\n",
       "      <td>0.052594</td>\n",
       "      <td>1059.898767</td>\n",
       "      <td>2137.143606</td>\n",
       "      <td>0.097552</td>\n",
       "      <td>1995.377958</td>\n",
       "      <td>3838.244082</td>\n",
       "      <td>0.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2162.322889</td>\n",
       "      <td>3840.759733</td>\n",
       "      <td>0.904895</td>\n",
       "      <td>2135.639879</td>\n",
       "      <td>3840.026480</td>\n",
       "      <td>0.958066</td>\n",
       "      <td>2155.672404</td>\n",
       "      <td>3839.283205</td>\n",
       "      <td>0.997360</td>\n",
       "      <td>2159.782144</td>\n",
       "      <td>3843.716391</td>\n",
       "      <td>0.900990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "scorer    DeepCut_resnet50_front4k60fpszoom3.0_TrimJun3shuffle1_286000  \\\n",
       "bodyparts                                                     LeftHand   \n",
       "coords                                                               x   \n",
       "count                                             396.000000             \n",
       "mean                                             1081.024990             \n",
       "std                                               801.865156             \n",
       "min                                                -1.075624             \n",
       "25%                                               306.234073             \n",
       "50%                                              1099.686393             \n",
       "75%                                              1978.196430             \n",
       "max                                              2162.322889             \n",
       "\n",
       "scorer                                                                    \\\n",
       "bodyparts                             RightHand                            \n",
       "coords               y  likelihood            x            y  likelihood   \n",
       "count       396.000000  396.000000   396.000000   396.000000  396.000000   \n",
       "mean       1915.892243    0.028620   507.260805  2028.133410    0.074291   \n",
       "std         925.669503    0.089883   570.154718   789.584933    0.167471   \n",
       "min           4.104054    0.000054     1.491787     0.110741    0.000132   \n",
       "25%        1363.246884    0.001314     6.579469  1460.181150    0.001657   \n",
       "50%        1541.223487    0.004285   137.076892  1945.901693    0.008166   \n",
       "75%        2062.913943    0.013648   941.707332  2366.580038    0.052594   \n",
       "max        3840.759733    0.904895  2135.639879  3840.026480    0.958066   \n",
       "\n",
       "scorer                                                                     \\\n",
       "bodyparts         Nose                                Pellet                \n",
       "coords               x            y  likelihood            x            y   \n",
       "count       396.000000   396.000000  396.000000   396.000000   396.000000   \n",
       "mean        832.015528  2042.639285    0.122190  1359.938273  3524.089973   \n",
       "std         477.821564   905.199187    0.237828   693.038635   567.594545   \n",
       "min          -1.233883     1.529313    0.000029     1.956876  1154.500621   \n",
       "25%         795.323176  1530.546586    0.000845   649.109436  3461.410198   \n",
       "50%         980.203261  1763.745589    0.006032  1651.501211  3830.206299   \n",
       "75%        1059.898767  2137.143606    0.097552  1995.377958  3838.244082   \n",
       "max        2155.672404  3839.283205    0.997360  2159.782144  3843.716391   \n",
       "\n",
       "scorer                 \n",
       "bodyparts              \n",
       "coords     likelihood  \n",
       "count      396.000000  \n",
       "mean         0.122058  \n",
       "std          0.173181  \n",
       "min          0.000364  \n",
       "25%          0.009609  \n",
       "50%          0.044030  \n",
       "75%          0.171100  \n",
       "max          0.900990  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_hdf(r\"C:\\Users\\vjj14\\Desktop\\DeepLabCut\\front4k60fpszoom3.0_Trim-nv-2019-06-03\\videos\\front4k60fpszoom3.0_Trim2DeepCut_resnet50_front4k60fpszoom3.0_TrimJun3shuffle1_286000.h5\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>scorer</th>\n",
       "      <th colspan=\"12\" halign=\"left\">DeepCut_resnet50_front4k60fpszoom3.0_TrimJun3shuffle1_286000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bodyparts</th>\n",
       "      <th colspan=\"3\" halign=\"left\">LeftHand</th>\n",
       "      <th colspan=\"3\" halign=\"left\">RightHand</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Nose</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Pellet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coords</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15512.000000</td>\n",
       "      <td>15512.000000</td>\n",
       "      <td>15512.000000</td>\n",
       "      <td>15512.000000</td>\n",
       "      <td>15512.000000</td>\n",
       "      <td>15512.000000</td>\n",
       "      <td>15512.000000</td>\n",
       "      <td>15512.000000</td>\n",
       "      <td>15512.000000</td>\n",
       "      <td>15512.000000</td>\n",
       "      <td>15512.000000</td>\n",
       "      <td>15512.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>476.662807</td>\n",
       "      <td>56.168319</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>94.257165</td>\n",
       "      <td>269.637234</td>\n",
       "      <td>0.001873</td>\n",
       "      <td>503.875873</td>\n",
       "      <td>39.304025</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>459.721134</td>\n",
       "      <td>70.262810</td>\n",
       "      <td>0.001173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>199.051516</td>\n",
       "      <td>113.435357</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>199.582534</td>\n",
       "      <td>103.977182</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>172.544028</td>\n",
       "      <td>96.968325</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>220.905013</td>\n",
       "      <td>127.125900</td>\n",
       "      <td>0.006426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.072265</td>\n",
       "      <td>-0.002832</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-1.198687</td>\n",
       "      <td>-0.875000</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>-1.059059</td>\n",
       "      <td>-0.550313</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.952420</td>\n",
       "      <td>-1.799350</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>556.934558</td>\n",
       "      <td>1.973024</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>4.560107</td>\n",
       "      <td>284.634581</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>571.378365</td>\n",
       "      <td>0.997769</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>564.175327</td>\n",
       "      <td>1.154488</td>\n",
       "      <td>0.000486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>571.585048</td>\n",
       "      <td>2.359020</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>5.130947</td>\n",
       "      <td>317.105902</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>571.993842</td>\n",
       "      <td>1.213735</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>571.777898</td>\n",
       "      <td>1.417860</td>\n",
       "      <td>0.000696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>572.115142</td>\n",
       "      <td>4.928002</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>5.801471</td>\n",
       "      <td>317.801870</td>\n",
       "      <td>0.002189</td>\n",
       "      <td>572.493355</td>\n",
       "      <td>1.552477</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>572.075551</td>\n",
       "      <td>1.793241</td>\n",
       "      <td>0.000972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>576.400276</td>\n",
       "      <td>322.130518</td>\n",
       "      <td>0.046496</td>\n",
       "      <td>573.605843</td>\n",
       "      <td>321.000363</td>\n",
       "      <td>0.067399</td>\n",
       "      <td>575.610920</td>\n",
       "      <td>321.239240</td>\n",
       "      <td>0.320831</td>\n",
       "      <td>574.988145</td>\n",
       "      <td>321.820386</td>\n",
       "      <td>0.218525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "scorer    DeepCut_resnet50_front4k60fpszoom3.0_TrimJun3shuffle1_286000  \\\n",
       "bodyparts                                                     LeftHand   \n",
       "coords                                                               x   \n",
       "count                                           15512.000000             \n",
       "mean                                              476.662807             \n",
       "std                                               199.051516             \n",
       "min                                                -3.072265             \n",
       "25%                                               556.934558             \n",
       "50%                                               571.585048             \n",
       "75%                                               572.115142             \n",
       "max                                               576.400276             \n",
       "\n",
       "scorer                                                             \\\n",
       "bodyparts                                 RightHand                 \n",
       "coords                y    likelihood             x             y   \n",
       "count      15512.000000  15512.000000  15512.000000  15512.000000   \n",
       "mean          56.168319      0.000372     94.257165    269.637234   \n",
       "std          113.435357      0.000808    199.582534    103.977182   \n",
       "min           -0.002832      0.000023     -1.198687     -0.875000   \n",
       "25%            1.973024      0.000192      4.560107    284.634581   \n",
       "50%            2.359020      0.000286      5.130947    317.105902   \n",
       "75%            4.928002      0.000423      5.801471    317.801870   \n",
       "max          322.130518      0.046496    573.605843    321.000363   \n",
       "\n",
       "scorer                                                             \\\n",
       "bodyparts                        Nose                               \n",
       "coords       likelihood             x             y    likelihood   \n",
       "count      15512.000000  15512.000000  15512.000000  15512.000000   \n",
       "mean           0.001873    503.875873     39.304025      0.000522   \n",
       "std            0.002485    172.544028     96.968325      0.004167   \n",
       "min            0.000019     -1.059059     -0.550313      0.000017   \n",
       "25%            0.000632    571.378365      0.997769      0.000229   \n",
       "50%            0.001176    571.993842      1.213735      0.000329   \n",
       "75%            0.002189    572.493355      1.552477      0.000470   \n",
       "max            0.067399    575.610920    321.239240      0.320831   \n",
       "\n",
       "scorer                                               \n",
       "bodyparts        Pellet                              \n",
       "coords                x             y    likelihood  \n",
       "count      15512.000000  15512.000000  15512.000000  \n",
       "mean         459.721134     70.262810      0.001173  \n",
       "std          220.905013    127.125900      0.006426  \n",
       "min           -0.952420     -1.799350      0.000008  \n",
       "25%          564.175327      1.154488      0.000486  \n",
       "50%          571.777898      1.417860      0.000696  \n",
       "75%          572.075551      1.793241      0.000972  \n",
       "max          574.988145    321.820386      0.218525  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_hdf(r\"C:\\Users\\vjj14\\Desktop\\DeepLabCut\\front4k60fpszoom3.0_Trim-nv-2019-06-03\\videos\\front1080p240fpszoom1.0DeepCut_resnet50_front4k60fpszoom3.0_TrimJun3shuffle1_286000.h5\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scorer</th>\n",
       "      <th>DeepCut_resnet50_frontslowmoJun5shuffle1_28000</th>\n",
       "      <th>DeepCut_resnet50_frontslowmoJun5shuffle1_28000.1</th>\n",
       "      <th>DeepCut_resnet50_frontslowmoJun5shuffle1_28000.2</th>\n",
       "      <th>DeepCut_resnet50_frontslowmoJun5shuffle1_28000.3</th>\n",
       "      <th>DeepCut_resnet50_frontslowmoJun5shuffle1_28000.4</th>\n",
       "      <th>DeepCut_resnet50_frontslowmoJun5shuffle1_28000.5</th>\n",
       "      <th>DeepCut_resnet50_frontslowmoJun5shuffle1_28000.6</th>\n",
       "      <th>DeepCut_resnet50_frontslowmoJun5shuffle1_28000.7</th>\n",
       "      <th>DeepCut_resnet50_frontslowmoJun5shuffle1_28000.8</th>\n",
       "      <th>DeepCut_resnet50_frontslowmoJun5shuffle1_28000.9</th>\n",
       "      <th>DeepCut_resnet50_frontslowmoJun5shuffle1_28000.10</th>\n",
       "      <th>DeepCut_resnet50_frontslowmoJun5shuffle1_28000.11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bodyparts</td>\n",
       "      <td>LeftHand</td>\n",
       "      <td>LeftHand</td>\n",
       "      <td>LeftHand</td>\n",
       "      <td>RightHand</td>\n",
       "      <td>RightHand</td>\n",
       "      <td>RightHand</td>\n",
       "      <td>Nose</td>\n",
       "      <td>Nose</td>\n",
       "      <td>Nose</td>\n",
       "      <td>Pellet</td>\n",
       "      <td>Pellet</td>\n",
       "      <td>Pellet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coords</td>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>likelihood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1390.192226409912</td>\n",
       "      <td>863.4161190986633</td>\n",
       "      <td>0.0021387303713709116</td>\n",
       "      <td>655.7109663486481</td>\n",
       "      <td>671.5528552532196</td>\n",
       "      <td>0.03249989077448845</td>\n",
       "      <td>858.313562631607</td>\n",
       "      <td>502.5887746810913</td>\n",
       "      <td>0.008192891255021095</td>\n",
       "      <td>780.1106552109122</td>\n",
       "      <td>385.51115894317627</td>\n",
       "      <td>0.005371846724301577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1342.6869158744812</td>\n",
       "      <td>1022.5391943454742</td>\n",
       "      <td>0.002777844201773405</td>\n",
       "      <td>655.604768037796</td>\n",
       "      <td>672.7327032089233</td>\n",
       "      <td>0.014964205212891102</td>\n",
       "      <td>858.8051793575287</td>\n",
       "      <td>503.0613145828247</td>\n",
       "      <td>0.009881128557026386</td>\n",
       "      <td>780.0567750260234</td>\n",
       "      <td>385.8125066757202</td>\n",
       "      <td>0.004218136891722679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1378.603021144867</td>\n",
       "      <td>839.2000513076782</td>\n",
       "      <td>0.04729203134775162</td>\n",
       "      <td>655.5680959224701</td>\n",
       "      <td>671.9902248382568</td>\n",
       "      <td>0.008627155795693398</td>\n",
       "      <td>1376.6142420768738</td>\n",
       "      <td>838.8206067085266</td>\n",
       "      <td>0.023861268535256386</td>\n",
       "      <td>779.368589758873</td>\n",
       "      <td>384.6133441925049</td>\n",
       "      <td>0.00545266130939126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>717.1648300886154</td>\n",
       "      <td>528.1120610237122</td>\n",
       "      <td>0.003874382469803095</td>\n",
       "      <td>713.5224957466125</td>\n",
       "      <td>529.766140460968</td>\n",
       "      <td>0.013523245230317116</td>\n",
       "      <td>712.549768447876</td>\n",
       "      <td>530.312716960907</td>\n",
       "      <td>0.013139158487319946</td>\n",
       "      <td>779.9379644244909</td>\n",
       "      <td>384.9282178878784</td>\n",
       "      <td>0.008757703006267548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1342.3471715450287</td>\n",
       "      <td>1021.1889725923538</td>\n",
       "      <td>0.002227584132924676</td>\n",
       "      <td>772.2066966891289</td>\n",
       "      <td>391.2881712913513</td>\n",
       "      <td>0.004997084848582745</td>\n",
       "      <td>776.2603969573975</td>\n",
       "      <td>387.5884371101856</td>\n",
       "      <td>0.016647374257445335</td>\n",
       "      <td>781.6834099292755</td>\n",
       "      <td>385.06572008132935</td>\n",
       "      <td>0.03011847473680973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>1341.7412056922913</td>\n",
       "      <td>1020.5597079992294</td>\n",
       "      <td>0.004295835737138987</td>\n",
       "      <td>764.2025130093098</td>\n",
       "      <td>382.7016522884369</td>\n",
       "      <td>0.006730221211910248</td>\n",
       "      <td>776.0733242034912</td>\n",
       "      <td>387.5614034831524</td>\n",
       "      <td>0.021196046844124794</td>\n",
       "      <td>782.169196844101</td>\n",
       "      <td>384.94352865219116</td>\n",
       "      <td>0.020823610946536064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>782.3293192386627</td>\n",
       "      <td>393.09867787361145</td>\n",
       "      <td>0.0023416001349687576</td>\n",
       "      <td>763.9979656348005</td>\n",
       "      <td>382.5667326450348</td>\n",
       "      <td>0.00377332279458642</td>\n",
       "      <td>776.1697680950165</td>\n",
       "      <td>387.81131461262703</td>\n",
       "      <td>0.022181008011102676</td>\n",
       "      <td>781.9936457872391</td>\n",
       "      <td>385.16719913482666</td>\n",
       "      <td>0.02330200932919979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>781.9441992044449</td>\n",
       "      <td>392.8441734313965</td>\n",
       "      <td>0.0025434144772589207</td>\n",
       "      <td>764.6089184880257</td>\n",
       "      <td>382.68700790405273</td>\n",
       "      <td>0.004340021871030331</td>\n",
       "      <td>776.1248214244843</td>\n",
       "      <td>386.51550006866455</td>\n",
       "      <td>0.016398178413510323</td>\n",
       "      <td>782.0050585269928</td>\n",
       "      <td>385.07819986343384</td>\n",
       "      <td>0.02907874435186386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      scorer DeepCut_resnet50_frontslowmoJun5shuffle1_28000  \\\n",
       "0  bodyparts                                       LeftHand   \n",
       "1     coords                                              x   \n",
       "2          0                              1390.192226409912   \n",
       "3          1                             1342.6869158744812   \n",
       "4          2                              1378.603021144867   \n",
       "5          3                              717.1648300886154   \n",
       "6          4                             1342.3471715450287   \n",
       "7          5                             1341.7412056922913   \n",
       "8          6                              782.3293192386627   \n",
       "9          7                              781.9441992044449   \n",
       "\n",
       "  DeepCut_resnet50_frontslowmoJun5shuffle1_28000.1  \\\n",
       "0                                         LeftHand   \n",
       "1                                                y   \n",
       "2                                863.4161190986633   \n",
       "3                               1022.5391943454742   \n",
       "4                                839.2000513076782   \n",
       "5                                528.1120610237122   \n",
       "6                               1021.1889725923538   \n",
       "7                               1020.5597079992294   \n",
       "8                               393.09867787361145   \n",
       "9                                392.8441734313965   \n",
       "\n",
       "  DeepCut_resnet50_frontslowmoJun5shuffle1_28000.2  \\\n",
       "0                                         LeftHand   \n",
       "1                                       likelihood   \n",
       "2                            0.0021387303713709116   \n",
       "3                             0.002777844201773405   \n",
       "4                              0.04729203134775162   \n",
       "5                             0.003874382469803095   \n",
       "6                             0.002227584132924676   \n",
       "7                             0.004295835737138987   \n",
       "8                            0.0023416001349687576   \n",
       "9                            0.0025434144772589207   \n",
       "\n",
       "  DeepCut_resnet50_frontslowmoJun5shuffle1_28000.3  \\\n",
       "0                                        RightHand   \n",
       "1                                                x   \n",
       "2                                655.7109663486481   \n",
       "3                                 655.604768037796   \n",
       "4                                655.5680959224701   \n",
       "5                                713.5224957466125   \n",
       "6                                772.2066966891289   \n",
       "7                                764.2025130093098   \n",
       "8                                763.9979656348005   \n",
       "9                                764.6089184880257   \n",
       "\n",
       "  DeepCut_resnet50_frontslowmoJun5shuffle1_28000.4  \\\n",
       "0                                        RightHand   \n",
       "1                                                y   \n",
       "2                                671.5528552532196   \n",
       "3                                672.7327032089233   \n",
       "4                                671.9902248382568   \n",
       "5                                 529.766140460968   \n",
       "6                                391.2881712913513   \n",
       "7                                382.7016522884369   \n",
       "8                                382.5667326450348   \n",
       "9                               382.68700790405273   \n",
       "\n",
       "  DeepCut_resnet50_frontslowmoJun5shuffle1_28000.5  \\\n",
       "0                                        RightHand   \n",
       "1                                       likelihood   \n",
       "2                              0.03249989077448845   \n",
       "3                             0.014964205212891102   \n",
       "4                             0.008627155795693398   \n",
       "5                             0.013523245230317116   \n",
       "6                             0.004997084848582745   \n",
       "7                             0.006730221211910248   \n",
       "8                              0.00377332279458642   \n",
       "9                             0.004340021871030331   \n",
       "\n",
       "  DeepCut_resnet50_frontslowmoJun5shuffle1_28000.6  \\\n",
       "0                                             Nose   \n",
       "1                                                x   \n",
       "2                                 858.313562631607   \n",
       "3                                858.8051793575287   \n",
       "4                               1376.6142420768738   \n",
       "5                                 712.549768447876   \n",
       "6                                776.2603969573975   \n",
       "7                                776.0733242034912   \n",
       "8                                776.1697680950165   \n",
       "9                                776.1248214244843   \n",
       "\n",
       "  DeepCut_resnet50_frontslowmoJun5shuffle1_28000.7  \\\n",
       "0                                             Nose   \n",
       "1                                                y   \n",
       "2                                502.5887746810913   \n",
       "3                                503.0613145828247   \n",
       "4                                838.8206067085266   \n",
       "5                                 530.312716960907   \n",
       "6                                387.5884371101856   \n",
       "7                                387.5614034831524   \n",
       "8                               387.81131461262703   \n",
       "9                               386.51550006866455   \n",
       "\n",
       "  DeepCut_resnet50_frontslowmoJun5shuffle1_28000.8  \\\n",
       "0                                             Nose   \n",
       "1                                       likelihood   \n",
       "2                             0.008192891255021095   \n",
       "3                             0.009881128557026386   \n",
       "4                             0.023861268535256386   \n",
       "5                             0.013139158487319946   \n",
       "6                             0.016647374257445335   \n",
       "7                             0.021196046844124794   \n",
       "8                             0.022181008011102676   \n",
       "9                             0.016398178413510323   \n",
       "\n",
       "  DeepCut_resnet50_frontslowmoJun5shuffle1_28000.9  \\\n",
       "0                                           Pellet   \n",
       "1                                                x   \n",
       "2                                780.1106552109122   \n",
       "3                                780.0567750260234   \n",
       "4                                 779.368589758873   \n",
       "5                                779.9379644244909   \n",
       "6                                781.6834099292755   \n",
       "7                                 782.169196844101   \n",
       "8                                781.9936457872391   \n",
       "9                                782.0050585269928   \n",
       "\n",
       "  DeepCut_resnet50_frontslowmoJun5shuffle1_28000.10  \\\n",
       "0                                            Pellet   \n",
       "1                                                 y   \n",
       "2                                385.51115894317627   \n",
       "3                                 385.8125066757202   \n",
       "4                                 384.6133441925049   \n",
       "5                                 384.9282178878784   \n",
       "6                                385.06572008132935   \n",
       "7                                384.94352865219116   \n",
       "8                                385.16719913482666   \n",
       "9                                385.07819986343384   \n",
       "\n",
       "  DeepCut_resnet50_frontslowmoJun5shuffle1_28000.11  \n",
       "0                                            Pellet  \n",
       "1                                        likelihood  \n",
       "2                              0.005371846724301577  \n",
       "3                              0.004218136891722679  \n",
       "4                               0.00545266130939126  \n",
       "5                              0.008757703006267548  \n",
       "6                               0.03011847473680973  \n",
       "7                              0.020823610946536064  \n",
       "8                               0.02330200932919979  \n",
       "9                               0.02907874435186386  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = pd.read_csv(r\"C:\\Users\\vjj14\\Desktop\\DeepLabCut\\frontslowmo-vj-2019-06-05\\videos\\1080pDeepCut_resnet50_frontslowmoJun5shuffle1_28000.csv\")\n",
    "csv.head(10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Demo-yourowndata.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
